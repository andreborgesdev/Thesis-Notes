# Rest

Most of the time when we hear this term, the first thing that comes to mind is APIs, it's how we build APIs, and JSON, because that's what we get back from these APIs, and that's a pretty common thing to say as all we read and hear about are, well, RESTful APIs that return JSON. But, REST isn't just about building an API which consists of a few HTTP services that return JSON. It's much broader than that.

Rest stand for Representational state transfer and is intended to evoke an image of how a well-designed web application behaves, a network of web pages where the user progresses through an application by selecting links (state transitions), resulting in the next page (representing the next state of the application) being transferred to the user and rendered for their use.

- REST is an architectural style, not a standard - we use standards to implement it
- REST is protocol agnostic - JSON isn't a part of REST, but theoretically not even HTTP is a part of REST, and that's really theoretical.

Imagine you want to read your favorite newspaper online. You've opened your browser. That browser, that's an HTTP client. You point it to a URI. That's the unique resource identifier. It identifies where the resource lives. By doing that, the browser actually sends an HTTP request to that URI. The server then does some magic and sends an HTTP response message back to the browser. That HTTP response message contains a representation of the page you've navigated to.  In our example, that would probably be some HTML and CSS. The browser then interprets that resource representation and shows it. In other words, the browser, our HTTP client, has changed state. Now let's say we click a link in our browser to access a specific article on the newspaper side. That one is again identified by a URI. A new request message is sent to the server, and the server again sends back a representation of the page, the resource. The browser interprets it and changes state. In other words, the client changes state depending on the representation of the resource we're accessing. And that's representational state transfer, or REST.

## REST Constraints

REST is defined by 6 constraints (one optional).

A contraint is a design decision that can have positive and negative impacts - for this, contraints that benefits are considered to outweight the disadvantages.

- Client-server constraint - a very basic fundamental constraint. What this does is enforce client-server architecture. A client, or consumer of the API in our lingo, shouldn't be concerned with how data is stored, or how the representation is generated, that's transparent. A server, the API in our lingo, shouldn't be concerned with, for example, the user interface or user state or anything related to how the client is implemented. In other words, client and server can evolve separately.
- Statelessness constraint - this means that the necessary state to handle every request is contained within the request itself. When a client requests a resource, that request contains all the information necessary to service the request.  It's one of the constraints that ensures RESTful APIs can scale so easily. We don't have things like server-side session state to keep in mind when scaling up.
- Cacheable constraint - This one states that each response message must explicitly state if it can be cached or not. Like this we can eliminate some client/ server interaction, and at the same time prevent clients from using out-of-date data.
- Layered system constraint - That's a pretty easy one actually. A REST-based solution can be comprised of multiple architectural layers, just as almost all application architectures we use today. These layers can be modified, added, removed, but no one layer can directly access a layer that's beyond the next one. That also means that a client cannot tell whether it's directly connected to the final layer or to another intermediary along the way. REST restricts knowledge to a single layer, which reduces the overall system complexity.
- Optional code on demand constraint - This one states that the server can extend or customize client functionality. For example, if your client is a web application, the server can transfer JavaScript code to the client to extend its functionality.
- Uniform interface constraint - as it's divided into four sub-constraints. It states that the API and the consumers of the API share one single technical interface. As we're typically working with the HTTP protocol, this interface should be seen as a combination of resource URIs, where we can find resources, HTTP methods, how we can interact with them, like GET and POST, and HTTP media types, like application/json, application/xml, or more specific versions of that that are reflected in the request and response bodies. All of these are standards, by the way. This makes it is a good fit for cross-platform development. By describing such a contract, this uniform interface constraint decouples the architecture, which in turn enables each part to evolve independently. The four subconstraint are:
  
    1. Identification of resources - It states that individual resources are identified in requests using URIs, and those resources are conceptually separate from the representations that are returned to the client. The server doesn't send an entity from its database or possibly a combination of fields from multiple additional databases and services because our author resource doesn't necessarily map to an author in one database. Instead it sends the data, typically for RESTful APIs, as JSON, but HTML, XML, or custom formats are also possible.
    2. Manipulation of resources through representations - When a client holds a representation of a resource, including any possible metadata, it has enough information to modify or delete a resource on the server, provided it has permission to do so. If the API supports deleting the resource, the response could include, for example, the URI to the author resource, because that's what's required for deleting it.
    3. Self-descriptive message - Each message must include enough information on how to process it. When a consumer requests data from an API, we send a request message, but that message also has headers and a body. If the request body contains a JSON representation of a resource, the message must specify that fact in its headers by including the media type, application/json for example. Like that, the correct parser can be invoked to process the request body, and it can be serialized into the correct class. Same goes for the other way around. Mind you this application/json media type is a simple sample. Media types actually play a very important role in REST.
    4. HATEOS (Hypermedia as the Engine of Application State) - This is the one that a lot of RESTful systems fail to implement. Remember that example we had in the beginning of the module when we explained while looking at a newspaper site how the state of our browser changed when we clicked the link? Well, that link, that's hypertext. Hypermedia is a generalization of this. It adds other types like music, images, etc., and it's that hypermedia that should be the engine of application state. In other words, it drives how to consume and use the API. It tells the consumer what it can do with the API. Can I delete this resource? Can I update it? How can I create it, and where can I find it? This really boils down to a self-documenting API, and that documentation can then be used by the client application to provide functionality to the user.

![REST Constraints 1](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/REST_Constraints_1.png?raw=true)

![REST Constraints 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/REST_Constraints_2.png?raw=true)

A system is only considered RESTful when it adheres to all the required constraints.

Not all of these constraints are straightforward to implement, and to be completely correct, an architecture that skips on one of the required constraints or weakens it is considered not conformed to REST, and that immediately means that most APIs that are built today and are called RESTful, well, they aren't really RESTful.

It does mean that you need to know the consequences of deviating from these constraints and understand the potential tradeoff.

HTTP APIs come in many forms.  It's called the Richardson Maturity Model, and it'll help us position REST for building HTTP APIs better.

## The Richardson Maturity Model

The Richardson Maturity Model is a model developed by Leonard Richardson.

It grades, if you will, APIs by their RESTful maturity, so it's interesting to look into it as it shows us how we can go from a simple API that doesn't really care about protocol nor standards, to an API that can be considered RESTful.

- Level 0 - The Swamp of POX, or plain-old XML - This level states that the implementing protocol, HTTP, is used for remote interaction. But, we use it just as that and we don't use anything else from the HTTP protocol correctly. So for example, to get some altered data, you send over a POST request to some basic entry point URI, like host/myapi, and in the body you send some XML that contains info on the data you want. You then get back the data you asked for in the response. To create an author, you send another POST request with some data in the body to that same entry point, and so on. In other words, HTTP is used, but only as a transport protocol. And example of this is SOAP, or other typical remote procedure call implementations. It's not a fully correct statement, but you see a lot of these RPC style implementations when building services with Windows Communication Foundation.

![The Richardson Maturity Model Level 0](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/The_Richardson_Maturity_Model_Level_0.png?raw=true)

- Level 1 - Resources - From this moment on, multiple URIs are used and each URI is mapped to a specific resource, so it extends on level 0 where there was only 1 URI. For example, we now have a URI, host/api/authors, to get a list of authors, and another one, host/api/authors, followed by an ID, to get the author with that specific ID. However, only one method like POST is still used for all interactions. So, the HTTP methods aren't used as they should be according to the standards. This is already one little part of the uniform interface constraint we see here. From a software design point of view, this means reducing complexity by working with different endpoints instead of one large service endpoint.

![The Richardson Maturity Model Level 1](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/The_Richardson_Maturity_Model_Level_1.png?raw=true)

- Level 2 - Verbs - To reach a level 2 API, the correct HTTP verbs, like GET, POST, PUT, PATCH, and DELETE, are used as they are intended by the protocol. In the example we see a GET request to get a list of authors, and a POST request containing a resource representation in the body for creating an author. The correct status codes are also included in this level, i. e. use a 200 Ok after a successful GET, a 201 Created after a successful POST, and so on.  This again adds to that uniform interface constraint. From a software design point of view, we have just removed unnecessary variation. We're using the same verbs to handle the same types of situations.

![The Richardson Maturity Model Level 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/The_Richardson_Maturity_Model_Level_2.png?raw=true)

- Level 3 - Hypermedia controls - This means that the API supports HATEOAS, another part of that uniform interface constraint. A sample GET request to the authors resource would then return not only the list of authors, but also links, hypermedia, that drive application state. From a software design point of view, this means we've introduced discoverability, self-documentation. What is important to know is that according to Roy Fielding, who coined the term REST, a level 3 API is a precondition to be able to talk about a RESTful API. So, this maturity model does not mean there's such a thing as a level 1 RESTful API, a level 2 RESTful API, and so on. It means that there are APIs of different levels, and only when we reach level 3 we can start talking about a RESTful API.

![The Richardson Maturity Model Level 3](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/The_Richardson_Maturity_Model_Level_3.png?raw=true)

## Positioning ASP.NET Core for Building RESTful APIs

RESTful APIs can be built with a variety of technologies. We're going to use ASP. NET Core and the MVC middleware.

It's very important to know that we don't just get a RESTful API out of the box just because we're building an API with ASP. NET Core. That's our responsibility. We get that by adhering to the constraints.

The rest of the explanation is on the API file.

## Structuring Our Outer Facing Contract

The outer facing contract consists of three big concepts a consumer of an API uses to interact with that API.

- Resource Identifier - the URIs where the resources can be found
- HTTP methods
- Payload (OPTIONAL) - Representation - Media types

 When creating a resource, the HTTP response will contain a resource representation in its body. The format of those representations is what media types are used for, like application JSON. The uniform interface constraint does cover the fact that resources are identified by URIs. Each resource has its own URI, but as far as naming of resources is concerned, there isn't a standard that describes that, or at least not unless you want to dive into OData. There are, however, best practices for this. A resource name in a URI should always be a noun. In other words, a RESTful URI should refer to a resource that is a thing, instead of referring to an action.

 So we shouldn't create a getauthors resource, that's an action. We should create an authors resource, that's a thing conveyed by a noun, and use the GET method to get it. To get one specific author then, we'd append it with a forward slash and the authorId. Using these nouns conveys meaning, it describes the resource, so we shouldn't call a resource orders, when it's in fact about authors. That principle should be followed throughout the API for predictability. It helps a consumer understand the API structure.

![Resource Naming Guideline 1](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline.png?raw=true)

 If we have another non-hierarchical resource, say employees, we shouldn't name it api/something/something/employees, we should name it api/employees. A single employee then shouldn't be named id/employees, it should be named employees, forward slash, and the employeeId. This helps keep the API contract predictable and consistent. There's quite a bit of a debate going on on whether or not we should pluralize these nouns. I prefer to pluralize them as it helps to convey meaning. When I see an authors resource, that tells us it's a collection of authors and not one author, but good APIs that don't pluralize nouns exist as well. If you prefer that you can, but do make sure to stay consistent. Either all resources should be pluralized nouns, or singular nouns, and not a mix.

![Resource Naming Guideline 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_2.png?raw=true)

 Another important thing you'd want to represent in an API contract is the hierarchy. Our data or models have structure. For example, an author has books that should be represented in the API contract. So if you want to define an author's books, where the books in the model hierarchy are children of an author, we should represent them as api/authors/authorId/books. A single book should then be followed by the bookId. APIs often expose additional capabilities like filtering and ordering resources, those parameters should be passed through the credit string, they aren't resources in their own right. So we shouldn't write something along the lines of api/authors/orderby/name. There's a few contract smells in that URI. A plural noun should be followed by an Id, and not by another word, and orderby isn't a noun, and a URI like this would mean we'd have defined three different resources - authors, authors/orderby, and authors/orderby/name. So api/authors followed by orderby=name in the credit string is a better fit.

![Resource Naming Guideline 3](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_3.png?raw=true)

But there is an exception. Sometimes there's these remote procedure calls, style-like calls, like calculate total, that don't easily map to resources. Most RPC-style like calls do map to resources, as we've just proven, but what if we need to calculate, say, the total amount of pages an author wrote? It's not that easy to create a resource from that using pluralized nouns. You'd end up with something like api/authors/authorId/pagetotals. We'd expect this to return a collection and not a number.

![Resource Naming Guideline 4](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_4.png?raw=true)

Api/authors/authorId/totalamountofpages. It isn't according to these best practices, but as long as it's an exceptional case, it doesn't mean you've suddenly got a bad API. Remember there isn't standards for following for these naming guidelines, these are just guidelines. So by following these simple rules we'll end up with a good resource URI design.

![Resource Naming Guideline 5](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_5.png?raw=true)

Talking about IDs. REST stops at that outer facing contract. The layers underneath, including the data store, are of no importance to REST, so getting an author might mean that you're actually fetching data from three different data stores, including some fields from Active Directory, to compose that author resource representation. So it's of no importance, our resource isn't the same as what is in the backend store. These are two different concepts. But from that follows the question, what should we use as identifiers? REST is unrelated to the backend data store, yet often you'll see APIs that actually use the auto-numbered primary key Ids from the database. If the backend doesn't matter, what happens to the resource URIs if you change the backend? The resource URI should remain the same, but if resources are identified by their database auto-numbered fields, and we switch out our current SQL Server to a backend that uses another type of auto-number sequence like MongoDB, all of a sudden all our resource Ids can change. We can, of course, work around that on migration, but still, it's a good idea to keep this in mind when designing resource URIs. And there's a solution for this. GUIDs, unique and unguessable values you can use as primary key in every database. From that we can then switch out datastore technologies and our resource URIs will stay the same. We're also no longer potentially exposing implementation details, as those GUIDs don't give anything away about the underlying technology. This advantage, in my book, is readability. As a developer it's not that convenient to type over a GUID to test an API call, but that's what testing tools are for, and for the end product it shouldn't matter. It's not users like you and me who tend to talk to an API, say for during development, its other pieces of code. And for those it really doesn't matter if the URI contains a hard-to-type GUID.

![Resource Naming Guideline 6](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_6.png?raw=true)

![Resource Naming Guideline 7](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Resource_Naming_Guideline_7.png?raw=true)

## Routing

Routing matches a request URI to an action on a controller. So once we send an HTTP request, the MVC framework parses the URI, and tries to map it to an action on a controller, and there's two ways it can do this, convention-based or attribute-based.

- Convetion-based - we have to configure these conventions. We can do that by passing in these routing conventions to the UseMvc extension method. This example would map the URI values index to an index method on a controller named values controller. As you can guess from that example, this is a typical sample of something that's used when building a web application with views that return HTML. The MVC Middleware can be used for that, but for APIs the ASP. NET Core team recommends not using convention-based routing, but attribute- based routing instead.

![Routing Guidelines 1](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Routing_Guidelines.png?raw=true)

- Attribute-based - as the name implies, allows us to use attributes at controller and action level. We provide these with a URI template, and through that template and the attribute itself, a request is matched to a specific action on a controller. For this we use a variety of attributes, depending on the HTTP method we want to match. We should not dinamically put the Route of the Controller as the controller name, because if we were to have a refactoring of our codes, and rename the controller class to URI, to our authors resource, would automatically change. For APIs this in not an advantage, resource URIs should remain the same, and if we were to refactor this controller so it has another name, all our resources URIs would change. The name of the underlying class is of no importance to the consumer, so that is something we want to avoid.

![Routing Guidelines 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Routing_Guidelines_2.png?raw=true)

## Interacting with Resources Through HTTP Methods

Different actions can happen to resources at the same URI. For example, getting an author and deleting an author are interactions with the same resource URI. It's the method that defines the action that will happen, and depending on the method, we'll potentially need to send or get a payload. It's important to follow this standard so other components of our application can rely on this being implemented correctly.

- GET - Reading Resources - There is no request payload, but the response payload contains either a list of author representations, or a single author.

- POST - Creating a resource - Payload we pass in is a representation of the resource we're going to create, an author in our example, and the response payload then contains the newly created author resource.

- PUT or PATCH - Ppdating resources - two options are available. The first one is PUT, which should be used for full updates. A PUT request to api/authors/authorId would update the author with that Id. The request payload is a representation of the resource we want to update, including all fields, and if a field is missing, it should be put to its default value. The response payload can be that updated author, or it can be empty, but you don't always want to fully update your resource, in fact, more often than not, you'll need partial updates to update only one or two fields instead of all of them. And that's what the PATCH method is for. The URI is the same as for PUT. The request payload is somewhat special here, it's a JsonPatchDocument, essentially a set of changes that should be executed on that resource. And just as with PUT, the response payload can be that updated author, or it can be empty.

- DELETE - Delete a resource - This time, both requests and response payloads are empty.

- HEAD - Is identical to GET with the notable difference that the API shouldn't return a response body, so no response payload. It can be used to obtain information on the resource like testing it for validity, for example, to test if a resource exists.

- OPTIONS - Represents a request for information about the communication options available on that URI. So in other words, OPTIONS will tell us whether or not we can GET the resource, POST it, DELETE it and so on. These OPTIONS are typically in the response headers and not in the body, so no response payload.

![HTTP Methods](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HTTP_Methods.png?raw=true)

## Outer Facing Model vs Entity

Outer Facing Model != Business Model != Entity Model

When we designed the outer facing contract, we learned that REST stops at that level. What lies underneath that outer facing contract is of no importance to REST. From that we already know that the entity model, in our case used by Entity Framework Core, as a means to represent database roles as objects, should be different from the outer facing model. In some application architectures there's a business layer in-between, which in turn is different from the outer facing model and the entity model. The outer facing model does only represents the resources that are sent over the wire in a specific format, but it also leads to possibilities. 

Take an author, for example. We can see some pseudocode for that. An author is stored in our database with a DateOfBirth, but that DateOfBirth, well that might not be what we want to offer up to the consumers of the API. They might be better off with the age. Another example might be concatenation. Concatenating the FirstName and LastName from an entity into one name field in the resource representation, and sometimes data might come from different places. An author could have a field, say, Royalties, that comes from another API our API must interact with. That alone leads to issues when using entity classes for the outer facing contract, as they don't contain that field.

![Outer Facing Model VS Entity Model](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Outer_Facing_Model_VS_Entity_Model.png?raw=true)

Keeping these models separate leads to more robust, reliably evolvable code. Imagine having to change a database table, that would lead to a change of the Entity class. If we're using that same Entity class to directly expose data via the API, our clients might run into problems because they're not expecting an additional renamed or removed field. So when developing, it's fairly important to keep these separate.

NOTE: Attributes can also be used in other classes, like our DTOs (not only Entities), and they're very useful for validation scenarios, but the point here is that we won't implement these on the AuthorDto class, because this one is only used for one purpose, returning data. So using validation attributes or data annotations used to validate input doesn't make sense on this AuthorDto class, which is only used for returning data to the consumer.

## Handling Faults

Server errors.

We get back a 500 Internal Server Error. ASP. NET Core will automatically return this when an unhandled exception happens.

We might want to execute some additional logic when an exception happens, like logging a custom message or some information for the system admins.

There's a few ways we can handle these exceptions:

- Try-Catch - We want to make sure a 500 Internal server error is returned when something happens that results in our API not being able to fulfill the request, like our random exception we're throwing. So when we catch this exception, we return a status code 500. For that we can use the status code helper method. We pass in the status code and an optional message. That message, an object actually, will be serialized into the response body. What's important here is that we don't want to pass in the exception itself, let alone the stack trace. Passing in the stack trace and sending it to the consumer of the API means we would be exposing implementation details to that consumer, and that's a security risk, but it's also useless. The consumer of the API has no use for that stack trace, as this is a fault an exception is not responsible for, and can't do anything about. So we return a generic message or no message at all, as consumers of the API are often machines rather than humans. This means we'd essentially have to write try-catch statements for each action, and maybe logging statements. Moreover, even though we're in the development environment, we no longer see the developer-friendly error page. That's because we handled the exception when we called it by returning that 500 Internal Server Error. When running in production, you don't want to return a stack trace, but when you're developing it can be very useful. There's another option.
- Global Exception Handling - what we saw before we wrote a try-catch statement, well that is global exception handling at work. Let's open the Configure method in the Startup class. There's two pieces of middleware here that handle these exceptions, the exception handler middleware is used when we're in a production environment. An approach I like to take to handle faults when building a RESTful API is to let the developer exception page as-is during development. That said, when not running in a development environment, I like to add a bit of configuration to the exception handler middleware to have it return a generic error message. So this is the exception handler middleware, and we can configure that by passing in a lambda that returns an action on IApplicationBuilder. We can then call Run on that appBuilder. What that will do is add a piece of code, which we're going to write, to the request response pipeline. So what we want to do is make sure that the status code is 500, and that we write out a generic error message as the response body. So let's write a piece of code. We need an action on the context here, and on that context we use context. Response. StatusCode to set the status code, and then we write out our error message by calling into context. Response. WriteAsync. That one is from the Microsoft. AspNetCore. Http namespace. We pass in that same generic error message and we're done.

## Parent/Child Relationships

Then there's that author entity. We could add an AuthorDto here, letting AutoMapper take care of the mapping for us, but in this case that would result in the same AuthorDto being returned for each book. That's redundant information, and it will hurt performance sending it over the wire for each book.  If you had that same author again, and again, for each book, while the author possibly includes a collection of books, we will run into circular reference errors, so we do not include the author here. We can, however, safely include the AuthorId.

let's set this off against one of our constraints, manipulation of resources through representations. That's the constraint that's stated that when a client altered a presentation of a resource, including any possible metadata, it must have enough information to modify or delete a resource on the server, provided it has permission to do so. o if the consumer of the API gets the response we see now, does he have enough information to modify or delete the author? Well, not really, what should be in the response to allow for that, at a minimum, is the resource URI. We already include an Id, and often that's considered enough. From the Id a consumer can create a URI, but if you think about this a bit further, it isn't completely correct. An Id alone isn't what identifies the resource, it's the URI that identifies the resource, and the resource URI is part of the request, but it's not part of the response. So to adhere to this constraint we should include the URI in each representation if update or delete is allowed. It's just a matter of adding an extra field and filling it up with the URI, but it's also not completely correct, and we're just getting started. There's a much better way of handling this than including the URI, and that's through HATEOAS.

## Method Safety and Method Idempotency

A method is considered safe when it doesn't change to resource representation. For example, get and head are considered safe. Mind you that doesn't mean that other types of manipulation can't happen as a result of performing a get request. Behind the outer-facing contracts, the API might update a fields in related tables, but the resource representation itself shouldn't change, and if those side effects happen, these weren't requested by the consumer of the API.

A method is considered idempotent when it can be called multiple times with the same result. In other words, the side effects of calling it once are the same side effects that happen when calling it multiple times.

Get is both safe and idempotent. It doesn't change the resource representation, and when you call it multiple times, the same results are returned. Options and head follow the same logic, but post is none of these, it causes changes in resource representations because it creates them, and if we call post multiple times, multiple resources should be created. Delete isn't safe, the resource representation changes as the resource is deleted, but it is idempotent. Deleting the same resource multiple times has the same result as deleting it once. Put for full updates isn't safe either. If you update the resource, its representation might change, but it is idempotent. Updating the same resource multiple times results in the same representation as updating it once. And lastly, patch for partial updates is neither safe nor idempotent. We learned that through patch it's easy to, for example, add items to an array, which results in different representations if you do that multiple times, and this is not just some theory, it helps us decide what we are allowed to do for each method. We're building a RESTful API that uses these HTTP methods. It's a standard, so we should correctly implement it so other components we might use can rely on this being correctly implemented.

![Method Safety and Method Idempotency](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Method_Safety_&_Method_Idempotency.png?raw=true)

## Creating a Resource

We should create different DTOs for different operations.

Mind you there are system where the properties on the class used for output are exactly the same as those on the class used for input, but even in those cases, I'd suggest to keep these separate. It leads to a model that's more in line with the API functionality, make change already factoring afterwards much easier, and when validation comes into play, you typically want validation on input, but not necessarily on output. So I suggest to use a separate Dto for creating, updating, and returning resources.

The first thing we need to check is if the input provided in the request body was correctly serialized to AuthorForCreationDto. If that isn't the case, the author value will be null, that means the client made a mistake, so we return a 400 BadRequest. If that checks out, we can map the AuthorForCreationDto to an author entity, and add it to the database finder repository. So we'll have to create a new mapping first.

This is the first time we are adding an item to the database, so this might need some additional explanation. At this moment, the entity hasn't been added to the database yet, it's been added to the DbContext, which represents a session with the database. To persist our changes, we must call save on the repository, and if that's save fails, we should return a 500 internal server error. This save methods returns a Boolean, true or false, and we should see the repository has a bit of a black box. In essence, the controller doesn't know about the implementation, so it might contain exception handling code, it might not, it all depends on the provided implementation. So, I do like to treat it as a black box, but we do know what we want to return, a 500 internal server error with a generic error message. We only return a generic error message, because the consumer of the API really doesn't need to know what exactly went wrong, it just needs to know that it's not its responsibility. Our generic exception handler will not catch this, as save doesn't throw an exception, but we can use the StatusCode method for that, passing in the StatusCode. The StatusCode is 500, and we provide a generic message. But we already configured the exception handler middleware in the previous module to return a 500 internal server error with a generic message if an unhandled exception occurs. So another option is to throw an exception from the controller, and let the middleware handle it. Now is this a good approach or a bad approach? Well it's, it kind of depends. Throwing exceptions is expensive, it's a performance hit, so that would lead us to returning the StatusCode from the controller as a best practice, but on the other hand, that also means that we'll have code to return 500 internal server errors in different places, on the global level and in the controller itself. At this moment, that's not too much of a problem, but once we start implementing logging, that would also mean we'd want to provide logging code on each StatusCode 500 we return. I've seen both approaches, and there's something to be said for both. In this case, we're going to have the middleware handle all our responses that warrant a 500 internal server error. That'll come in nicely when we need to implement logging for these types of StatusCodes later on. We'll only have to write that code in one place, being at the configuration of the exception handler middleware.

In case of a successful post, we should return a 201 created response. For that, we can use the CreatedAtRoute method. This method allows us to return a response with the location header, and that location header, that will then contain the URI where the newly created author can be found. So the first thing we need to pass into this method is the route name that's going to be used for generating the URI. it should refer to the action to get a single author, that's our GetAuthor action. So, let's give this a name we can refer to, say GetAuthor, there we go. And we pass in GetAuthor as the first parameter of our CreatedAtRoute method. Now to get an author, we need the author ID. We need to pass that in as a route value, so the correct URI can be generated containing that ID. To do that, we pass in an anonymous type, and we give that anonymous type one field, ID, which is the name used in our route template, and we give it a value of authorToReturn. Id. And lastly, we want to pass in the actual authorToReturn Dto. This one will get serialized into the response body.

 Let's send it, we get back a 201 created, so that means that our author has been created again. We've now got two James Ellroy's in our database. Let's get that list of authors, and here we see that James Ellroy has been added to our database twice. So post is not idempotent, we cannot send that same request twice, and have the same result as only sending it once, because we now have two authors instead of just one. So now you've got a bit of unnecessary data in our database.

## Creating a Child Resource

There's already an AuthorId in the URI, so if we allow the AuthorId in the payloads, well, we might end up with an issue we want to avoid, and that issue is that a post to the book's resource for author A would create a book for author B.

There's two ways to tackle this issue. One is adding the AuthorId in the BookForCreationDto. But that would mean we have to check if that AuthorId matches the AuthorId from the URI. The second approach is not sending over the AuthorId in the request body, and thus not adding it as a property on our BookForCreationDto.

Everytime we do a create we should return an CreatedAtRoute("GetBookForAuthor", new { authorId = authorId, bookId = bookToReturn.Id }, bookToReturn);

## Create Child Resources Together with a Parent Resource

There's a pretty common use case when working with RESTful APIs, creating a list of children together with a parent resource in one go. So rather than having to send subsequent requests to the book's resource for an author, we want to create the author and its list of books all together.

We're going to extend this, so it can create both an author without books and one with books. The first thing we need to do is extend the AuthorForCreationDto. Here we'll want to add a collection of books, and we already have a Dto for creating books, our BookForCreationDto. We must use that class and not the BookDto to avoid running into issues like having a book with the wrong AuthorId. It's a good idea to always initialize these types of collections as to avoid null reference exceptions.

What's going to happen when we pass in an author with a list of books is that the author will be de-serialized into an AuthorForCreationDto, which contains a list of BookForCreationDto objects, so all the books in our request payload will be de-serialized into a list of that type. The checks here can remain as is, and the actual persistence logic is in the repository. With this little change, we've now got an action that we can use to create one author with or without books. This again drives home the point of differentiating between input and output. Input, in this case, is an author with optionally a list of books, an output is just the author, but with different fields. Now of course the repository must support this as well. We're treating it as a bit of a black box, because we're focusing our rests, and that stops at the outer-facing layer.

he AddAuthor method on the repository will look at the books, and it will generate an Id for each of them if there are books in the collection. From that moment on, the DbContext contains a new author and a new list of books for that author, and once we call save, these are persisted, and that's handled by Entity Framework Core. Were we to make the Ids for each entity an identity column, EF Core will automatically generate GUIDs for us, so we don't even have to write code to create them. Our request payload was de-serialized into an author with two books. Let's continue. After mapping, our authorEntity also has these two books, as the authorEntity also has a collection of books, the AuthorForCreationDto has been mapped to an authorEntity, and the BookForCreationDto has been mapped to bookEntities, but the entities don't have their Ids filled out yet, and the author is also null at the moment. After they go to the repository, the Ids will be filled out.

## Creating a Collection of Resources

We can't just post a list of authors to api/authors, as how we named URI implies that one author will be posted, so what can we do? This is one of the reasons it's so important to realize there doesn't have to be a direct mapping between the entities in the backend store and our outer-facing contract. It's not because the author's resource maps to the authors in our database, that other resources cannot have an effect on the same database table, so what does all of this mean, and what should we do then? Well, we design a new resource, an AuthorCollections resource. This requires a request body that's an array of all representations, and it will result in a list of authors being added to the database.

As we're creating a resource, this warrants a 201 Created status code. So we should return CreatedAtRoute, and this is the tricky part. We should include the URI where we can get this collection in the location header, so we need an action to get this AuthorCollection, but how do we do that then? If we're adding to the authors to the author table via our data store, so we don't have a direct key for an AuthorCollection. The key is a combination of a set of AuthorIds, the separation between the outer-facing contract and the data store is becoming even more apparent.

## Working with Array Keys and Composite Keys

Let's first separate these two. With an array key, I mean that the key is a comma-separated list, something along the lines of 1, 2, 3, if we were working with integer keys. With a composite key, I mean the key is a combination of key-value pairs, something along the lines of key 1=1, key 2=2, and so on.

the question is more related to URI design than implementation. We need an array key for our location header when creating an AuthorCollection, so let's start with that. There isn't a standard that states how to work with this, but what's often done is using round brackets containing the comma-separated key. So, let's add a method to get one AuthorCollection that accepts a key that is actually a list of author GUIDs. (key1,key2,key3)

hat id's parameter is an array of GUID, but there's no implicit binding to such an array that's part of the route. But if there's no implicit binding, well, we'll just have to provide it ourselves, and we can do that with the help of a custom-model binder. So let's add one to the Helpers folder, ArrayModelBinder sounds like a good name.

```csharp
public class ArrayModelBinder : IModelBinder
{
    public Task BindModelAsync(ModelBindingContext bindingContext)
    {
        // Our binder works only on enumerable types
        if (!bindingContext.ModelMetadata.IsEnumerableType)
        {
            bindingContext.Result = ModelBindingResult.Failed();
            return Task.CompletedTask;
        }

        // Get the inputted value through the value provider
        var value = bindingContext.ValueProvider
            .GetValue(bindingContext.ModelName).ToString();

        // If that value is null or whitespace, we return null
        if (string.IsNullOrWhiteSpace(value))
        {
            bindingContext.Result = ModelBindingResult.Success(null);
            return Task.CompletedTask;
        }

        // The value isn't null or whitespace, 
        // and the type of the model is enumerable. 
        // Get the enumerable's type, and a converter 
        var elementType = bindingContext.ModelType.GetTypeInfo().GenericTypeArguments[0];
        var converter = TypeDescriptor.GetConverter(elementType);

        // Convert each item in the value list to the enumerable type
        var values = value.Split(new[] { "," }, StringSplitOptions.RemoveEmptyEntries)
            .Select(x => converter.ConvertFromString(x.Trim()))
            .ToArray();

        // Create an array of that type, and set it as the Model value 
        var typedValues = Array.CreateInstance(elementType, values.Length);
        values.CopyTo(typedValues, 0);
        bindingContext.Model = typedValues;

        // return a successful result, passing in the Model 
        bindingContext.Result = ModelBindingResult.Success(bindingContext.Model);
        return Task.CompletedTask;
    }
}
```

So any ModelBinder should implement IModelBinder. And that interface exposes exactly one method, BindModelAsync. We get a model bindingContext passed in, and through that model bindingContext, we can get information of what we're binding to, and we can get the input values. So the first thing we do is make sure that the metadata about the model tells us that it's an enumerable type. If not, we set the result on the bindingContext to ModelBindingResult. failed, and we return a completed task signifying that this part is done, otherwise we can continue. The bindingContext also exposes a value provider. Through that, we can get the inputted value by passing in the model name on the getValue method. That value will, in our case, then contain a string that's a list of GUIDs. If the value IsNullOrWhiteSpace, we want to return null, that's what we use to check for a bad request. If the checks we just did check out, we can continue. We look for the generic type argument on our model type, that should return GUID. And then we create a converter. These converters are built in, and they help us with converting, in our case, string values to GUIDs. To get that converter, we call into GetConverter on the type descriptor class, passing in the elementType, GUID in our case. After that, we run through our list of values. So we split it up with a comma as a separator, and each string is converted to a GUID by calling into Converter. ConvertFromString. Lastly, we call to array, so values now contains an array of GUID. The last thing to do is create an actual result for our bindingContext. This should contain an IEnumerable of GUID. So we create an instance of that by calling into Array. CreateInstance. This creates an array of elementType for a specific length. ElementType is GUID, and values. Length, well, that's the amount of GUIDs we passed in. Then we copy over the values array into our typedValues array, and we set the typedValues as the model on our bindingContext. At this moment, our binding was successful. So, we set the result to success, and we pass in the bindingContext. Model. Lastly, we return Task. CompletedTask.

Let's make sure we can use our custom model binder to bind the Ids from the URI to our enumerable of GUID. We can do that with the ModelBinder attribute, passing in the BinderType. That's our ArrayModelBinder.

We need to be able to refer to the route from our method that creates an author collection, so we can create URI for the location header. So let's give it a name, say getAuthorCollection.

To correctly create a response, we'll need the content for the response body, an IEnumerable of AuthorDto. So let's map the AuthorEntities to that. But also we're going to need something extra, a list of Ids, as that's our key. For that, we can use a string. join statement, passing in a comma as a separator. From each author in our collection to return, we then select the ID.

That takes care of that, but what about composite keys? A composite key consists of multiple key value pairs, instead of a list of one field. So it could be something along the lines of key 1 equals value 1, key 2 equals value 2. This is a requirement that often comes up in systems where there is no simple one-on-one mapping between the outer-facing contract and the backing data store. We haven't got a good example of such a resource, but we already know how to implement something like this. We should use a route template with two keys, in this case, that map to two parameters in the action signature. So, there's actually nothing new about that, say for designing the resource URI.

```csharp
[HttpPost]
public IActionResult CreateAuthorCollection([FromBody] IEnumerable<AuthorForCreationDto> authorCollection)
{
    if (authorCollection == null)
    {
        return BadRequest();
    }

    var authorEntities = _mapper.Map<IEnumerable<Author>>(authorCollection);

    foreach (var author in authorEntities)
    {
        _libraryRepository.AddAuthor(author);
    }

    if (!_libraryRepository.Save())
    {
        throw new Exception("Creating an author collection failed on save");
    }

    var authorCollectionToReturn = _mapper.Map<IEnumerable<AuthorDto>>(authorEntities);

    var idsAsString = string.Join(",",
        authorCollectionToReturn.Select(a => a.Id));

    return CreatedAtRoute("GetAuthorCollection",
        new { ids = idsAsString },
        authorCollectionToReturn);
}

[HttpGet("({ids})", Name = "GetAuthorCollection")]
public IActionResult GetAuthorCollection([ModelBinder(BinderType = typeof(ArrayModelBinder))] IEnumerable<Guid> ids)
{
    if (ids == null)
    {
        return BadRequest();
    }

    var authorEntities = _libraryRepository.GetAuthors(ids);

    if (ids.Count() != authorEntities.Count())
    {
        return NotFound();
    }

    var authorsToReturn = _mapper.Map<IEnumerable<AuthorDto>>(authorEntities);

    return Ok(authorsToReturn);
}
```

## Handling POST to a Single Resource

We've always posted to collection resources up until now, one author to the author's resource, but what about posting to a single resource instead of to a collection resource? Posting to a URI like this can never result in a successful request. It should either return a 404 Not Found if the author doesn't exist, or a 409 conflict if the author already exists. Let's imagine we'd allow posting to a single resource URI that contains an Id of an nonexisting author. While post is used for creating resources, the Id part of the URI signifies a mistake. It's the server that's responsible for creating the resource URI and not the consumer of the API. If we allow the consumer to generate the URI, a post like this would have to be idempotent, and posts isn't idempotent, we cannot rely on the fact that multiple post requests will result in the same outcome. So if treating post as idempotent can be avoided, it should. In other words, a 404 is warranted. Say we send a request like this to an existing URI, we'd expect a 409 conflict as we're trying to create your resource that already exists. Let's try this, and we also get back a 404 Not Found. It's still the same type of URI not linked to a route template, so this makes sense, but it's not correct. It's a matter adhering to the HTTP standard. Most APIs would not bother with adding an additional action just to return a correct HTTP StatusCode.

ere we're going to add a new action, BlockAuthorCreation. We'll again use the HttpPost attribute, this time adding Id to the route template. We then accept that Id in the parameter list. We don't have to bother with accepting a specific type of parameter to serialize the request body to. We're not going to use it to effectively add an author as that's not allowed, we're only going to return the correct StatusCodes. We use this Id to check if an author with that Id exists. If it does, we should return a 409 conflict. There isn't a convenient helper method for that like OK and not found, but we can use the StatusCodeResult class for that. StatusCodeResult is an action result that results in a response without a body, but with a specific StatusCode. So we new that up, and pass in Satus409Conflict from the StatusCode enumeration. If the author doesn't exist, we return not found.

```csharp
[HttpPost("{id}")]
public IActionResult BlockAuthorCreation(Guid id)
{
    if (_libraryRepository.AuthorExists(id))
    {
        return new StatusCodeResult(StatusCodes.Status409Conflict);
    }

    return NotFound();
}
```

We've been working with post in the last few demos, and we've used a content-type header to signify the media type of the request body. Let's open a post request again, and let's look at those headers, here's that content-type header. What would happen if we didn't provide a content-type header. Let's give that a try, let me uncheck that, and let's send. We get back 415 Unsupported Media Type. We didn't pass in the media type, so our API doesn't know how to handle this. In other words, we always need to provide a content-type header when providing a request body that's in line with the self-descriptive message constraint. But what if you want to support other types of input like XML, especially when you're working on an API that integrates between different systems, this can be a requirement.

## Supporting Additional Content-type Values and Input Formatters

 We added an additional output formatter in the previous module by manipulating the output formatters collection. There's another collection InputFormatters, which as the name implies, allows us to add a new input formatter. Let's add one for XML. What we're looking for is the XmlDataContractSerializerInputFormatter. In case you're wondering why we're using the XMLDataContractSerializer versions of the formatters, well, that's because this formatter supports types like the dateTime offset value we have for dateOfBirth. The XmlSerializer requires that the type be designed in a specific way in order to serialize completely. Most types in. NET were not designed with the XmlSerializer in mind, so that's why it's preferable to use the dataContractSerializer versions of it.

```csharp
services.AddMvc(setupAction => 
{
    setupAction.ReturnHttpNotAcceptable = true;
    setupAction.OutputFormatters.Add(new XmlDataContractSerializerOutputFormatter());
    setupAction.InputFormatters.Add(new XmlDataContractSerializerInputFormatter());
});
```

## Deleting a Resource

There's no response body after the successful delete to resources gone, so we return a 204 No Content. This signifies that the request was successful, but doesn't have a response body.

Delete is obviously unsafe, as it changes the resource, in this case it deletes it. It's also idempotent, sending the same delete request over and over again will not change the resources any more than it did with the first request the first time it was deleted.

## Deleting a Resource with Child Resources

We'll see how we can handling deleting an author. When we do that, the books for that author should be deleted as well, as an author is the parent resource of a book's resource.

First thing to check, does the author exist? We don't use the AuthorExists method, because we need that author entity to pass into the repository to delete it. So we call GetAuthor on the repository, passing in the Id. If it's not found, we return Not Found. If that checks out, we call into the DeleteAuthor method on the repository. We pass in authorFromRepo. CascadeOnDelete is on by default, so when we delete an author with Entity Framework Core, the books for that author are deleted as well. To effectively execute this statement, we call Save on the repository, and throw an exception if the save fails. Lastly, we return NoContent. Let's give that a try. We'll send a delete request to the author Stephen King, and we get back a 204 No Content. So the resource is gone and so are all the books, there's no way to get them anymore.

## Deleting Collection Resources

There's one case we didn't cover, sending a delete request to a collection resource. There's nothing that would stop us from doing so, so it's perfectly allowed. A resource is identified by a URI, and that URI can refer to a collection resource or a single resource, but it's still, well, just a resource, but what would that do? Say we send a delete request to api/authors. That would mean we'd have to delete all our authors, and as books are children of authors, the books for all those authors are well. In other words, we'd end up with not a single resource left to get. So while supporting this is allowed, it's advised against, because delete is a pretty destructive action. Unless you really need it, you don't want to allow this on a collection resource, as that might have the effect of thousands of resources being deleted in one go. Note that that's different from what we just did in the previous demo, where we deleted the books for an author when the author was deleted. In that case, we still send the request to a single resource, which happened to result in other resources getting deleted, because they don't make sense without that parent resource.

![Deleting Collection Resources](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Deleting_Collection_Resources.png?raw=true)

## Updating a Resource

We also learned that as put is for full updates, when a value for a specific field isn't passed in, it should be put to its default value. The approach we're taking here makes sure of that. If a put request is sent without a description, the description string will have its default value. But what about an Id? We're updating an existing resource, so we know the Id. Shouldn't we be able to pass it in then through the request body/ Well, we could, but let's look back at the URI of our resource. That already contains the Id of the book. Adding the Id in the request body is thus redundant information, and it will also mean we'd have to add an additional check. The Id in the URI should be the same as the Id in the request body to avoid allowing requests to one resource that actually updates another one. If they are not the same, the Id in the URI wins, and the exact same goes for the AuthorId, that as well is in the URI. So what I prefer to do is avoid these mistakes as soon as we can by not allowing the Ids that are already in the URI in the request body. That then brings us to another issue, if we want to call it that. Let's open the Dto for Creation next to the one for Update. Our BookForUpdateDto and our BookForCreationDto now contain the exact same properties. Can't we just reuse that then? Well, currently we could, but there's a conceptual difference between these Dtos reflected by the name. The one is for updating and the other one for creating, and even though the fields will stay the same in a lot of cases, what is allowed at an update isn't necessarily the same as what is allowed at creation.

Important here is that in REST we are updating the resource and not the entity. So while it might look tempting to just take the input and copy the field values over to this BookForAuthorFromRepo entity, we must keep in mind that projections might have to happen. So that actually means we should first map the entity to BookForUpdateDto, then apply the updated field values to that Dto, and then map the BookForUpdateDto back to an entity. The nice thing is that we're using AutoMapper, and it allows us to combine these steps.

1. Map
2. Apply Update
3. Map back to entity

why is this method empty in our implementation? Well in Entity Framework Core, these entities are tracked by the context. So by executing that Mapper. Map statement, the entity has changed to a modified state, and executing a save will write the changes to the database. So let's call into the save method on the repository.

Then we'll need to return something. For a successful update, a 200 OK could be returned, containing the updated resource representation in the response body. Alternatively, we could return a 204 No Content, both are valid. These days I tend to return a 204 No Content to avoid sending over data that isn't required by the consumer of the API. This approach leaves it up to the consumer to decide on this, but it's not a good case for all APIs. Sometimes fields can be updated that weren't part of the request. Think about a modified at timestamp that's returned when getting a resource. As learned when talking about method safety, put can have side effects like this. If that's the case, a 200 OK response containing the updated resource representation in the body seems more appropriate, but in the end, both are valid.

By the way, this does lead to the fact that put is used less and less these days. Imagine a resource with 30 fields, it's not that good for performance that a consumer of an API should have to send over all these field values when he just wants to change one. That's why patch for partial updates is often the preferred option.

## Repository Pattern

An abstraction that reduces complexity and aims to make the code safe for the repository implementation, persistence ignorant. So, from that we know there's a few good reasons to use the repository pattern.

- Less code duplication - Writing code to access a backing data stored directly in a controller action easily leads to code duplication. We might need to access authors or books from multiple parts in our application, so it's preferable to write that code just once instead of in every action or part of the application.
- Less error-prone code - Code will also become less error prone, if only for avoiding that application.
- Better testability of the consuming class - if you want to test the controller action, but the action also contains logic related to persistence, it's harder to pinpoint why something might go wrong. Is it logic in the action, or is it a persistence-related code in the action that fails? If there's a way to mock the persistence-related code and test again, you know that the mistake isn't related to that persistence logic. So using the repository pattern allows for this mocking of persistence logic, which means better testability of the consuming class.

Sometimes it's said that through a repository, you can switch out the persistence technology when needed, and while that is strictly speaking true, it's not really the purpose of the repository pattern. What it is very useful for, however, is allowing us to choose which persistence technology to use for a specific method on the repository. Getting a book might be easier through Entity Framework, getting an author with some complex logic might be more advisable through ado. net, or you might even call into an external service. For consumers of the repository like our controllers, it's of no interest what goes on in the implementation, rather than switching out one persistence technology for another for the complete repository, it allows us to choose the technology that's the best fit for a specific method on the repository, thus persistence ignorant.

Let's get back to why we're calling into that empty update method on the repository. We're working on a repository contract, not an implementation. Different implementations of that repository contract could exist. There's often a mocked repository implementation of a contract used for testing. For that, an update might actually have to do something. So that's why I'd suggest to always have this update method on your contract if an update is allowed. Even if in the specific implementation we're using, it doesn't do anything. By calling into it from the controller, we ensure that older implementations will still execute as expected. If we wouldn't call into this method, because we don't need it for our repository implementation, we might run into issues with other implementations like a mock implementation for testing.

![Repository Pattern](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Repository_Pattern_2.png?raw=true)

## Updating Collection Resources

As we remember from when we talked about deleting a collection resource in the previous module, a resource is just a resource. Single or collection, there's nothing that forbids an update. Say we send such a request to api/authors/ an AuthorId/ and books. That means we'd be updating the books resource to a new value, in this case a set of books. We are replacing the current set of books with a new set of books. Put is for full updates after all. The request body replaces what's at the URI the put request is sent to, which means that the correct way to handle this is to delete all the previous books for that author, and then create the list of books that's inputted for that author, so we end up with a completely-new books resource. Very important here is the distinction between what we're actually doing and what the side effects could be. We are still updating a resource, books, so put is warranted. The fact that some book resources are deleted, and others are created when sending that put request is just a side effect. So the reasoning is a bit the same as for deleting collection resources. It's allowed, but in general it's advised against, because it can be quite destructive. The full list of books must be replaced. We won't implement this in our API, but if you need to, you can. Just be aware of the potential consequences.

## Upserting

Create a new resource with put or patch instead of post. it requires a bit of explanation, because it's often misinterpreted and can cause quite a bit of confusion. We've got a consumer of our API on the left, and the API server on the right. In a lot of systems, it's the server that's responsible for creating the identifier of a resource. Part of it is often the underlying key in the data store, an integer value or a GUID, for example. A consumer might post a new author to the authors resource, and the server response with that newly-created author in the response body, and the location of the author resource, which the server generated in the location header. In fact, in most systems, the server decides on the resource URI, but REST does not have this as a requirement. It's perfectly valid to have a system where the consumer can do this, or where it's allowed for both the consumer and the server. Now if the key in the database is part of the resource URI, this doesn't just work with AutoNum fields. It does, however, when working with GUIDs. So yeah, you got me, this is one of the reasons I chose GUIDs instead of ints to store data in the backend store. Now let's think about what could happen. Say the server is responsible for creating the resource URI, and we want to send an update request. We need to get the URI to a resource from the server to be able to update it. The resource must already exist. And if it doesn't, we must return a 404 Not Found. Now let's imagine the consumer is also allowed to create resource URIs. In that case, we no longer need to get the URI from the server. The hard requirement for the resource having to have been created before vanishes. We can now send the put request to a previously nonexisting resource identifier that is valid, because the consumer is allowed to create it. In that case, that resource must be created when sending the put request, or in a way it's updated from being empty. So if the server is responsible for the resource identifiers, we must use post to create resources. We can't not know the URI of the resource in advance, but if the consumer of the API is allowed to create the resource identifier, well we can use put as well, and this is called upserting. Now let's think back at method idempotency to see if this still fits. We learned that post isn't idempotent, sending the same request more than once will result in different outcomes. Put, however, is, and that fits. If the consumer of the API chooses the URI, sending the request once will create a resource. Sending it again after that will have the exact same result. The same resource with the same Id is created.

![Upserting](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Upserting.png?raw=true)

![Upserting 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Upserting_1.png?raw=true)

We are in our UpdateBookForAuthor action on the BooksController. We've got code here that returns a 404 Not Found if the book we're trying to update doesn't exist yet. This is what we'll need to change. If the book doesn't exist, we want to create it.

The first thing to do is map the book from the request body to book entity, as it's an entity we want to add. We also want to set the Id of that bookToAdd to the Id from the URI. Then we call the AddBookForAuthor method on the repository, passing in the AuthorId and the bookToAdd. We are now using this method in both post and put, a good example of one of the advantages of using the repository pattern.  If there's a book Id filled out like in this case, we're upserting, and we don't fill it out again. If there isn't, we generate a book Id, and that's what happens when we post.

## Partially Updating a Resource

Full updates with put aren't always advised, if only for the overhead it creates. In fact, if we would look at the data, which is a standard that's essentially a set of best practices for creating RESTful APIs, we'd notice that standard state, patch should be preferred over put, and patch, that's partial updates. But that's not sufficient, we need a way to pass a change set to a resource using this HTTP patch method. In other words, what should the body of a patch request look like? Luckily there's a standard for this, the JSON patch standard. This defines a JSON document structure for expressing a sequence of operations to apply to a JSON document. You can look at that structure as a change set, a set of operations that'll be applied to the resource at patch request with that JSON patch document in the body is sent to. The application/json-patch+json media type is used to identify such patch documents.

It starts with straight brackets signifying an array, that array, that's a list of operations that have to be applied to the resource. The first one is a replace operation signified by op. Path signifies the path to the property. These are the property names of the resource, the Dto, and not on whatever lies beneath that layer. Value signifies a new value, so the title property gets the value new title.

There's six different operations possible. The add operation will add a property at a path location with a specific value, passed through via value. If it's used on a path that exists, that property will be replaced. If it's used on an nonexisting path, the property should be added to the resource, but something like that is only possible when working with dynamic resources often in CRM-like systems. It's not applicable in our case, as we're currently working with statically-typed classes. The remove operation will remove a property, or in non-dynamic cases, set it to its default value. Next to the operation value, it only has one property that has to be set, path. Replace replaces the value at a specified path with the provider value. It's functionally the same as a remove operation, followed by an add operation. Copy will take the value from the from property, and copy it over to the path property. It starts an add operation at the path location, with the value specified in the from member. Move then will copy over the value at the from property to the path property, and remove the value at the from property. So this operation is functionally identical to a remove operation on the from location, followed by an add operation at the path location with the removed value. And lastly test tests that a value at the target location is equal to a specified value. These cases aren't limited to simple properties on a resource. We can manipulate array properties, we can access nested properties, and we can even add a list of items to an array. So path doesn't have to be a simple property, and value doesn't have to be one string value. It might as well be an array, so it's a really powerful standard.

![JSON Patch Operation](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/JSON_Patch_Operations.png?raw=true)

![JSON Patch Operation 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/JSON_Patch_Operations_1.png?raw=true)

## Partially Updating a Resource

Then it's time to apply the patch document. It's a patch document that works on a BookForUpdateDto. So we need to get the matching book and map it to a BookForUpdateDto before applying the patch document, and that principle is not new. Remember that with put we talked about the fact that the update is on the resource or the Dto, and not the entity, so we map to BookForUpdateDto.

Important to know is that problems might happen when we do this, if the patch document is invalid, for example, it contains operations on fields that don't allow those operations, like adding a field to a static type that doesn't exist, or replacing a read-only value. Well, if you do that, this will fail, so we do need to add some validation here.

## Upserting with PATCH

With put this was quite easy, because in the request body for those types of requests, the full resource representation should be passed in. For patch, it's a change set to JSON patch document that's passed in. We need something to apply that change set to, a BookForUpdateDto. So let's create one and apply the patch document to it. We do that when the BookForAuthorFromRepo wasn't found. We create a new BookForUpdateDto, and we call apply on patchDoc to apply the change set to it.

One word of caution though, those fields for which no operations are passed in will keep their default values, so keep that in mind when using upsert together with patch.

## HTTP Method Overview by Use Case

Let's start with reading resources. If you want to read a collection resource like authors, we send a get request to authors, that then results in a 200 OK with an author's collection in the response body. A 404 Not found is also possible if the URI doesn't exist. In ASP. NET Core MVC, we didn't have to code anything for that, the framework handles this for us. If we want to read one specific resource, we add the Id, and we should get back a 200 OK with the author in the response body. If the resource doesn't exist, a 404 is warranted. The leading a single resource is done by sending a delete request to that specific resource URI. If the delete is successful, we send back a 204 no content. If the resource doesn't exist, we should return a 404 Not Found. Deleting a collection resource follows the same principles, but it's rarely implemented, because it can be very destructive. For creating resources, we have to make a separation between those requests where the responsibility of creating the URI is at the server versus when it's at the consumer of the API. Let's start with the most obvious case, the server. To create a new author, we should send the post request to the author's resource with the author representation as the request body. If creation is successful, we send back a 201 Created response, with the author representation in the response body and location in location header. If there is no author's resource, a 404 is warranted. Posting to a single resource URI can never result in a successful request. It either returns a 404 Not Found if the author doesn't exist, or a 409 Conflict if the author already exists. And to add a collection in one go, we should create a new resource for that collection. For example, author collections, a post to that resource then contains an author collection, in other words an array of authors, and it can result in a 201 Created, or a 404 Not Found. When the consumer of the API can create resource URIs, for example, when we're working with GUIDs, we can upsert to create resources. A put request to a previously nonexisting URI for an author then results in that author being created. It warrants a 201 Created response with the author in the body. There's no 404 for this URI, because if it's not found, it will be created. And the same is possible with patch with that difference that we're passing in a JSON patch document for that author. Lastly, updating resources. We're starting with full updates with put. A put request to a specific author resource updates that author. We should pass in the full author representation. Fields that are omitted are set to their default values. That can result in a 200 OK with the author representation in the body or a 204 No Content. If the author isn't found, a 404 Not Found is warranted. Of course, if the consumer is allowed to choose the URI, you could upsert instead of returning a 404. Put requests to collection resources are allowed, but rarely implemented. A partial update, well that's what patch is for. To update a specific author, we'd send the patch request to that author's URI, a patch request body can be defined by the JSON patch standard, a list of operations on that specific author. If the patch request is successful, a 200 OK with the updated author representation in the body or a 204 No Content is warranted. A 404 Not Found is returned in the author isn't found, unless a set with put we're upserting. Patch requests to collection resources are just as put requests allowed, but rarely implemented. And that should cover all our use cases. When in doubt, have a look at that method safety and method idempotency table from the previous module. Now there are also other StatusCodes that can be returned, 500 is always possible, and once we add functionality like validation, validation-specific codes can be returned. But this overview is the gist of it, and it should go a long way in mapping required functionality to an outer-facing contract.

![HTTP Method Overview by Use Case](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HTTP_Method_Overview_By_Use_Case.png?raw=true)

![HTTP Method Overview by Use Case 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HTTP_Method_Overview_By_Use_Case_2.png?raw=true)

![HTTP Method Overview by Use Case 3](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HTTP_Method_Overview_By_Use_Case_4.png?raw=true)

## Working with Validation in a RESTful World

When talking about validation, there's essentially three things we need. We need a way to define our validation rules, we need a way to check them, and we need a way to report these validation errors to the consumer of the API, if there are any. Mind you we don't return those errors to let the consumer know whether he or the server made a mistake, that's what the StatusCodes are for. However, often the response body contains a set of validation error messages that the client application could use to show to the user of the application that consumes the API. Most applications require some rules on their objects, be it on a Dto, a business, or a domain object, or an entity. If those rules don't check out, the request should be halted. To create these validation rules, we can use ASP. NET Core's built-in approach or a third-party component. In ASP. NET Core, the default approach to this is using data annotations on our properties. For all common property validation rules, annotations exist, like required or max length. Next to that, we can also define custom rules, rules we can't just easily define with annotations. From a conceptual point of view, what's of importance is what we want to validate. The rule is that we typically validate rules on input, and not on output, so for post, put, and patch, but for not for get. After all, only the input needs to be validated, that's where something can go wrong. This again drives home the point that we should use separate Dtos for separate operations.

![Validation in a RESTful World](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Validation.png?raw=true)

![Define Validation Rules](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Validation_2.png?raw=true)

Checking validation rules. For checking validation rules in ASP. NET Core, the building concept of ModelState is used. ModelState is a dictionary containing both the state of the model and model binding validation. It represents a collection of name and value pairs that were submitted to the API, one for each property. It also contains a collection of error messages for each value submitted. Whenever a request comes in, the rules we define in step one are checked. If one of them doesn't check out, the ModelState's valid property will be false, and this property will also be false if an invalid value for a property type is passed in. So that's what we'll use for checking our rules.

![Check Validation Rules](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Validation_3.png?raw=true)

reporting validation errors. When a validation error happens, the consumer of the API needs to be notified. It's a mistake of the client, so that warrants a 400 level StatusCode. There's one that's specifically used for this, 422 Unprocessable Entity. As we learned before, the 422 StatusCode means that the server understood the content type of the request entity, and the syntax of the request entity is correct, but it wasn't able to process the contained instructions. For example, the syntax is correct, but the semantics are off, and that's a perfect fit for what we need. But when we say reporting validation errors, we're not only talking about the StatusCode, we're also talking about response body. What we write out in the response body isn't regulated by REST, nor by any of the standards we're using, but for validation issues, this will often be a list of properties and their related error messages. f the client application needs to be able to do something more than simply binding these errors to a control, often an error code is included as well, so the client application can act on that error code, also quite useful for multi-language applications. In APS. NET CORE MVC, typically the ModelState's validation errors are serialized, so we end up with a list of property names and related validation errors. But this can vary depending on what we're building the API for.

![Report Validation Errors](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Validation_4.png?raw=true)

## Working with Validation on POST

At entity level, title is required and has a maximum length. Description also has a maximum length. Okay, this ensures no invalid data, at least at this level, is persisted to the database, and that's a good thing, but a 500 Internal Server Error doesn't sound right. That would mean the server made a mistake, but obviously we as consumers of the API made the mistake, and that's an error, not a fault, and it warrants a StatusCode in the 400 range. Moreover, these are data annotations at the level of the entity model, and what we should report back are validation errors at the level of the Dtos, our outer-facing contract. First we need to define the rules, and we can do that with data annotations. So let's open our BookForCreationDto model class. This is the one the body of the post request is de-serialized into.

There's still some duplication of rules between the Dto and the entity, the MaxLength for example, but these are objects that live on different layers, and the rules might not always be the same across layers, so it is warranted.

we've got our data annotations on our BookForCreationDto, but as we know, that's just part of the story. We also need a way to check if these rules are adhered to, and this is where the ModelState comes into play. On to the CreateBookForAuthor action on our BooksController. ModelState is a dictionary containing both the state of the model and model binding validation, and it also contains a collection of error messages for each value submitted. And whenever a request comes in, the rules we just applied to our model, the Dto are checked. If one of these doesn't check out, the isValid property will be false. It will also be false if an invalid value for a property type is passed in, so it's a great value to check to see if we can continue with what's in our action. We do need to keep the null check, as the ModelState will be valid if the body can't be de-serialized to the excepted type. That still warrants a 400 Bad Request, 422 Unprocessable Entity has a requirement that the syntax of the request body must already be correct. When we don't provide input, i. e. an empty body, that's not the case. So if ModelState isn't valid, we should return Unprocessable Entity. In the response body, we can then write out the validation errors, but there is no Unprocessable Entity helper method available on the controller. No worries though, there's something else we can use, an object result used to return responses that contain an object. We can create our own class that inherits from it, so let's create a helper class. We need a constructor, and in that constructor, we want to accept an object, the content that will be written out to the response body. So we'll name it error. This constructor should override the base constructor of ObjectResult, and we pass in our error object. And this will already work, it will return a response with StatusCode 422, but the problem is that if we pass in our ModelState as error object, which is what we're going to do, what will be written out is a huge list of all the properties of the ModelState object and all their values. And that's not what we want. We do not need the full list of everything that's in the ModelState, we want key value pairs, the property that triggered the error, and the error messages. We don't want to accept just any object, we only want to accept the ModelState dictionary. That can be found in the Microsoft. AspNetCore. Mvc. ModelBinding namespace. Let's name it ModelState, and instead of passing this ModelState directly to the base constructor, we wrap it in a serializableError object. SerializableError defines a serializable container for storing ModelState information as key value pairs, in other words it was made for this. Just one additional check, if the ModelState is null, we throw an ArgumentNullException. Okay, that's it for our unprocessable entity result, back to the controller.

This is looking good, but while we're on the subject of these error messages, what we see here are the default messages that come with the annotations. We can change those, let's open the BookForCreationDto class again. The data annotations we've used allow us to pass in custom error messages. To do that, we can set the error message property to the message we want to return if validation fails.

Okay, so now we know how to handle validation and how to correctly respond to failed validation, as always related to the resource, and not to a lower layer in the application architecture, but this is just simple validation. Some business or validation rules can't be covered with data annotations, so let's make one more change to our code.

Let's say we have a rule that states that the description should be different from the title of a book. We can't easily cover that with the annotations we're using, so we'll have to write a custom rule. But we do want any validation errors that happen to result in an invalid ModelState, like that, we can keep on using it like we learned in this demo. Well we can do that. First of all we'll have to write that rule, and that's custom manual code. Right after we check our input for null, but before we check the ModelState, we check if the book's description equals the book's title. If that's the case, we add a ModelError, we can do that by calling into AddModelError on the ModelState. This method requires a key and an error message. The key can be a property name, but it doesn't have to be. Often for cross-property problems, the class name is used. In our case, that's BookForCreationDto.

## Working with Validation on PUT

So isn't this exactly the same as on post? Well, for some resources, if not most, that's probably the case, but it doesn't have to be. In fact, these days we often see use cases for exactly that. Think about those registration forms on fancy startup company sites. They want to onboard you as soon as possible and want to make the process frictionless, for example, they're not going to ask you to fill out tens of pieces of information on yourself at registration, but once you start using the service more, they might ask for more information. And then that information is required, like credit card information if you want to order something via their service. In our case, we could say we want to make it easy for admins to create a book. A title should be sufficient, but if the admin wants to book to a peer on a site, well he must fill out the description as well. So updating that book resource adheres to different rules than creating it. And we can do that. Once more a good reason to separate the Dtos for update and create. On screen we see the BookForUpdateDto. This should have the same rules as the BookForCreationDto and an additional one. Description is now required. First, let's copy over the annotations from the BookForCreationDto. And then let's add a new one, Required for description. But this doesn't feel good, let's open the BookForCreationDto again next to this one. What we see here is a lot of duplicate validation annotations, and to be honest, properties as well. Where possible I prefer to minimize the amount of duplicate code, and we can do that. We can create an abstract class that will serve as the base class for both these Dtos, so let's add one, and let's add the common properties and rules to it. We'll name it BookForManipulationDto.  It's an abstract class, because we don't want that class to be used on its own, it must be derived from. Using the abstract modifier in a class declaration indicates that a class is intended only to be a base class of other classes. And then let's have our other two classes derive from it. We still need to be able to apply that required attribute to the description for the BookForUpdateDto. And preferably we want to do that without having to copy the MaxLength for description to both classes. Let's open the BookForManipulationDto again. We can use the virtual modifier. Virtual properties are great when you have an implementation in the base class, which we have, but we do want to allow overriding. Now we no longer have to implement this property in the BookForCreationDto. Let's open our BookForUpdateDto. Here we can override the description property. Once it's overwritten, we can add our additional validation annotation. The validation annotations on the properties on the base class will still apply, but for the BookForUpdateDto, the additional required annotation will apply as well.

Our custom rule here in the UpdateBookForAuthor action, well, that's almost the same as the custom rule in the CreateBookForAuthor action. We could factor that out into a separate method, or create some sort of validation service that would handle all the validation for us, but the main issue is in my personal opinion with how ASP. NET and ASP. NET Core by default handle this validation. Annotations mix in rules with models, and that's not really a good separation of concerns. And having to write validation rules in two different places, the model and the controller, for the same model, doesn't feel right either. It's good enough for our purposes, because after all, we are talking about RESTful architectures, so that's what we're focusing on. However, I do want to give a tip for when you're building more complex applications, then it might be a good idea to keep an eye on something like fluent validation, which offers a fluent interface to build validation rules for your objects. From version 6. 4 and onwards,. NET Core is supported.

## Working with Validation on PATCH

A patch document can be malformed in many ways. We learned how it has to look in the previous module, so if a consumer passing in a patch document that, for example, tries to apply an operation on an existing property, we must notice, and we can by using that same ModelState we've used before. We applied a patch document to a Dto, but there's an overload for that applyTo method that accepts the ModelState. If we pass in the ModelState, any errors in the patch document will make the ModelState invalid. So we do that, and afterwards we check the isValid property, and if it's not valid, we return a new Unprocessable Entity object result, passing in the ModelState.

his patch request will remove the description, effectively setting it to null. Let's send it, and we get back a 204 No Content, so this just works. But that's not what we want, because a book without a description isn't valid on update. We just added the required annotation to a BookForUpdateDto in the previous demo. Let's see how we can fix that. The issue is related to the fact that the input is a JSON patch document and not a BookForUpdateDto. That means that we must manually validate that BookForUpdateDto after the patch document has been applied to see if it's still valid. We shouldn't map or send it to the repository before that. Currently after the patch document has been applied, we check the ModelState. This ModelState possibly contains errors on the inputted model. The inputted model isn't the BookForUpdateDto, it's the JsonPatch document. So as long as that document is valid, the ModelState will be valid. First let's add that extra validation we couldn't achieve with validation annotations. Title and description must be different. Then let's validate. We can use the TryValidateModel method, passing in the now patched BookToPatch instance. This triggers validation of BookToPatch, and any errors will end up in the ModelState. So after that, we check the ModelState, and return a new unprocessable entity object result containing the errors, that's code we already have.

That leaves us with upserting, and that's quite easy, it's exactly the same principle we have to follow. If a book for an author hasn't been found, we create it. First we pass in the ModelState into the applyTo method. Then we check the now PatchDto, so we define our extra rule, then we call TryValidateModel on the Dto, and if it isn't valid, we return a new UnprocessableEntityObject result containing the ModelState.

## Logging Faults

ASP. NET Core has a bunch of built-in services we can inject and use throughout our application. The logger service, that's one of those, but we do need to configure this. In the configure method, a loggerFactory is injected. This logger factory can be used to configure the logging system, and that's exactly what happens right below. There's this line of code, loggerFactory. AddConsole, what that statement does is add a console logger to the logging system. This means that any logging statement made through this system, typically via an iLogger implementing instance, is logged to the console. What we want to do is add another provider. To do that, there's an addProvider method on the logger factory. But there are shortcuts to this. AddConsole will log to the console window, and that's already there. What I like to do is log statements to the debug window, which is quite convenient when developing. We can do that by calling AddDebug on the loggerFactory. AddDebug is part of the Micorosft. extensions. logging. debug package.

When calling into addDebug, we can choose the minimum log level, and there's a few of those.When calling into addDebug, we can choose the minimum log level, and there's a few of those. Trace is used for the most detailed log messages, which are typically only valuable to a developer debugging an issue. Debug is used for messages that have short-term usefulness during development. They contain information that may be useful for debugging, but it doesn't have any longterm value. Information-level messages are used to track the general flow of an application. These logs do have some longterm value. The warning level should be used for abnormal or unexpected events in the application flow. So this may include errors or other conditions that do not cause the application to stop, but which do need to be investigated further in the future. Errors should be logged when the current flow of the application must stop due to some failure, such as an exception that cannot be handled or recovered from. And lastly, critical, this log level should be reserved for unrecoverable application or system crashes, or catastrophic failure that requires immediate attention.

- Trace is used for the most detailed log messages, which are typically only valuable to a developer debugging an issue. Debug is used for messages that have short-term usefulness during development. They contain information that may be useful for debugging, but it doesn't have any longterm value.
- Information-level messages are used to track the general flow of an application. These logs do have some longterm value.
- Warning level should be used for abnormal or unexpected events in the application flow. So this may include errors or other conditions that do not cause the application to stop, but which do need to be investigated further in the future.
- Errors should be logged when the current flow of the application must stop due to some failure, such as an exception that cannot be handled or recovered from.
- Critical, this log level should be reserved for unrecoverable application or system crashes, or catastrophic failure that requires immediate attention.

The default is Information or higher.

Adding the debug output logger or console output logger isn't required when you're targeting ASP. NET Core 2.

We stated that if an exception happens, we want to return a 500 Internal Server Error response with a generic error message. But by doing that we lose our exception, it shouldn't be sent to the consumer of the API, as we learned, but we do want to know about this, so we need to log it. So first let's get that actual exception. Through context. Features, we can get collection of HTTP features provided by the server and middleware available on this request. And what we want is the iExceptionHandler feature. This one is defined in the Microsoft. ASP. NetCore. Diagnostic namespace. If it's found, we can look at the error property to get the actual exception. So with that, we've got the exception, now we just need to log it, and to log something, we need a logger instance. We can inject that through constructor injection in a class, which is what we'll do in the next demo, but in this part of the code, we can use the second possible approach. We've got a loggerFactory injected in the configure method. Through that loggerFactory, we can also create a logger. We do that by calling the createLogger methods on the loggerFactory instance, passing in a name for the logger. To effectively log, we have a few methods at our disposal on that logger, one for each level. So we've got logCritical, logDebug, logError, and so on. An exception like this, which can be handled, should be logged with logError. The first thing to pass in is an EventId, that's a numeric Id to associate with the log, which can be used to associate a series of logged events with one another. EventId should be static and specific to a particular kind of event that's being logged. This allows intelligent filtering and processing of log statements. In our case, the event we're logging is an unhandled exception in our application that will result in a 500 Internal Server error. So 500 sounds like a good Id for that. The EventId type can be implicitly cast to an int, so we can just pass in an int to this argument. Then we can pass in the actual exception. We get that through the exceptionHandlerFeature. error property. And lastly, we can pass in a message for which we can use exceptionHandlerFeature. error. message. And that should be it. Now we just need to throw an exception of course. Let's open the BooksController again, we're in the patch method, so let's just for the sake of this demo comment out the patchDoc. ApplyTo method that accepts the ModelState. If we just use patchDoc. ApplyTo passing in the Dto, we will get a 500 internal server error when we pass through an invalid patch document. Let's run this, we've got an invalid patch document here from the previous demo.

ow that's great and all, but logging exceptions through the debug output window is kind of already included by default when you're debugging. And it's still not very useful outside of development. What we really want to do is log this to a persistent store, like a file for example.

## Logging Errors and Other Information

we might want to log from other parts of the application like from our controllers. A good example might be deleting a book for an author. That's quite an unrecoverable thing to do, so maybe we'd want to log an informational message when that happens. We're in our BooksController, and the first thing we'll need to do is get a logger somehow. In the previous demo, we created one through a loggerFactory, but loggers can also be injected. If an application component doesn't need the factory, we shouldn't inject it and use it. So we'll inject the logger for the BooksController. iLogger is defined in the Microsoft. extensions. logging namespace. Let's add that using statement. The IOC container can directly provide us with an iLogger of T. When this technique is used, the logger will automatically use the types name as its category name. So that's why we inject an iLogger BooksController. Let's make it accessible through a private field, and there we go. Then let's scroll down to our deleteBookForAuthor action. When a book has been deleted, we want to log this. That's a piece of information, so we call the log information method on the logger. We pass in an eventId, which we can choose, and an informational message. Let's give that a try. We've got a delete statement here from a previous module that will delete a book for an author. Let's send it, we get back a 204 No Content, so far so good. Now let's have a look at the debug output window, and there's our informational message. So with that, we know how to log. But we're still just logging to the debug output window.

## Log to a File

When we're logging, we typically want to be able to look through logs afterwards, so we can see what went wrong. So that means we need a persistent store like a file or database. ASP. NET Core does not contain a built-in logger to a database or a file, but the built-in logging system was built in a way that allows third-party providers to easily integrate with it. We're at the GitHub page of the logging component. Let's scroll down a bit. At the moment of recording, there are providers for Serilog, elmah, Loggr, Nlog, and Graylog. Nlog is a provider that allows logging to a file, so we're going to use that one. The nice thing is that regardless of the logger provider we're using, the way to integrate them remains the same. So even though the configuration of Nlog is different than that of, for example, Loggr, the same principles are valid when integrating them in an application. Currently for Nlog on ASP. NET Core, it's advised to install the Nlog. web. aspnetcore package.

By default, it looks for a Nlog. config file in the root folder, so let's add one. We choose the XML. File template, and we name the file nlog. config. In this file, we'll want to configure Nlog so it logs messages to a file. This is purely Nlog related, and it can go pretty far. See on NLog wiki.

Let's save this file, and now we have to make sure that we copy this to the output directory on build, so Nlog can pick it up. To do that, we right-click the file and open the Properties. Build Action should be set to None, and Copy to Output Directory should be set to Copy always. Let's click OK, and now we can integrate this, and we already know how to do that. Let's open the Startup class. We're in the configure method, we can use the AddProvider method on the loggerFactory for this. This AddProvider method expects an instance of a class that implements iLoggerProvider. In the case of Nlog, that's the Nlog. extensions. logging. NLogLoggerProvider class. But there's an easier way. Most of these logging providers include a shortcut extension method, so we don't have to directly call AddProvider. In the case of Nlog, that's called AddNlog. It's defined in the Nlog. extensions. logging namespace, so let's add a using statement. And let's scroll down again, and there we go, we can now use the AddNlogExtension method. And that's it for ASP. NET Core 1. Let's switch to ASP. NET Core 2. We could just leave all of this as is, and it'll just work in ASP. NET Core 2, but there's two changes we can make to improve our project. The first one concerns the package version. From version 4. 5 onwards and log. web. aspnetcore can specifically target the. NET standard 2. By doing that, we require less other dependencies as version 2 has an extended API surface that's partially used by this new version of Nlog. Currently version 4. 5 is in prerelease mode, so make sure that Include prelease checkbox is checked. Then let's click Update. On to the second change, and that's code related. We added the NlogLogger provider in the configure method in ASP. NET Core 1, and that'll work, but as we learned, ASP. NET Core 2 allows us to add logging earlier on in the bootstrapping process. That allows us to log errors that happen before we hit the configure method. We're in the Program class. We can add Nlog by calling into UseNlog on the web host builder. That's an extension method defined in the Nlog. web namespace, so let's add a using statement to Nlog. web by pressing Enter. What this call does is register the Nlog services, so they can be used by the dependency injection system, which is exactly what we need to ensure our logger. log statements actually use Nlog as well. Every issue that happens now will be logged to a file, and that's earlier in the process than what we get with ASP. NET Core 1. And this is it, the nice part about the fact that these third-party providers integrate with the built-in logging system is that we don't have to change any other parts of our code. Our logging code will keep on working, it's now simply logging to an additional provider.

## Paging through collection resources

Most APIs, just like ours, expose collection resources. In our case, that's authors and books, and these collection resources can grow quite large. It's considered best practice to always implement paging on each resource collection or, at least, on those resources that can also be created. This is to avoid unintended negative effects on performance when the resource collection grows. Not having paging on the list of ten authors might be okay, but if our API allows creating authors, this list can grow, and we don't want to end up with accidentally returning thousands of authors in one response, as that will definitely have an adverse effect on performance. If you remember from the second module, where we designed the outer-facing contract, options like paging, sorting, filtering, and others are passed through via the query string. These are not resources in their own right. They're parameters that manipulate a resource collection that's returned. You can see an example of this on screen. As far as paging is concerned, the consumer should be able to choose the page number and page size, but that page size can cause problems as well. If we don't limit this, a consumer can pass through 100, 000 as page size, which will still cause performance issues. The page size itself should be checked against a specified value to see if it isn't too large, and if no paging parameters are provided, we should only return first page by default. Always page even if it isn't specifically asked for by the consumer. We're manipulating collection resources, but for things like paging and the other options we're looking through this module to work correctly and have a positive impact on performance, we need to ensure this goes all the way through to our data store. If we have thousands of authors in our database, and we first return all those authors from the repository to the controller and then page them, well, we're still fetching way too much data from the database. We're not really helping performance in that regard then, but how can this happen technically, with link and Entity Framework Core? Well, it can happen thanks to the principle named deferred execution.

![Paging](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Paging.png?raw=true)

## The principle of Deferred Execution

When working with Entity Framework Core, we use link to build our queries. With deferred execution, the query valuable itself never holds the query results, and only stores the query commands. Execution of the query is deferred until the query variable is iterated over. Deferred execution means that query execution occurs sometime after the query has been constructed. We can get this behavior by working with iQueryable implementing collections. iQueryable T allows us to execute a query against a specific data source, and while building upon it, it creates an expression tree. That's those query commands, but the query itself, isn't actually sent to the data store until iteration happens. Iteration can happen in different ways. One way is by using an iQueryable in a loop. Another, is by calling into something like ToList(), ToArray(), or ToDictionary() on it because that means converting the expression tree to an actual list of items. Another way is by calling singleton queries. Singleton queries are queries like average, count, and first, because, to get the count, or first item of an iQueryable, the list has to be iterated over. As long as we can avoid that, we can build our query by, for example, adding take and skip statements for paging and assure it's only executed after that.

![Paging Metadata](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Paging_Metadata.png?raw=true)

![Paging Metadata 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Paging_Metadata_2.png?raw=true)

![Paging Metadata 3](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Paging_Metadata_3.png?raw=true)

In this demo, we're going to implement paging functionality. As an example, we'll do this on our old collection. We're in the GetAuthors action on the authors controller. Here, we'll want to accept two parameters from the query string. Page number signifies the page we want to get, and page size signifies the maximum amount of items on the page. It's important to give these parameters default values. Like that, we can ensure that if a consumer doesn't pass in values for these parameters, the defaults will be used. We give the page number a default of one, and page size a default of 10. These parameters should be bound to values coming from the query string. To signify that, we can use the FromQuery attribute. Now, this isn't absolutely necessary in this case. Framework will look for parameters with matching names to bind automatically. In fact, if we look at the GetAuthor action, we see that it accepts an Id parameter, and that's coming from the route template. We could've used a FromRoute attribute to explicitly signify that, but it isn't necessary, as said. However, it is very useful when our parameter names don't match with what will be implemented in the query string. For example, if we want the consumer of the API to input page instead of page number, we can use from query attribute passing in page as the name. There's one more thing we have to think about, a maximum page size. We've got a default here, 10, but a consumer of an API can still input 1, 000, and that'll be accepted, so we'll add a constant that defines the maximum page size. Let's name it "max Author Page Size" and give it a value of 20. In the action, we then check if the page size isn't higher than the maximum page size, and if it is, we set it to the maximum page size. With that, we've got our parameters. These should be passed to the GetAuthors action on the repository.

We'll end up with a long list of action parameters, all coming from the query string. What I like to do is create a small helper class that holds all these parameters. Let's create such a class in our Helpers Directory. Let's name it Authors Resource Parameters. First, we'll add a page number property, keeping in mind to matching name. We'll give it our default value of one. Then, we'll add our constant. We need this to calculate the actual page size. This time, we can just name it maxPageSize, as we're already in a class that signifies through its name, that this will be used for authors. Then, there's the page size property. That's it for this class for now, but how do we now ensure that the query string input is bound to the properties in this class? Let's go back to our authors controller. The model binding framework is pretty smart. If you change the parameter list here to an authors resource parameters instance, it will look for matching property names inside that class. This is all there is to it. We can now also remove the constant and the calculation. Then, we want to pass this instance to our repository. We haven't got a method like that yet on our repository, nor on our contract, so we'll have to change both. Let's open the contract. Let's change the GetAuthors method so it now accepts an authors resource parameters instance as a parameter. Let's do the same in our implementation. GetAuthors will eventually return a paged, sorted, and filtered collection of entities. We're in the current implementation of this method, and we talked about deferred execution. We can actually already see that at work here. The current implementation asks for the authors on the context and applies a default order on them. Without deferred execution, that would mean all authors would be fetched from the database and only ordered after that. If you look at what authors actually is, let's go to the definition, we see that it's a DbSet, and if you look at the definition of that, we see that it implements iQueryable, so what context dot authors returns is an iQueryable, which we can add additional options to, and that's what we do by calling in to OrderBy. If we look at OrderBy, that actually returns an IOrderedQueryable, and that's, of course, also an iQueryable. Same story for ThenBy. It's only when ToList is called that the query is created and executed on our database. What we can do is apply paging before calling ToList. It's important to add the paging functionality last because we want to page on the sorted, filtered, and searched collection. If we add paging first, the OrderBy and author extension methods will execute on the small page instead of on all the data, and that would lead to incorrect results. What you want to do is first skip an amount of authors. The amount of authors we want to skip is the page size times the requested page number minus one. That'll ensure that, for example, if page two is requested, the amount of items on page one will be skipped. Then, we take the current requested page size.

## Returning Pagination Metadata

Let's think about what would be useful for the consumer of the API to know about a page set of data. At the very least, we should include URIs to request the previous and next pages, so the consumer doesn't have to construct those himself. Additional information, like total count or total amount of pages, is often included as well, just like page number and/or page size. How do we include this data in a response then? Well, what's often done, and you've probably already encountered this if you've worked with third party APIs, like Facebook's, is that paging information is included in the response body itself. Often with a metadata tag or paging info tag, but this isn't correct. If a consumer of the API requests a page of authors, with the application/json media type as value for the accept header, the API should return a JSON representation of the resource. An envelope that has a results field and metadata field, well, that doesn't match the media type we asked for. It's not a JSON representation of the authors collection; it's a different media type. Returning the paging info like this combined with application/json as media type, effectively breaks REST, as we are no longer adhering to the self-descriptive message constraint. That constraint states that each message must contain enough information on how to process it. Next to returning the wrong representation, the response will have content-type pattern with value application/json, which doesn't match the content of the response. In other words, we also don't tell the consumer how to process it. The consumer doesn't know how to interpret the response judging from the content-type. We will learn a lot more media types in the next module because, to be honest, application/json isn't such a good media type to ask for a header. But more on that later. When requesting application/json, paging information isn't part of the resource representation. It's metadata related to that resource. Metadata, well, that's something that's put in a response header. The consumer can empower that header to get that information. We should create a custom pagination header, like X-Pagination.

First of all, we'll need to generate that metadata. This is something we'd like to be able to reuse, and one way of doing that is by creating a custom list type that holds the data we need to generate the metadata from. Let's add a new class to the Helpers Folder, Page List. We'll make this a page list of T, and we just want to add a little bit of additional functionality to a list of T. We'll start with CurrentPage, and we'll give it a private setter, so it cannot be manipulated from outside this class because allowing that could lead to an invalid value for current page. We do the same for TotalPages. Two other properties that can be useful in the metadata are page size and total amount of items. Let's add those as well. Then, we'll want to know if there's a previous page or a next page. These are properties we can calculate from the CurrentPage and the TotalPages properties. HasPrevious should be true if the current page is larger than one. HasNext should be true if the currrent page is smaller than TotalPages. Now, we'll need a constructor for this. We'll except a list of T, items, the count, the page number, and the page size. From that, we can already set three of our properties. The total amount of pages can be calculated with Math. Ceiling. We divide count by the page size and call Math. Ceiling on it. Math. Ceiling returns the smallest integral value that is greater than or equal to the input, so if we have 101 records with a page size of 10, this will return 11. Lastly, we call AddRange, passing in the inputted items. AddRange is a method on list of T, which our page list derives from. This is adds all the items to the underlying list. Now, we won't directly call this constructor. Instead, we'll add a static method, create, that will create this page list for us. This allows us to call pagelist. create, passing an iQueryable, which is exactly what we get after applying the order by class in our repository. All casting and calculations required can then be done in this create method, rather than having to do that before creating the page list. The static method returns a page list of T, it accepts an iQueryable of T, the page number, and the page size, and that's all the information we have in our repository method. To be able to call the constructor from this method, we need to calculate a total amount of items and the correct page items. To get the actual page, we do the exact same calculations we did in the repository. Lastly, renew of the page list, passing in the items, count, page number, and page size. That's already it for our page list class. Now, let's use this. First, let's change the repository contract. The GetAuthors method should now return the page list of author, instead of an IEnumerable. Then, we do the same in the implementation. Now, we can call page list dot create, passing in the iQueryable right up to the skip statement. Let's just put that in an in between variable, and then we return, page list dot create, passing in the collection before paging, the page number, and the page size. Okay, so now we have a collection in our controller that contains all the data we need to create a metadata from. Back to our controller. The authorsFromRepo variable is now a page list of author. What we want to do is generate URIs to the previous and next page. If you've worked with the old ASP. NET web API, you know there used to be a class that would help with that, the URL Helper. A comparable class exists in ASP. NET Core, URL Helper, which implements IUrlHelper. We'll first have to register this on the container, so we can inject it into our controller. On to the configured services method on the startup class. Let's register the IUrlHelper with its implementing type, UrlHelper. IUrlHelper is defined in the Microsoft. AspNetCore. Mvc name space. We use AddScoped, so an instance is created once per request. This isn't sufficient. A URL Helper will generate URIs to an action, and to be able to do that, it requires the context in which the action runs. In ASP. NET Core, that's accessed through an action context accessor. We'll have to register that service as well. These are defined in the Microsoft. AspNetCore. Mvc. Infrastructure name space. We register it before we registered the URL Helper because the URL Helper will use this, and this time we use AddSingleton because we only want to create is the first time it's requested. If you use transient or scoped here, the action context will still be null. Then, let's make sure our URL Helper has access to it. Instead of just registering an instance of URL Helper, we're going to tell the container how it should be constructed. To do that, we pass in an action to construct it, an implementation factory. In that, we first get the action context, and we can do that by calling GetService and passing in IActionContextAccessor. That'll give us an instance of action context accessor, which has the action context as a property, and then we return a new URL Helper, passing in that action context. That should be it. Now, let's inject our URL Helper in our controller. There we go. Let's scroll to the GetAuthors action. We want to refer to the GetAuthors action, so let's give the route a name first. Then let's create our links. Let's add a private method for that, to help us with it. This one should accept the old resource parameter, as that's needed to generate URIs. It should also accept an enumeration, so we can pass in whatever we want to generate a previous or a next page link. Like that, we can reuse this create authors resource URI method. Let's add that enumeration to the Helpers Folder. It should have two values: previous page and next page. Back to the controller, and let me paste in the implementation, so we can run through it. To create a link to the previous page, we use URL Helpers link method. We pass in a route name, GetAuthors, and a set of values, and these values are the query string parameters, page number and page size, so we pass in a new object, and set the page number to the currently requested page minus one because we want to link to the previous page. Page size stays the same. Likewise, for the next page link. We use URL Helper to generate link, but this time, we set the page number to the current page plus one because we want to link to the next page. As a default, we simply return the link to the current page. These helper methods can now be used to generate links. Back to the GetAuthors action, here we can check if there's a previous page. We can do that by checking the HasPrevious Boolean on the page list, our authorsFromRepo variable. If there is a previous page, we call CreateAuthorsResourceUri, pass in the authorsResourceParameters, and the previous page as resource URI type because that's the type of link we want to create. If there is no previous page, the previous page link will be null. Then, we follow the same logic for creating the next page link. With that, we've got our previous page and next page links. Now, let's create a metadata. For that, we instantiate a new object, and we pass in the values we get from the page list, total count, page size, current page, and total pages, and next to that, we also pass in the previous and next page links we just generated. The last part of the story is adding this as a custom header to the response. We name the custom header X-Pagination, and as a value, we serialize the pagination metadata. For that, we use JSON. net, calling to SerializeObject on JsonConvert, passing in the pagination metadata object. That's it. Let's give this a try. Let's send that request to get one page five items again. We get back 200 okay, and we indeed see five authors in the list. Now, let's have a look at the headers. Here's our next pagination link. It contains the total count, the page size, the current page, and the total amount of pages. We also see that there is no previous page link, but there is a next page link.

## Filtering and Searching

Filtering a collection resource means limiting the collection resource, taking into account a predicate. For example, we want to return all authors where the genre matches horror, or fantasy. We're passing in the field name and the value we want that field to match, to make it part of the collection that will be returned. Searching goes beyond that. For searching, we don't pass in a field name that should match, we pass in a value to search for, and it's up to the API to decide which fields shall be searched for that value. Often, that's done with full-text search, and it's useful for cases where basic filtering isn't powerful enough. We might pass in search query, signified by search querying our example, and the text to search for, say, "king. " The API should then return all resources that have "king" as part of any string field. Filtering and searching are different. As far as the rest is concerned, two things are important. First of all, filter and search options are sent to the API via the query string. You can already see that on screen in the two examples. Secondly, and this is very important, for filtering, allow filtering only on fields that are part of the resource. For example, our author resource has a name field. That means that a filter should be on that name field, and not on fields like, "first name" or "last name" that aren't part of the resource. Those are part of the underlying entity, and that's of no importance to rest. That also means that depending on how we design the outer-facing contract, we can choose to offer less granularity in our filter. We can't filter on first name or last name in this case, only on name.

![Filtering and Searching](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Filtering_&_Searching.png?raw=true)

## Filtering Collection Resources

First thing to do is allow the consumer of the API to pass in value for genre through the query string, and that's why we created that authors resource parameters class. All we have to do now, is add an additional property to this. We'll leave this default value at null. String is a reference type, so that's what the default will be. By default, we don't want to apply a filter on genre to our resource collection. Then, we need to implement a filter, so on to the repository implementation. We're going to apply this to the collectionBeforePaging variable. First, we check if the (mumbles) genre is filled out. If it is, we trim it and lowercase it, so casing is ignored. This is a design decision, by the way. URIs are case sensitive, so you can choose to take the casing into account, if you wish. Once we've got the genre, we use a where class to filter on this value. We see there's a red line here. That's because we're assigning the result of. Where, which is an iQueryable, to the collectionBeforePaging, and collectionBeforePaging, is an IOrderedQueryable because we apply order by and then by first. We can easily fix that by casting it to a queryable. That's it already, save for one thing. Let's open the authors controller again. We're generating previous and next page links. It's important that these links contain the filter as well. Let's open that CreateAuthorsResourceUri helper method. Here's the actual code to generate the next and previous page links. If we go to the next or previous page, we must keep that filter into account, as otherwise, we end up with a page of data that comes from a different, non-filtered collection. Let's add Genre to the object that's used by the URL Helper to generate the link. We do that for the previous page, for the next page, and for the default link to itself.

## Searching Through Collection Resources

First, we need to ensure the search query can be passed in. That means, adding a search query property of type string to the authors resource parameters class. On to the repository. How search is implemented depends on your architecture or requirements. Often, this is a full-text search with the help of full-text search components, like Lucene, but it can also be much simpler. In our case, we'll use a where class again. This will return any author for which the genre first name or last name contains the search query. You can keep on adding where classes. This is, again, deferred execution at work. That's all I need for the changes to our repository. Just as with filtering, the next thing we need to ensure is that the next and previous page links are authored. We're back in the create authors resource URI Helper method. This time, we ensure that the search query is passed through. That's the previous page, and we do the same for the next page, and for the link to itself.

## Sorting Collection Resources

A simple sample would be OrderBy=Age. That's along lines of what we know. One value for an OrderBy class. We immediately notice a likeness with filtering. Sorting statements relate to the fields of the resource and not on whatever lies beneath that layer. Let's have a look at another sample. Here, we're ordering by Age, but descending, and well, that's different, because we're now also stating how we should order. Only one OrderBy class, but in our code we have to take into account that part of the string is about a field and another part is about the order, order direction. And we can go beyond that. In this example, we're ordering by Age, descending, and then by Name, because sorting typically doesn't stop after the collection is sorted on one field. That's another thing we have to take into account when implementing this.

![Sorting](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Sorting.png?raw=true)

Put in our Authors controller. To GetAuthors action, you must be able to accept an OrderBy query string parameter. So let's make sure we can accept that. All we have to do for that is add it to our AuthorsResourceParameters class. Currently, we're ordering by FirstName and LastName by default, which translates to Name on the AuthorDTO. So let's put that as the default value. Then we should apply the requested order. Let's open the repository. We gave the OrderBy parameter a default value of Name. So we should actually replace that with the OrderBy and ThenBy class we see here. And that means we needs a mapping from the property names on the DTO to the property names on the entities. That's part one. And secondly, we'll need to be able to apply the sorting. But what we get are strings. Let's have a look at that OrderBy class. If you look through the overloads, we see there is no overload that allows us to pass in a string. And we really want to avoid having to write a huge switch statement for all possible sorting combinations. Now, luckily, this isn't a new requirement. In fact, it's for these types of requirements that Microsoft created the System. Linq. Dynamic library. Through that library, we are able to sort on strings. So let's add that package. So we want to search for System. Linq. Dynamic. Core, and let's install this. And there we go. Okay, so now we should be able to sort by string. But doing all that in this repository method won't exactly lead to reusability. We can separate it out into an extension method on IQueryable. So what you want to end up with is a sort of ApplySort extension method on IQueryable that accepts the OrderBy string from our query string parameters, and a mapping dictionary.

## Creating a Property Mapping Service

So we can have sorting on multiple fields of the resource, but one resource field might map to multiple properties on the entity. For example, the Name property of the AuthorDTO is concatenation of FirstName and LastName on the entity. Moreover, we allow sorting, ascending and descending, and in the case of Age, that'll be an issue. The higher your Age, the lower your DateOfBirth. So in that case, the passed in sort order should be reversed. We could just create a hardcoded list of mappings, but let's make this reusable. We're going to create a propertyMappingService and register it with the ASP. Net Core's built-in container. Now, how do we do that? We'll need a propertyMappingService class. It implements an interface, IPropertyMappingService, that allows us to register it on the container on interface instead of directly on implementation. The service holds a list of PropertyMappings like one that maps to properties we can sort on of an AuthorDTO to the properties on the underlying entity. One such property mapping from a sorted destination then holds all the mappings from, for example, the AuthorDTO fields to the Author entity fields. And that's a dictionary. A simple one could be ID maps to ID, but as we know, that's not sufficient. One field of the resource might map to multiple fields on the entity, like Name to FirstName and LastName. So the dictionary should have the field name on the resources key but another classes value, a PropertyMappingValue. That PropertyMappingValue should then contain a list of destination properties. With that, we can map the Name field to FirstName and LastName. And we'll need something else. As we mentioned in case of Age, the sort order should be reversed, as when we sort ascending by Age, that actually means we sort descending by DateOfBirth. So our PropertyMappingValue should also have a Revert property. And with that, we have all the information we need for our mappings. So on the PropertyMapping service, we should then create a method to get a specific mapping: GetPropertyMapping from source to destination. For example, from AuthorDTO to the Author entity. It's a bit of work, but in the long run, this really helps toward reusability.

![Property Mapping Service](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Property_Mapping_Service.png?raw=true)

![Property Mapping Service 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Property_Mapping_Service_2.png?raw=true)

we'll create the propertyMappingService we just looked into. Let's start by adding a propertyMappingService class to the services folder. We know it has to contain a list of property mappings. Such a property mapping contains a dictionary of string PropertyMappingValue. So let's add a PropertyMappingValue class next. This one contains an IEnumerable of string. The destination properties, one resource property will map to, and next to that, a Revert Boolean allowing us to revert the sort order if needed. Back to our propertyMappingService. Let me paste in the mappings we'll use from an AuthorDTO to an Author entity. Of importance here is that we can now map Age to DateOfBirth using reverse ordering and Name to FirstName and LastName. Then let's add that method to get one property mapping. Through this, we'll be able to ask for a mapping from a source type to a destination type, like a mapping from AuthorDTO to Author. But we haven't yet got a class to hold such a property mapping. So let's add that. And to it we add one property: a mapping dictionary of string PropertyMappingValue. And that already takes care of our three class. So let's go back to the propertyMappingService. We can now add an IList of property mapping from Tsource to Tdestination. But we run into a bit of a problem. Tsource and Tdestination can't be resolved. That can be overcome by using a marker interface. That's an interface without any methods in it, and it's often used for cases like this. If we let our property mapping class implement the marker interface, we can add an IList of that marker interface. So let's add one. IPropertyMapping looks like a good name. So it's just an empty interface, and now we let our property mapping implement this interface. And now we can register a list of IPropertyMapping in our propertyMappingService. We're getting there, right? In the constructor we can then add our property mapping to this list as a mapping from the AuthorDTO to the Author entity. And all that's left in this class is implementing the GetPropertyMapping methods. First, we find the matching mapping by searching our list of IPropertyMapping. For that, we can use the OfType method. If one is found, we can return to mapping dictionary. And otherwise, we throw an exception. And that's it for this class. Now let's create an interface so we can register it as an interface implementing class on the container. Let's name that one IPropertyMappingService. And it only has one method, GetPropertyMapping. Our propertyMappingService class should then implement this IPropertyMappingService interface, and that's it. Now we can register this on the container. And we know how to do that. Let's open the ConfigureServices method on the startup class. Transient is lifetime advised by the ASP. Net Core team for lightweight, stateless services. So, let's register it as such.

We'll start by injecting our newly created propertyMappingService into our library repository. So let's open that. Let's use constructor injection and that should be it. With that, we can get our Author property mapping dictionary in the GetAuthors method. Now we just have to write that ApplySort extension method. So let's add another new class: IQueryableExtensions. Let's make it static, because we'll use this class to add an extension method to. And let's add a static ApplySort method. It's an extension method to an IQueryable of T, and it an accepts an OrderBy string and a mapping dictionary. As we're going to be using Dynamic LINQ for this, let's add a using statement. And the implementation is quite a lot of type, so let me paste that in and then we can run through it. Let's have a look. First, we check the input parameters. If those are null, we throw argument null exceptions. If there is no OrderBy string inputted, we can simply return the source. The OrderBy string is separated by commas, and we need each part individually. So we call split on OrderBy to get those parts. And what we then want to do is apply each OrderBy class, but we need to do this in reverse order, because otherwise the IQueryable will be ordered exactly the wrong way. An OrderBy class might contain trailing or leading spaces, and we don't want that, so we trim it. And as we know, we can also input the sort order, ascending or descending. If desc is inputted, we should order descending, and otherwise, ascending. Then we remove those parts of the string from the OrderBy class, because then we need a property name to look for in the mapping dictionary. For example, Name. We look through the mapping dictionary and look for a key with that property name. There should be one there, otherwise we cannot sort. So if it's not found, we throw an exception. If it is found, we fetch it, and then we can use the destination properties from the PropertyMappingValue. For example, if the inputted OrderBy class contained Name, the destination properties will be FirstName and LastName. As you remember, there's a possibility that we have to revert the sort order in case of Age and DateOfBirth, for example. So we check for that value on the PropertyMappingValue as well. And then we use Dynamic LINQ to generate our OrderBy class. For that, we call OrderBy passing into destination property as a string, combined with either ascending or descending, and that's it for the ApplySort extension method. If you have a look at our library repository now, we can see all the errors are gone. So that should be it, save, of course, for one thing. Just as in the previous demos, sorting can be combined with paging, filtering, and searching, so we have to change the next and previous page links. They should contain the requested sorting order. That's in our Authors controller, in the create Authors resource URI method. We extend the object by passing in the OrderBy class that was inputted. We do that for previous page, next page, and the link to itself. Okay, that was lot of typing, and quite a bit of work, but now we should really have something reusable.

## Taking Consumer Errors into Account When Sorting

So we're back in Visual Studio. What we want to do is add a method that checks for invalid input. We can then use that result in the GetAuthors action to return the correct status code. So when is an OrderBy invalid due to a consumer mistake? Well, we already decided on that before. It's invalid when a consumer asks to order on a property for which no key in the dictionary exists. Let's open our propertyMappingService. Here we defined our list of Author property mappings. But this doesn't mean that, if that checks out, that other issues might not arise. For example, you might introduce an unwilling bug like make mistake in a mapping, or provide a wrong mappings to the ApplySort action. That can result in an exception, or it might pass without us noticing. It can be a bug. But those are mistakes of the server, or rather, of us, the developers, and not of the consumer of the API. So we need to check if, for a given string of fields like our OrderBy string, there is a valid mapping dictionary that contains mappings for all those fields. So, this propertyMappingService sounds like a good place to add such a method to. Let's name it ValidMappingExistsFor. It returns a Boolean that will be true if all checks out. Let me paste in the implementation so we can run through it. We get the mapping dictionary by calling in to GetPropertyMapping. Pass in the source and the destination types, Tsource and Tdestination. If no field is inputted, we can safely return true. But if there are fields inputted, we should run through them. We split the inputted fields and run through them, and we trim them so leading and trailing spaces are ignored. We also remove everything after the first space. That'll make sure that, if the fields are coming from an OrderBy string, asc and desc are ignored. Then we look for the property mapping with that field as key in our mapping dictionary. And if the key isn't found in the dictionary, we return false. If all checks out, we can safely return true. And that's it for this method. Let's add it to the IPropertyMappingService interface, so we can access it through that interface. Now we should be able to call ValidMappingExistsFor from wherever we inject an IPropertyMappingService implementing class. So let's go to the Authors controller. This is where we'll want to check if the OrderBy string is valid. We want to do that in the GetAuthor section so we can return a bad request if it isn't. That means we need to inject the propertyMappingService in our controller. And this is one of the reasons we separated all of this out into a class of its own, a propertyMappingService. It really helps with reuse of our code. And there we go, we now have access to a propertyMappingService instance. In the GetAuthors action, we can now call into the propertyMappingService's ValidMappingExistsFor method, passing in the source and destination type we want to check a mapping for, and the OrderBy class from the AuthorsResourceParameters. If it doesn't check it out, we return a bad request.

## Data Shapping or Shaping Resources

The word kind of gives away what it does. This principle allows the consumer of the API to choose the fields of the resource that have to be returned. So rather than returning all properties of say, an author, a consumer of an API might only need to know the ID and the Name. Data shaping allows for this by looking at the fields query string parameter, of which the value is a comma separated list of field Names. Implementing functionality like this can be very good for performance. In case of our authors, the impact will be quite small, but just imagine a resource with thirty properties of which the consumer needs a collection of fifty, but he only needs two fields. Without data shaping, 1, 500 property values would be sent over to the consumer. And with data shaping, only 100 will be sent, and that does have a noticeable impact. What's important here, just as for filtering and sorting, is that this field level selection is at level of the resource.

![Shapping Resources](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Shapping_Resources.png?raw=true)

First, let's have a look at what's returned from our GetAuthors action. We see that it's an IEnumerable of AuthorDTO. But with data shaping, we no longer want to return an IEnumerable of AuthorDTO. We want to return an IEnumerable of a new object, one that only contains the requested fields. We need a way to dynamically create an object that runtime starting from our AuthorDTO. So we're going to work with dynamically typed objects in this demo. This is where the ExpandoObject comes in. ExpandoObject represents an object whose members can be dynamically added and removed at runtime. The extension method we want to create, it doesn't matter to that extends. IEnumerable of T accepts our fields and returns an IEnumerable of dynamically typed objects, ExpandoObjects that only contain the fields we requested. So let's create a new class to hold this method, IEnumerableExtensions. Our method should return an IEnumerable of ExpandoObject. It should work on an IEnumerable of a specific type, Tsource, and it should accept the fields. Let me paste in the implementation, as it's a lot to type, so we can run through it. Let's import Name spaces we need. We're going to use bit of reflection, so we'll need a using statement for System. Reflection, and let's run through it. So the first part of this method is much like our ApplySort method. We check the source parameter, and if it's null, we return an argument null exception. We're going to return a list of ExpandoObjects, so we create a list to hold these. What we do next is create a list with PropertyInfo objects on Tsource. We're going to be using reflection to get PropertyInfo objects, which allow us to access property values. But reflection is quite expensive, so rather than doing it for each object in the list, we do it once and reuse the results. After all, getting the PropertyInfo objects is done on the type of the object, Tsource, and not on the instance. These properties we need to return depend on the Fields parameter. If it's empty, we want to return a list of ExpandoObject with all the public fields. So, we use a bit of reflection on the type of Tsource. GetProperties will return all properties on Tsource. We want to return all public instance properties. Private properties and static properties shouldn't be returned. That call returns an array of PropertyInfo objects, and these we put in the PropertyInfo list. If fields are passed in, you have a bit more work. You must run through that. So we split them up, and run through them with a foreach loop. In that loop, we trim the field name so leading and trailing spaces are ignored. To get a property value we can use reflection as before. This time, we call into GetProperty on the type of the source object. In that GetProperty method, we pass in the property Name. And this will return PropertyInfo object that contains information on that property. We want to ignore casing of the field name and return only public and instance properties, just as before. We check if it is found, and if it is, we add it to a list of PropertyInfo objects. Our PropertyInfo list. So now we have a correct list of PropertyInfo objects. In both cases, when fields are inputted or when no fields are inputted. Up next is creating the ExpandoObjects themselves, and we need to do that for each object in the source list. So we run through our source list. For each object in the source list, we new up an ExpandoObject. We need to get the value of each property from the list of requested properties on the source object. And we have that list as a list of PropertyInfo objects, so we run through that. We can get the actual value of a property by calling into GetValue on the PropertyInfo object, passing in our source object. And now we need to add this property to our dynamic object. An ExpandoObject, underneath the covers, uses a dictionary of string object to hold its properties. We can cast it with that, and then add our property. The key of the dictionary holds the property name, while the value of the dictionary holds the property value. And with that we have our new data shaped object. We added to the list that must be returned, and then we returned the list. And that's it for our data shaping functionality.

The first thing we need, and this is becoming predictable, is a way to pass the fields we want to return to our API. So let's add a string parameter fields to the AuthorsResourceParameters class. Let's open the GetAuthors action on our Authors controller. Right before our IEnumerable of AuthorDTO is returned, we call into our newly created ShapeData methods passing through the fields parameter to shape our results. One more thing we need to do is make sure we return a bad request from our GetAuthors action when a consumer of our API inputs an invalid field name. Theoretically, we can reuse the check we previously wrote for checking the OrderBy class. However, that wouldn't be correct. It'll work because we stated that we need a mapping for each property. So for all our fields on the DTO, mappings will exist, but our data shaping component doesn't use that mapping dictionary. So if you have a resource collection that supports data shaping, but not ordering, well, we're out of luck. And it's also not a good separation of concerns. Thus, it's better to keep these separate. What we need to check is that, for a given string of fields, all properties matching those fields exist on a given type. So that sounds like something we want to create a service for. Like that, it'll be reusable and injectable wherever we need it. Let's name it TypeHelperService. We add one method to it, TypeHasProperties, which returns a Boolean and works on a given type, D. So how does the implementation look? Well, it's much like our ShapeData method, so let me paste that in. We're again using some reflection, so we need a using statement to System. Reflection. If no fields are passed in we can safely return true. If fields are passed in, we split the field string and run through all the fields. Each of them is trimmed to get the property name. And then we use GetProperty to check if a property with that property name can be found. If one can't be found, we return false. And if all checks out, we return true. And that's it for that method. Now let's define it on the contract and interface. Let's name that one ITypeHelperService. It's an interface with one methods, TypeHasProperties. And then let's have our TypeHelperService implement this interface. Like this, we can register it by interface on the container. And as we know by now, that's done in the ConfigureServices method on the startup class. Just as our property mapping service, this too is a lightweight and stateless service. So Transient is a good lifetime for this. Back to our controller, where we can now inject this service. In the GetAuthors method, we call into the TypeHasProperties method, passing in the AuthorDTO's type and fields as parameters. If it doesn't check out, we return a bad request. One thing left, and you know what's coming. We have to ensure that our previous and next page links also contain the fields parameter as query string. And we do that for the next page and the link to itself as well. And that should be it. We already applied ShapeData to the Authors we returned from our GetAuthors action.

## Camel-casing Dictionaries When Serializing to JSON

Our field names are no longer CamelCased. In this demo, we'll solve that. They are no longer Camel-cased because the default contract resolver that serializes the output doesn't correctly work with the dictionary. The key is serialized as is instead of being CamelCased, and as an ExpandoObject uses a dictionary underneath the covers, where the key is the property name, this is the result. But, no worries, we can change that by using another contract resolver. Let's open the ConfigureServices method again on our startup class. By calling into AddJsonOptions, we can configure the options for serializing to and from JSON. The options object has a SerializerSettings property, and on that SerializerSettings property, there's a contract resolver property. We can set this to any contract resolver we want to use. What we want to use is the CamelCase property names contract resolver from Json. net that's found in the Newtonsoft. Json. Serialization name space. All right, let's give that another try. Let's send that same request again. This time, the field names again start with a lowercase letter, as expected. And with that, we know how to implement data shaping, including a fix for our serialization issue. It's perfectly valid to use this on a single resource as well.

## Shaping a Single Resource

We're in the GetAuthor action on the Authors controller. The first thing we need to do is allow passing in the fields we want to see returned. We don't need a containing class like AuthorsResourceParameters this time. There's only one that can be passed through, and that's fields. And now we need another extension method. This time, one to shape one object and not an IEnumerable of objects. So let's add a class, ObjectExtensions. We make this static as it will contain an extension method. And let me paste that one in. It'll look fairly familiar. We're again using some reflection, so we need the using System. Reflection statement. And it's a ShapeData method that returns one ExpandoObject instead of an IEnumerable of them, so we also need to import System. Dynamic. What we do here is almost exactly the same as what we did for an IEnumerable in the previous demo, save for the fact that we don't run through a list. So instead of repeating all that, let's focus on why we use two different methods instead of only creating one. This one an object, and reusing this one for all the items in a list in the previous demo. The reason is performance. As explained when we implemented the ShapeData extension method on IEnumerable of Tsource, reflection is expensive. So in IEnumerable of Tsource, we get around that by saving a list of PropertyInfo objects and we reuse them for each object in the list. Were we to use ShapeData extension method on object, for each object in the list of Authors, we would fetch the PropertyInfo, so reflection, for each object in the list of Authors. By creating two extension methods, we avoid this performance impact. Okay, let's get back to the GetAuthor action on our Authors controller. The first thing we have to do is check if the fields are valid. We already have a service for that, our TypeHelperService. If this doesn't check out, we return a bad request. And then right before returning an okay, we call into ShapeData on our Author object. We pass in the fields parameter.

## Exploring Additional Options

Let's explore a few additional options. We ended with data shaping, but that same principle can also be used to automatically expand or include child resources. One option would be to declare an additional expand parameter, and as a value, the child property you want to see included. The books for an author, in this case. Now those expanded resources can also be shaped. In the example on screen, we don't use a new expand parameter. Rather than that, we expand the possible options for the fields parameter. If it includes a field of books, it should automatically include the books collection. And something else that's sometimes seen on APIs is using more complex filters like a contains filter. The query in this example should contain all Authors with a genre that contains the word horror, and not only exact matches. The more of this we allow, the less we might need searching functionality and so on. If you want to implement functionality like this, you can build upon what we did in this module. The principles, by now, should be quite clear. But think well about how you want your API to be used. There's no need to implement all of this functionality if it's not needed. Just remember, pause the parameters, fire a query string, and only use fields that are part of the resource, not of objects related to lower level layers.

![Additional Options](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Additional_Options.png?raw=true)

![Additional Options 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Additional_Options_2.png?raw=true)

## Hypermedia as the Engine of Application State

Currently, our APIs already quite good but it's not exactly evolvable nor is it RESTful. HATEOAS will help with this. It makes APIs evolvable and even self-describing. Hypermedia like links drives how to consume and use the API. It tells the consumer how it can interact with the API. Can I delete this resource? Can I update it? How can I create it? And how can I access the next page of data? And so on. But, what problem does it solve? Well, imagine we want to allow the consumer of the API to update or delete a book. Currently it does not know how to do that by looking at the response body. It has to have some intrinsic knowledge of how our API contract is structured. From contractor documentation, the consumer can learn that he'd have to send a book or batch request to API, authors, author ID, books, book ID, to update a book with a specific payload. And delete request to delete it. But it cannot get this information from the response itself. The consumer has to know this in advance. Now imagine another case. Let's say we have an additional field, NumberOfCopiesInLibrary on our book. If there are still copies in our library, somebody can reserve a copy. This is obviously an optimistic example, set in the hopefully near future, as "The Winds of Winter" hasn't been released yet. Anyway, in this case, the consumer of the API will have to know it has to inspect that field before it can send a post request to, say, api/bookReservations'. And that's again intrinsic knowledge of how the API works. But say this changes. Say an additional rule is implemented. There have to be copies left, but if the user wants to make a reservation for a book that's marked with content mature, he must be older than 16. This rule will effectively break the consumers of the API. They have to implement that on their side as well. In short, if an API doesn't support HATEOAS the consumers of the API have to know too much. And if the API evolves, this might break the consuming applications because the assumptions made by those applications can become invalid. And that's the issue that HATEOAS solves. A lot of these assumptions can be thrown overboard by including links in the response that tell the consumer which action on the API is possible. And those links, well, that's hypermedia. So HATEOAS allows the API to truly evolve without breaking consuming applications. And that in the end results in self-discoverable APIs. So in our example, we could add an additional property, links to the response. And the client would just have to inspect these links. For example, the first link would be a link to get to resource itself. I abbreviated it a bit in the code sample for readability, but it must be a true link, so with the correct host and Guids for the author and the book ID. To update a book, you fool with put and partially with batch, links are added as well, if that functionality is offered by the API. And if deleting a book is allowed, we add link with URI and meta to delete a book. And if a reservation link appears, a reservation can be made for this book. So it's up to the server to decide whether or not to show this link. The consumer needs no knowledge about that business rule. And moreover, the rule can change without having to redo anything at the client level. If all of a sudden we expand the rule, stating that there must be copies left and that the user must be over 16 to reserve a book marked with content mature, this is no longer something that has to be checked by the client. The server would simply not include that link to make the reservation and the client only has to inspect the link to say, show a button to make the reservation. Let's have a look at another Roy Fielding quote, the guy who invented REST. "You can't have evolvability if clients have their controls baked into their design at deployment. Controls have to be learned on the fly. And that's what hypermedia enables. " And this quote does go quite far. If we match this to building applications, we're almost talking about self-generating client user interfaces. Most apps don't go that far. But, as the example we used teaches us, for things like rules that change, this is pretty great. And, additional pieces of functionality can be added. For example, marking a book as one of your favorite books. In that case, an additional link will be provided and this will not break existing client applications. But, from that moment on, they can implement this functionality starting from that link. We'll get back to this sample later on. If you think about all of this, this is actually nothing more than how we should work with the HTTP protocol. As we know, RESTy folks, any much of how a good web app should work. Well, on the app, it doesn't really matter if a link changes, we start at De Tijd, where I have a newspaper site for example. And from that we navigate to an article by clicking a link or submitting a comment to a forum. And that's two examples of hypermedia driving application state. And, if the server decides that the link should change, well, to request that we turn to route page, we'll contain that new link. The browser, which is our application in this context, does not break because that link changed. Instead, it's the hypermedia in the response that's used by the browser to show us what we can do next. Okay, now what do these links look like? JSON or XML don't have a notion on how to represent links. But HTML does, the anchor element. In HTML, href contains the URI, rel describes how the link relates to the resource, and type, which is optional, describes the media type. For supporting HATEOAS, this format is often expanded upon. Let's have a look at one of the links from our previous example again. They all follow the same principles. The method property defines the method to use. Rel defines a type of action. This is what clients look for in the links list. Href contains the URI to be invoked to execute this action. The client simply uses this link, he doesn't have to create it anymore. Mind you, HATEOAS does not completely lift a client of having to have some knowledge of what to expect. It still needs to know about the link types that can come back. In other words, the rel values and whether or not it wants to use them. But if we're no longer hard coding URIs and assumptions in the client, a URI or assumption change no longer breaks the client. Now, if this were a collection resource like our authors resource, this is also where we'd include the pagination links. So they're no longer in the pagination header, they're now in the links area on our response. For collection resources, we will need some sort of envelope, an object to hold the value, which has the list of authors and the links. Were we to just add an area of authors and a links property, which is an area of links, we'd have invalid JSON. And this might be the moment when you're thinking, Wait a minute, this does not make sense. When we learned about paging, we learned that we couldn't use an object like this because that would break the self-descriptive message constraint. After all, when requesting the authors with media type application JSON, the representation should be an area of authors. This isn't truly RESTful, but that can be fixed.

![HATEOAS](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS.png?raw=true)

![HATEOAS 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_2.png?raw=true)

![HATEOAS 3](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_3.png?raw=true)

![HATEOAS 4](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_4.png?raw=true)

![HATEOAS 5](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_5.png?raw=true)

![HATEOAS 6](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_6.png?raw=true)

![HATEOAS 7](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_7.png?raw=true)

![HATEOAS 8](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_8.png?raw=true)

![HATEOAS 9](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_9.png?raw=true)

![HATEOAS 10](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/HATEOAS_10.png?raw=true)

## Supporting HATEOAS

We'll change our API so it adheres to the hypermedia as the engine of application state constraint. But, how are we going to implement this? The logic for creating the links can't just be automated, as it will depend on the business rules. For example, a book might contain links to update or delete it but also a link that's supposed to book reservations. Which only appears when some rules are met. We can't just guess what these links might be for each resource. We'll have to code this ourselves. That said, what remains a fact is that we have to add the links to the output. So, there should be a links property on each representation we're sending back to the consumer of the API. Essentially, there's two approaches I often see used for this. And, coincidence or not, we've got a case for both. The first one, a statically typed approach, involves working with base and wrapper classes. If you think about our books controller, the get book for alter action returns a list of BookDto. We can ensure that that BookDto class inherits a class that contains links. So they can be serialized. And get books for author returns a list of books, so that's an action for which we'll have to wrap the results in a containing class so we can include the links. Second approach is a dynamically typed approach. It involves working with anonymous types and dynamics. For example, an ExpandoObject. If you think back about what we did with the get author action on our authors controller, we remember that it no longer works with the author Dto. It works with the ExpandoObject because we shaped the data before we return it. And that's a dynamically typed class. As it's an ExpandoObject, we can add links to it. For collection resources, we can wrap that in anonymous type.

![Supporting HATEOAS](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Supporting_HATEOAS.png?raw=true)

![Supporting HATEOAS 2](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Supporting_HATEOAS_2.png?raw=true)

## Supporting HATEOAS (Base and Wrapper Class Approach)

First, let's add a class for our link to the models folder. We'll name it LinkDto. It should contain three properties for the three values we talked about on the slides. Href, rel, and methods. We'll also give it a constructor, accepting three parameters for these three properties. So we can easily construct it. Then let's add another class. We'll name this one LinkedResourceBaseDto a base class for our linked resources. We make it abstract because we don't want this to be used on its own. Other classes should inherit it. We give it only one property: links, a List of LinkDto. Then let's have our BookDto class inherit this class. Now we can add a collection of links to the BookDto. So, on to our books controller. We're going to have to create these links. We'll need a URL helper for that. So let's inject it. And let's add a method to create these links for a given BookDto. It returns that BookDto after the links have been added. And let's name it CreateLinksForBook and as a parameter, it accepts a book Dto. The one we're going to add the links to. So the final statement will obviously be return book. Now this is where the custom logic would go. Here we decide on which links should be returned when a consumer of the API gets back a book or presentation. And the first link should be a link to itself, i. e. where we can get this book resource. And for the other links, we need to look at the functionality our API exposes. So let's collapse our codes to the definitions. When we return a single book, our API allows a full update, a partial update, and deleting of that book. So we need links to those in our link collection. To create URI through the URL helper, we have to be able to access the routes to these actions. So we need to give them names. And let's start with that. "GetBookForAuthor" to return a single book already has a name, so the next one is "DeleteBookForAuthor". Now we have "UpdateBookForAuthor" and "PartiallyUpdateBookForAuthor". But, what about creating a book? Well, we're covering that later on. Because creating a book doesn't really belong in the links related to a single book. For now, let's add the links. The first one is a link to itself. We call add on the links collection. A book now has that as it inherits the linked ResourceDto base class. To add a link, we must input a value for href. That is the URI to the action for the link. So we pass in a route name, "GetBookForAuthor" and we pass in an object containing the book ID. Then we need a value for rel. We can choose this ourselves, self in this case. This is the part that consumer or client will still have to know about. Because it's this that will be used by the consumer to see if a specific piece of functionality is offered by the API. And lastly, we input the method, 'GET'. Let me paste in the others. For delete, we need the link to the delete book for author action, passing in the ID. We'll give this a rel value of delete_book and the method, DELETE. To update the book, we need a link to the update book for author action. Also, needing the ID of the book. We name this update_book and the method is PUT. And same logic applies for PATCH, but this time we name it partially update book with underscores. Other links can be created depending on additional functionality and business rules. But for the book resource on our API, this is sufficient. Then, let's ensure these links are effectively created. And this should be done whenever a BookDto is returned. So that's when we return a list of books, one book, and right after we create a book. Let's start with amending the action we use when returning one book. That's to get book for author action. Instead of directly returning a book for author, we first create links for this book. And what we return now is not just a book, but it's the book including those links. And the same goes for post, ie; when we create a book for an author. Instead of directly returning the book to return, we first call in to create links for book. And we have one left, returning a book collection, that's to get books for author methods. Here we'll need to create these links for each book in the books for author list. So we run through them and call CreateLinksforBook on each book. For that we can use the select extension method. Alright, that should be it, let's give this a try. First, let's GET one book for an author. These are the same requests we've used previously in this course. Let's send it, and we get back our book, "The Winds of Winter", but with an additional links property. And this links property contains the links we added. Let's try creating a book by sending a post request to the books resource. We're going to create "The Restaurant at the End of the Universe". That's a Douglas Adams book. We get back a 201 created and the body of our response also contains our links. But what about getting all the books for an author? Let's send the get request to the books resource. This should give us back all George R. R. Martin's books. And we see that we indeed get back "A Dance with Dragons", "A Game of Thrones", "The Winds of Winter", and that's all that's in our library. We also see that for each book, the links are included. So that's great. But what we're not getting back are the actions on the books resource itself. We only have actions available on each separate book. We can't just add a links property to this JSON because it would result in an invalid JSON. So we're going to need a wrapper class. Let's add a new class and name it LinkedCollectionResourceWrapperDto. We'll make it work on a specific type, and it should inherit LinkedResourceBaseDto. We do this because our LinkedResourceBaseDto is an abstract base class that contains the links property. And we also want to make sure that the type it works on is in itself a LinkedResourceBaseDto implementing class. We do this to ensure that the Dto we're returning a list from will also have the links property. Next to links, this one only has one property, value. This must be an ienumerable of T, in the constructor we allow passing in this ienumerable. In case of a list of books, this will be an IEnumerable of BookDto. Okay, on to the books controller. We already have a create links for book method, now let's add an additional one, create links for books. This one works on our new link collection ResourceWrapperDto for BookDto in this case, and it also returns itself. That's the same as in the create links for book methods. So here we want to add a link to itself. For that we need to be able to refer to the get books for author action. So let's give that one a name, and then we add a link to itself. We already know how to do that. In the get books for author action, we then create a wrapper instance. The value should be our books for author list. And instead of returning the books for author, we now return the wrapper but first we call into CreateLinksforBooks on that wrapper. This creates the self-link to the books resource. Let's give that another try. Now let's send the request again. And this time we see an envelope, the value now contains that list of books including the links for each book instance. And if we scroll down a bit, we see that we now also have a links field, containing a link to the books resource itself. And with that, we've implemented HATEOAS support for our books by using a base class and a wrapper class. But there was another option, one that's suited for when you're not working with Dto's but rather with dynamics or anonymous objects like what we get when data shaping our resources.

## Supporting HATEOAS (Dynamic Approach)

So this approach involves working with dynamics and anonymous types. And if we look at what we did with the get authors action, which is what you see onscreen now, we see that we no longer return a list of authorDto. The get authors action returns an ienumerable of ExpandoObject, the datashaped version of our authorDto. So we can't just extend this datashaped object with links property to a base class. For that, we'll use the anonymous type approach. But let's start with a simpler case, one author. That returns one ExpandoObject, a shaped authorDto. And what we want to do is add a property links with the list of links to the response body. So let's add a method to create these links first. Let's name it create links for author. It returns an IEnumerable of LinkDto. To create these links, we need the input parameters of the get author action. So Guid ID and field string. Let's add a variable to all the links, which is a List of LinkDto. In the end, we'll want to return this. Now, how do we create it? Well the first thing to create is the self-link. If the field string is null or wide space, we need a URI to the author resource. We can again use the URL helper for this. Likewise for when there is a fields value. The only thing that changes is that we now also pass through the fields to generate a link. Our API supports deleting a book, so that's another link we want to add. As we know, we need to be able to refer to the delete author route to create that URI. So let's give the delete author route a name. And then we can add a link. I'm pasting these in because this is exactly the same as what we did before. And this is where this is starting to shine. Once a client application has an author, we can decide we can now let the client application create a book for an author. So in the previous demo, I mentioned this was coming back. Well, this is where it is coming back. We are truly driving application state now. We're deciding on the functionality of the consumer. So we need to be able to refer to the route to the create book for author action. And that's on the books controller. So let's give it a name, create book for author sounds like the obvious name to choose. And back to our authors controller. And let's add the link. Next to this, we can also add information on how to navigate through our API. An author has a list of books. So we use the URL helper to create a link to the get books for author action. Like this, again, we're really driving application state through these links. And this is where HATEOAS shines. What's next is adding these links to the response body. Let's scroll up to the get author action. Let's create links by calling the create links for author method. We pass in the ID and the fields. Then we need to add the links. As we know, the shaped data action returns an ExpandoObject. And that's an IDictionary of string object. So we can use that knowledge. We call author dot shape data, passing in the fields. And we cast it to an IDictionary of string objects. And through this dictionary, we can then add a new key value pair. So let's add one with links as key and our generated links as the value. And then we return our linked resource to return. Before we test this, there's one more place where we return an author and that's after successful post, so let's scroll down to the create author action. We've got our author to return here. This is the object you want to add those links to, so let's generate them first. We pass in the ID and null for fields. There's no datashaping on post. Then we need to add these links to the response. We need an ExpandoObject for that. So we have to convert our AuthorDto to an ExpandoObject. And that's exactly what a shape data extension method does when we don't pass in fields. So let's call that. There had to be a good reason for including all of those checks when we created the shape data method, right? Well, this is one of them. We call add and pass in links as the key for the links property and the links list as value. And lastly, we want to return this linked resource, so we pass it in in the created add route action. This requires us to input the ID, we could just leave it at author to return dot ID, but it isn't exactly correct because we're passing in the ID of another object, even though it's in the same scope and in the same methods, I do prefer to keep the code as correct as possible. So we should pass in the ID from the linked resource to return. And we can get that because it's an ExpandoObject, which is a dictionary of string object, so we can use the indexer passing in the key, in this case ID, the property name. And there we go, let's give this a try. Let's get one author by sending a get request to a specific author. We get back George R. R. Martin, including the links to itself, the link to add a new book for George R. R. Martin, we may hope, and the link to his books. Now let's send a datashaped request, just to see if it still works. We are only selecting the ID and the name this time. And indeed, that works out as expected. We also see that the link to self now included fields equals ID, name. The nice thing is, with this approach, datashaping cannot ask to not include links. If you remember from the previous module, we briefly mentioned that datashaping could allow a user to omit the ID link or URI link, which would then violate the manipulation of resources by a presentation constraint. In this case, that's no longer the case. Even though we might omit the ID value through datashaping, we cannot omit the links through datashaping anymore. So that's pretty nice. And now let's post an author. Our response also includes the links, so this looks good. Up next is the authors resource. So let's open that author's controller again. The first thing we want to do is add a method to create the links for the authors resource. The GetAuthors method accepts an author's resource parameters instance. So we'll also need that to generate our links. The return value is again an IEnumerable of LinkDto. We'll name it create links for authors. And it accepts an authorsResourceParameters instance. Let's create a variable to hold those links, the new list of LinkDto. And let's already write the final statement, return links. Okay, so now we can create the links for the authorsResource. Or at least almost, because there's something else. Let's crawl up to the GetAuthors action again. We support paging on the authorsResource and when we talked about paging, we discussed the various means of returning metadata. And we learned that the best place to put this is in the header. But in fact we included things in the metadata that aren't really paging metadata. The previous and next page links, these are, as we know now, links that drive application state, so we can now put the previous and next links in the links area. That means our create links for authors method should also accept a hasNext and a hasPrevious boolean value. So let's add those. Okay, let's create links. First, a link to self. We've got this CreateAuthorsResource URI methods from when we learned about pagination. And this already creates a few links for us so we can use those to generate links. Let's just add another type, current, to the ResourceUriType numeration. And let's make sure that when that type is passed in, we return a link to the current resource. That's actually our default case. Okay, we've already got our URI generation for the links. Now let's add the links. Let's scroll down and let me paste that in, after all we've done that before. First we add the link to self. For this we create a new LinkDto and to generate the actual URI, we call into CreateAuthorsResourceUri. We pass in the authors resource parameters, and we state that we want the current link to link to itself. If there is a next page, we call into same create authors resource URI methods. This time, stating we want the next page. And we do the exact same to generate URI for the link DTO for the previous page. And that already takes care of link generation. On to the get authors action. We want to add the links to the response body but we also want to wrap the authors in an envelope, just as we did with books. First, let's change the pagination metadata, link generation for previous and next page can be removed and those two values can also be removed from the metadata object. They are, after all, now included in our links area. Then we create links by calling our create links for authors methods. As a second parameter, we pass in the hasNext value from the authors from repo paste list. And as the third parameter, we pass through hasPrevious. From that same page list. Then we shape the data. This returns an ienumerable of ExpandoObject. And for each of these, we need to add the author links. So we call select on our datashaped author list. We cast each author to an IDictionary of string object. For each author we then create links, this requires us to pass through the ID, which is a Guid. We can get that ID from the author calls to a dictionary. And we cast it to a Guid. And as a fields parameter, we need to pass through the fields from the authors' resource parameters. These assure us that the links that are generated for each author also match the requested fields for those authors. And then we add the author links. And we return the author as dictionary. That takes care of creating our shaped authors with links list. So we now have the links for the authors' resource, we have the shaped authors themselves, and for each of those shaped authors, we have their own specific set of links. That leaves us with thinking about what we actually need to return and that's a resource with this shaped authors with links list as a value for the value field, and the links for the authors resource as a value for the links field. So we create a new anonymous object and we return that object. By the way, it's not because we've now used this approach on authors and the base class approach for books, that we couldn't use the dynamic and anonymous type approach for the books as well. In fact, I'd advise you to use one of both rather than mixing both of these approaches. But I wanted to show you how to use them both. The approach you want to take tends to depend on the architecture of your API and the functionality it offers. Anyway, that should be it. Let's build a run and give this a try. Let's send a request to get the authors. As a value, we have a list of authors and each author has their own set of links. And if you scroll all the way down, we see that the authors resource itself also has an area of links. Now let's try a paged, ordered, filtered and shaped request. So what we get now are the ID and named fields of each author. We see that in the self-link for a specific author, the fields to take into account when datashaping have been added. We only get two authors, Neil Gaiman and George R. R. Martin, as the page size we've asked for is two. And we see that the link to this authors resource contains the same parameters we've inputted in our get request. Just one thing left, it would be nice to see a previous or next page link here. We only have two authors that match this request, so let's change the page size to one. And if you scroll down now, we see that this time we get a next page link as well. So we're already pretty far at driving application state, via these links. We're now always returning these links, but should they really be part of the resource presentation? When asking for application JSON, we already learned that what we're doing now isn't the correct implementation. To fix that, we'll have to dive into media types a bit deeper. And that's coming up next, in the Advanced HATEOAS Media Types Inversioning module.

## HATEOAS and Content Negotiation

Onscreen is part of a response from a GET request to the author's resource. We've got two fields, value containing the authors and links containing a self-link, which you see onscreen. The links are part of the resource body. That begs the question, Is this really the correct place to put these links? If we think back about pagination, we talked about where the pagination information should go. We concluded that it's metadata, so it should be in the header. It describes the resource. And that's true for fields like total count, current page, and so on. But what about those next page and previous page links? We put the paging links in the response body and kept the rest of the paging information as metadata in the header. So are these links part of the resource or do they describe the resource, i. e. are they metadata? Same goes for the other links, links to self and so on. If it's metadata describing the resource, they should go in the header. If they are explicit parts of the resource, they can go in the response body. What we're actually dealing with here are two different representations of the resource. With content negotiation, we ask through the accept header to get back a JSON representation of the resource. We ask for media type application/json, but what we return isn't a representation of the author's resource. It's something else. Additional semantics, the links, on top of the JSON representation. So the links should not be part of the resource when we ask for media type application/json. We've effectively created another media type which were wrongly returning when asking for application/json. So by doing this, we're breaking the self-descriptive message of constraint, which states each message must include enough info to describe how to process the message. If we return a representation with links with an accept header of application/json, we're not only returning the wrong representation, the response will have a content-type header with value application/json, which does not match the content of the response. So the consumer of the API does not know how to interpret the response judging from the content type. In other words, we also don't tell the consumer how to process it. The solution is creating a new media type. So how do we do that? Well, there's a format for this, a vendor-specific media type. You can see an example of that onscreen. First part after application is vnd, the vendor prefix. That's a reserve principle has to begin the mime type with. It indicates that this is a content type that's vendor specific. It's then followed by a custom identifier of the vendor and possibly additional values. A good one to use in our case would be vnd. marvin. hateoas, as you see onscreen. Vnd plus my company, which happens to be called Marvin, the paranoid android from Douglas Adams' Hitchiker's Guide to the Galaxy books and then followed by HATEOAS, stating we want to include those resources links. Then we add a plus sign and JSON. What we're actually stating here is that we want a resource representation in JSON with HATEOAS links. If that new media type is requested, the links should be included. The consumer must know about this media type and how to process it. That's what documentation is for. If this media type isn't requested, so we simply request application/json, the links should not be included.

