# Kubernetes

## Background

So, Google was running search and stuff on containers, and obviously search, and even Gmail and the likes, they're pretty humongous. I mean, we're talking like billions of containers a day stuff here, which would be right, seeing as every good search runs in its own container. Well, at scale like that, you just can't have humans pushing buttons, so what they did was they built a couple of in-house systems to help. First, they built something called Borg, quality name, then they built Omega. So, Borg came first, and as you do, you learned a bunch of stuff, and they fed that into Omega. Then, for whatever reasons, they decided to build another system, obviously learning from both Borg and Omega, and they made this new one open source, and lo, Kubernetes was born. So, Kubernetes came out of Google, and it's open source. And these days, it's the superstar project for the Cloud Native Computing Foundation, and to say it's gone from strength to strength, wow, that would be an epic understatement.

From a back-end perspective, it is backed by pretty much everyone.

The cloud players are all over it, and so are the traditional IT vendors.

In fact, it's probably the most extensive, like it does stateless, stateful, batch work, long running. It does security, storage, networking, serverless, or functions as a service, machine learning. Honestly, we could be here all day. There is not a lot that Kubernetes can't do. And all of the stuff it can do, it can pretty much to anywhere. Like we said, in the cloud and on-prem in you datacenter, and even on your laptop when you're developing. 

The name Kubernetes, it's Greek for helmsman or captain, the helmsman being the person who steers the ship, which I guess is why they picked it. I mean, after all, we have got this nautical theme going on in the container ecosystem. Oh yeah, and you'll see it shortened to this quite a lot, the 8 replacing the 8 characters between the K and the S, and some people pronounce this kates.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Foundation.png?raw=true)

## Functionality

Container bring scalability challenges

A leading cluster manager or orchestrator that lets us manage containerized apps at scale. And it's a lot like a data center scale OS, we give it work to run and it makes the decisions about where in the data center to run it.

It's all about managing containerized apps at scale, and the focus is very much on the app.

We're abandoning that traditional view of the data center as a collection of computers in favor of the more powerful idea that the data center is a computer, a giant one. But what do I mean by that? Well, if we look at a typical computer, and we're keeping it high level, right, but it's got processing cores, high speed memory, slower persistent storage, and a bunch of networking. And for the most part, or at least a big part, right, app developers, they don't care which CPU core or memory DIMM their application is using, we just leave all of that up to the OS, and you know what, it works a treat, and the world of application development thrives on this. Well, with that, I suppose it's only natural to take it to the next level and treat the data center the same way. So view it as just a pool of compute networking and storage. And again, right, application developers, honestly, they're not hung up on which server their code is running on. So, how cool would it be if we could just package all the code up, give it to some kind of data center scale OS, and let that take care of which nodes to run it on. Well, say hello to Kubernetes again.

So forget about naming your servers and treating them like pets, and the likes, these systems don't care. They deal in pure cattle. So, gone are the days of taking your app and saying, okay, run this part on this particular node here, and that part on that particular node. No, in the brave new Kubernetes world, we're all about saying, hey, Kubernetes, I've got this app, and it consists of these containers, uh, yeah, just run it for me please, and Kubernetes goes off and does the hard work for us. You know what, and I don't claim that this is my idea, right, but it is a lot like sending packages via courier services. So we package up our goods in their standard packaging, we label it, include a manifest, and give it to the courier, and they take care of everything else. All the complex logistics of which plane it goes on, which drivers to use, all of that is taken care of by them, and all they care about is that it's packaged properly and labeled up. Well, the same goes for an app in Kubernetes. Package it up as a container, give Kubernetes a declarative manifest, and it'll take care of the rest of the complexities of running it and making sure it stays running. It is a beautiful thing.

Oh yeah, yes, I know we're not massively bothered about which CPU core our app runs on or which DIMM it stores its working set on, but actually we do kind of care a bit when it comes to networks and maybe persistent storage. But that's fine, Kubernetes can handle that, and we'll see it soon enough.

Well, once we've got that, we package our apps, tell Kubernetes what they should look like, and then we just sit back, and we let Kubernetes do all the hard stuff of deploying and managing. So things like scaling, self-healing, running updates, all that stuff, no sweat. Kubernetes does it. I mean, there's obviously some up-front work from us to do like the packaging and set some of the thresholds and the likes, but honestly, with actually not a huge amount of effort from us, Kubernetes really can manage our apps, which definitely is magic, but capping it all off is the fact that it decouples our apps from any underlying environment, meaning we can switch between clouds, we can move back on-prem, and even back to the cloud again. It's all pretty easy with Kubernetes.

Docker provides the mechanics for starting and stopping individual containers, which in the grand scheme of things is pretty low-level stuff.

Well, Kubernetes, it doesn't care about lower-level stuff like that. Kubernetes cares about higher-level stuff, like how many containers to run in, maybe which nodes to run them on, and things like knowing when to scale them up or down or even how to update your containers without downtime.

If you think about your application as a musical masterpiece, I know, bear with me, if you did that, it'd be made up of lots of different musical notes from different instruments. There'd be violins, maybe they'd be front-end services, and I don't know, maybe the brass section would be the back end or whatever, but when they play together, they form this amazing musical experience. Well, if you've seen an orchestra, you know that there's a conductor at the front, and that person's in charge, and she's doing things like telling the trombones when to come in, how many violins, how loud, all of that stuff. Well, applications are similar. Loads of different parts that need to know how and where to run, which network to operate on, how many instances are required to meet demand, and probably a load more. And if this is the case, which it is, then Kubernetes is the conductor. So it's basically issuing commands to Docker instances, telling them when to start and stop containers and how to run them, sort of. And like with the orchestra, when all of this stuff comes together, they form this amazing application experience.

A bit more technical though, I guess if you know VMware at all, maybe think of Docker as ESXi, that low-level hypervisor. Then Kubernetes, I suppose, would be vCenter that sits above a bunch of hypervisors.

at the kind of high level we're at, we'd have a Kubernetes cluster down here to host our applications, and it can be anywhere. Well, each of these nodes is running some Kubernetes software and a container runtime. Usually the container runtime is Docker or containerd, but others do exist. The point is there's a container runtime on every node so that every node can run containers. Then sitting above all of this is the brains of Kubernetes, and that's making the decisions, like the conductor in the orchestra. Well, assume we've got a simple app with a web front end and a persistent back end. The web front end is maybe containerized NGINX, and let's say it's containerized MySQL on the back end. We tell Kubernetes maybe we want a single container on the back end and give it a lot of resources, like CPU and RAM, but on the front end, tell you what, we'll have two containers, but keep these smaller, and Kubernetes deploys it. So one of the thing Kubernetes does is decides which nodes to run stuff on, and it'll look something like this. And that's fine, but let's say load on the front end increases, and those two containers are not enough. Okay, no issue. Kubernetes is watching, so it sees the situation, and maybe it spins up two more, and it does it without a human getting involved. So literally, load goes up on the front end, and Kubernetes has enough intelligence not just to sit there and suffer. No, it spins up more containers. Problem averted. But the same goes if the load decreases. It's automagic. Kubernetes sees the drop in load, and it scales back down. Oh, and it's the same if a node fails or something. Seriously, Kubernetes is a fighter. It sees the node go down, and it doesn't run away and hide, and it doesn't freeze and hope the situation isn't happening. No chance. Kubernetes fights. So remember up here we asked for two web front ends, but right now we've only got one. Kubernetes observes this, and it fixes it, and we call that self-healing.

We tell Kubernetes what we want, and Kubernetes makes it happen. Then when things change, increased load, failed nodes, whatever, Kubernetes deals with it.

Docker's doing all the low-level container spinney up spinney down stuff, but it only does it when Kubernetes tells it to, meaning in this respect, Kubernetes is managing a bunch of Docker nodes.

Kubernetes is the absolute business for decoupling your applications from the underlying infrastructure. So, we've said Kubernetes runs everywhere. Kubernetes on-prem, Kubernetes in the cloud, it's all the same, meaning if your apps run on Kubernetes, it is a piece of cake migrating them on and off the cloud or even from one cloud to another. No joke. I mean, unless you're writing your apps to be tightly coupled to the services of one particular cloud, which ideally you wouldn't, but yeah, I understand why we sometimes do, but assuming you're not writing your apps to be locked to a specific cloud, then you can absolutely move seamlessly between one cloud and another and even on and off the cloud, which I think you'll agree has the potential to be huge going forward.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture_2.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture_3.png?raw=true)

## Architecture

So at the highest level, Kubernetes is an orchestrator for microservice apps that run on containers, and a microservice app is just a fancy name for an application that's made up of lots of small and independent services. But when you bring them together and they talk to each other, they create this more meaningful or more useful app.

We start out with an app made up of multiple services, each packaged as a pod, and at a massively high level at this point, obviously. But you know what, each one's got a job within the overall app. So we've got load balancers, web servers, logging, the whole picnic. And Kubernetes comes along, a bit like the coach in the football team analogy, and it organizes things so that they work together on the right networks with the right sequence, all of that hocus-pocus, and what we end up with is a useful app made up of lots of smaller moving parts. And we call this, what Kubernetes is doing, orchestration, so it's orchestrating all of these pieces to work together.

So we start out with our app, right, we package it up and we give it to the cluster, the cluster being Kubernetes, and it's made up of one or more masters and a bunch of nodes.

Well, the masters are in charge, right, and they make the decisions about which nodes to run work on. And if you're like me, and you like to try and sound intelligent, then the bits and bobs that run on the master make up what we call the cluster control plane. So it's all that stuff that monitors the cluster, makes the changes, schedules the work, responds to events, all of that jazz, right, this is the master or the control plane. Then the nodes, minions, these run the actual work, okay. As well as that, they do stuff like they report back to the masters and they watch for changes.

But at the start we said that we packaged up our application and we gave it to the cluster. Well, the best way to do that these days is probably as a Kubernetes deployment. So we've got our code, right, and we containerize it. Then we define it in an object called a deployment, and we'll come to this, right, but for now, all I'm going to say is to do this, we write it up, or we define it, in a YAML file. Look, it's just a manifest that tells Kubernetes what our app should look like. So, what images to use, what ports, networks, how many replicas, all that stuff, right, in a file. We give the file to Kubernetes on the master here, and the master looks at the file and deploys the app on the cluster.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture_4.png?raw=true)

## Managers aka Controle Plane

So a Kubernetes cluster is effectively a bunch of masters and nodes. Well, we need something to run these on, and we've said already that Kubernetes is pretty platform agnostic, so it runs on Linux, but it's just not opinionated on whether underneath that it's bare metal, VMs, cloud instances, open stack, it's just not bothered.

a master is actually a bunch of moving parts, and in the simplest and most common setups, they all run on a single server. Now, this isn't like set in stone or anything, and in fact, no doubt it'll change in the future. I mean, multi-master HA is already a thing. It's a bit complex to configure right now, but yeah, it's totally doable. And if we get the old telescope out and look further out into the future, I think it's totally reasonable to expect these master bits to get distributed all around the cluster instead of being centralized. Because, you know what, right now it is a bit of a monolith, in that we lump them all together in a big honking machine, and that's just not the way of things these days, so don't be surprised if it changes.

we generally consider it best practice to keep the master free of user workloads, so run your apps on the nodes, and keep the master free for looking after the cluster. It just keeps things clean and simple.

we need to know about the apiserver right, this is a biggie, okay. It's our front-end into the master or the control plane. In fact, it's the only master component that we should be talking to. For example, it's the only one with a proper external-facing interface. And you know what, like all good things these days, it exposes a RESTful API and it consumes JSON. Now how that looks in the real world is, right, we send in our manifest files, which we mentioned already, these declare the state of our app, like a record of intent, the master validates it, and it attempts to deploy it on the cluster.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_APIs.png?raw=true)

Then there's the cluster store. Now if the apiserver is the brains of the cluster, then the store is definitely its memory. So the config in the state of the cluster gets persistently stored in here. Right now it's etcd. Yeah, there's been talk, like forever, about there being other options, but right now etcd is where it's at. It's battle-hardened, and you know what, in my opinion it is pretty production ready. Oh, hang on. This is bad of me actually. Apologies. If you don't know what etcd is, that's fine. So etcd is an open source distributed key value store. It's developed by the guys at CoreOS, and if key value stores aren't your things, it's a NoSQL database. But it's distributed, consistent, and watchable, and when I say consistent, right, I mean, yeah, all copies can be written to, but they talk to each other and they work out consistency. Kubernetes uses etcd as the source of truth for the cluster. So it is vital with a capital everything, right. No etcd or cluster store, no cluster. It would be a bit like taking a person and entirely wiping their memory. Alright, they might be pretty to look at and full of potential, but they're not a lot of use right here and now. So make sure you protect this store and you've got a solid backup plan for it.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Cluster_Store.png?raw=true)

the controller manager. This is a bit like a controller of controllers, if you will, and dare I say, a bit of a mini monolith. But I don't want to get hung up on detail like this, because it's the kind of thing that could easily change, especially with something like Kubernetes that's evolving and improving at a rate that only software can. But right now, this component implements a few features and functions that I wouldn't be surprised if at some point in the near future they get split out into separate and even pluggable services. But right now, today, okay, we've got a bunch of controllers, node controller, endpoints controller, namespace controller, there's practically a controller for everything. And these guys all sit in a loop, and they watch for changes, the aim of the game being to make sure that the current state of the cluster matches our desired state. But like we said, right now they are all managed by the controller manager.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Controller_Manager.png?raw=true)

the scheduler. This is a biggie, okay. It watches for new pods, and it assigns them to workers, and yeah, that's the high level, right, but it's doing it a huge disservice because it's got a ton of things to think about. So things like affinity and anti-affinity, constraints, resource management, there's loads more,

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Scheduler.png?raw=true)

Now then, because the apiserver is our front-end into the master, and it's the only component in the master that we really deal directly with, well sometimes we fall into just calling this the master. So when we say things like, I don't know, maybe issue commands to the master, or whatever, we actually mean issue commands to the apiserver, we just mix and match master and apiserver quite a lot in our lingo. I think the important thing to remember, though, is no other master components expose an endpoint for us, just the apiserver, and by default, that does it on port 443. So the master is the brains of Kubernetes. Commands and queries come into the apiserver component of the master, we can stick authentication on at this point, that's beyond the scope of this course, though. We've not covered this yet, but commands come in usually via the kubectl command line utility, but they're formatted as JSON. Then there's a bit of chatter goes on between all these components here, and depending on what's going on, commands and action items make their way to nodes over here. Speaking of which, nodes.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Master.png?raw=true)

## Nodes

Worker that just does what some powerful master tells it to.

Kubernetes is like way over on the cattle side of the pets versus cattle thing, and cattle is all about working nodes being these faceless, characterless drones that we don't really care about, and that we can swap out without even noticing.

Straightaway we can see that they're a lot simpler than the master, so there's only the three things the we care about, the kubelet, the container runtime, and then the kube-proxy.

So first, and actually definitely foremost, is the kubelet. Let me be clear about this. The kubelet is the main Kubernetes agent on the node. In fact, it's probably fair to say that the kubelet is the node. So you install it on a Linux host, it registers the host as a node in the Kubernetes cluster, and then it watches the apiserver on the master for work assignments. Anytime it sees one, it carries out the task and then it maintains a reporting channel back to the master. If, for whatever reason, the kubelet can't run the work or maybe something goes wrong, it reports back to the master and the control plane magic on the master decides what to do. So, if a pod fails on a node, the kubelet is not responsible for restarting it or finding another node for it to run on, it simply reports the state back to the master. And, actually, speaking of reporting back, that reminds me, the kubelet exposes an endpoint on the local host on port 10255 where you can inspect it. Now, we're not going to dive into this on this course here, right, but I do think it's worth you knowing that port 10255 on your nodes, lets you inspect aspects of the kubelet. So, the spec endpoint, as we can see here, gives you some information about the node it's running on, healthz, that's a health check endpoint, and pods, this shows you running pods. And I mean there's more, but it's not for this course. Anyway, the kubelet works with pods, which we're going to learn about in a minute, but for right now, just think of a pod as one or more containers packaged together and deployed as a single unit.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Kublet.png?raw=true)

But because it's containers inside of pods, the kubelet needs to work with a container runtime to do all of the container management stuff, you know, things like pulling images and starting and stopping containers. But for the most part, the container runtime is going to be Docker, though, CoreOS rkt is also on the scene. So, a major part, I guess, of what the kubelet does is talk to the container runtime here. In the case of Docker, it uses the native Docker API, but yeah, it's pluggable, and if you prefer the rkt way of thinking, knock yourself out and go with rkt.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Container_Engine.png?raw=true)

Well, the last piece of the puzzle here is the kube-proxy. This is like the network brains of the node. So, for one thing, right, it makes sure that every pod gets its own unique IP. And, yeah, actually that is one IP per pod. So if you're a bit advanced and you're running pods with multiple containers in them, all of those containers are going to share a single IP. So if you want to address individual containers within the pod, you're going to be using ports, and the likes. But that's getting ahead of ourselves, right. Actually, no, you know what, on the point of getting ahead of ourselves, the proxy also does lightweight load balancing. So, hmm, getting ahead again. Among other things, right, a service is a way of hiding multiple pods behind a single network address. So let's say we got a bunch of web servers here, all talking to a backend down at the bottom. But we put this backend behind a service here, so a single IP and the likes, and well, we configure the front-end to take a look at to the service. Then it'll balance all requests across all three backends behind it here. And that load balancing is the responsibility of the kube-proxy.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Kube_Proxy.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Kube_Proxy_2.png?raw=true)

The kubelet's the daddy, and sorry about the blatantly male reference there, but you know what I mean, it's the main Kubernetes agent on the node. It registers the node with the cluster, and then it watches the apiserver on the master for work packages. Those work packages are basically pods, and then it reports back to the master on the status of the work that it's running. Well, sitting right next to it is the container runtime. This is usually Docker, but rkt from CoreOS gives us a choice there. Then there's the kube-proxy, the piece that makes all of the Kubernetes networking happen on the node.

## Declaritive Model and Desired State

Kubernetes operates on a declarative model. This means we give the apiserver the manifest files that describe how we want the cluster to look and feel. We do not give it a long list of commands that it needs to run to get to that state, we just tell it what we want it to look like. Then it's up to Kubernetes to do whatever is necessary to get there. Time for a quick cheesy analogy, right, but it's a little bit like getting a contractor in and saying, right, we want a new kitchen building on the back of the house. We want it to be open plan into the living area, hook into the underfloor heating system, we want a lot of glass on the south-facing exterior overlooking the garden, we want access directly into the garage, and we want a roof garden on the top. I'm just making it up, right. But that's pretty high level, and it is describing what we want. What it's not doing is saying, ok, knock down this existing wall and hold the roof up with whatever supports you need, yeah. Build 30 courses of brick work for a double-skinned wall with pins every 18 inches, use 25 ml pipes for the underflow heating, blah, blah, blah. It is not doing any of that. Now, the analogy only goes so far, right, you probably want a lot of control over something like a new kitchen, but the point is valid. You describe what you want the cluster or the app to look like, use this image, make sure there's always five copies running, that type of stuff, and Kubernetes takes care of all of the hard work of pulling images, starting containers, building networks, running reconciliation loops, all of that, we don't have to care about it. Well, to do this, we issue Kubernetes with manifest files that describe what we want the cluster to look like. Then those manifest files are effectively a record of intent, and they constitute what we call the desired state of the cluster.

Fabulous. But things go wrong, or things change, right, and it is totally possible for the actual state of the cluster to drift from this desired state. Who knows, I mean maybe a node fails, or maybe we even change the desired state. The point is, any time actual state diverges from desired state, Kubernetes gets like all, ahhh, that's not right, I must rectify, and it doesn't rest until desired state and actual state are back in sync. So as a quick example, right, let's say we've got a desired state that says we always want three instances of a web front-end pod running. Well, right now we've got three nodes, and one pod is running on each of those nodes, and that's just fabulous. We want three, and we've got three, so Kubernetes is relaxed and all chilling out. But then, horror of all horrors, one of the nodes goes down. Well, desired state still says, three pods please, but actual state says, uh-oh, only two pods, and this is like torture for Kubernetes, right, it just can't abide it. So, it kicks into action and fires up another pod on one of the surviving nodes. Great, this brings actual state back in line with desired state, and Kubernetes can relax again

We never interact with Kubernetes imperatively, or we shouldn't. We give it declarative manifest files that describe how we want the cluster to look. These form the basis of the clusters desired state. Then the Kubernetes control plane stuff is loading a load of reconciliation loops that are constantly checking that the actual state of the cluster matches the desired state. And when the two don't match, it's all hands on deck until they do.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Desired_State.png?raw=true)

## Pods

In the VMware world, the atomic unit of deployment is the virtual machine, in the Docker world it's the container. Well, in the Kubernetes world it is the pod. So let's be 100% clear about this. Yes, Kubernetes runs containers, but always inside of pods. Thou canst not deploy a container directly onto Kubernetes. You see, a container without a pod in Kubernetes is a naked container, and Kubernetes has pretty strict views on nudity. Now, you absolutely can run more than one container inside of a pod.

But anyway, even if a pod has only one container, I'm telling you, it's still a pod. You can't just say, oh well, since there's only one container there, might as well just deploy the container. No, not going to work.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_2.png?raw=true)

let's take a look at what a pod actually is. at the highest level, it's just a ring-fenced environment to run containers. So the pod itself doesn't actually run anything. It's just a sandbox, of sorts, to run containers in. So you ring-fence this area on a node, build a network stack, create a bunch of kernel namespaces, and then you run one or more containers in it. Now if you are running multiple containers in a pod, they all share a single pod environment. So, I'm talking about things like the IPC namespace, shared memory, the network stack, so IP addresses here. I mean, if you've got two containers in a pod, they both share the same IP. If they want to talk to each other, then they've got the localhost interface in the pod to do that over. As well, any volumes in there are also available to all containers in the pod. So, if you've got a use case where you need something really tightly coupled, maybe a couple of containers that need to share memory or volumes or stuff, then stick them in a single pod. But if they don't really need to be tightly coupled, and I know tight coupling is like blasphemy in some people's minds these days, so if you only need them more loosely coupled, honestly, stick them in separate pods and couple them over the network. Now, let me be crystal clear about this as well.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_3.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_4.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_5.png?raw=true)

The unit of scaling in Kubernetes is also the pod. So you want to scale a component in your app, you do it by adding and removing pods. You don't scale by adding more of the same containers to an existing pod. No, you want to scale an element up or down in your application, add or remove pod replicas. So if they're not for scaling right, they offer two or more complementary containers or application services, right, that need to be intimate. An example might be a containerized web server with a log scraper that you want right beside tailing the logs off to a logging service somewhere. In that situation, it might make sense to put these two together in a pod. And if you do that right, we tend to call the pod the main container and the log scrapper the, so-called, sidecar container, but it's just an example right.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_6.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_7.png?raw=true)

let's tie this back to the idea of pods being atomic. So the deployment of a pod is an all-or-nothing job. You never get to a situation where you're like, okay, let's scale this to another pod over here, and okay, let's stop bringing that pod up. Well, the pod's environment is created, and look, there's the first container up, so I'll tell you what, we'll mark the pod is up while we wait for the sidecar container. Nuh-uh, it doesn't happen like that. The pod gets deployed like this. It's not there, and then boom, it's there, the whole thing is up. Not there, there, not there, there, and I mean, okay, behind the magic curtain these things obviously don't just magically appear all at the same time, but the pod is never declared up and available until the whole lot is up and running. So as far as Kubernetes is concerned, it's atomic. Oh, and pods exist on a single node, right, it's atomic like that as well. You can't have a single pod spread over multiple nodes.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_8.png?raw=true)

Pods are mortal. They're born, they live, and they die, that's it. They are never brought back to life, no Lazarus experiences. If a pod dies, the replication controller starts another one in its place. It is not the same pod resuscitated, resurrected, anything like that, it's a shiny new pod that just happens to look exactly the same as the one that just died. And that's the pets versus cattle thing again, right? Pods should be treated like cattle. Don't build your apps to be emotionally attached to pods so that when one dies you get sad and you try and nurse it back to life. It's not going to work like that. Build your app so that when pods die, a totally new one can pop up somewhere else in the cluster and just take its place. So they're atomic and mortal.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_9.png?raw=true)

Now for the most part, we deploy pods indirectly as part of something bigger, like a deployment, but it is possible to deploy them directly by giving the apiserver a pod manifest file. Remember, that's our declaration of desired state, and it's a YAML file. Well, if we do that right, the apiserver validates it, ranks the nodes in the cluster, and then deploys it to one of them that satisfies requirements. Cool and all, but there's a pretty good chance you'll also see them deployed via replication controllers. This is another high-level construct like a deployment that takes pods and adds features around them. I think, as the name suggests, replication controllers are all about deploying multiple replicas of a single pod definition, then making sure that the required number of replica is always running. See, that's our desired natural state thing again, I can't tell you how fundamental that is, but replication controllers are being superseded a bit. You see, the same way a replication controller is a higher level than a pod so it adds more to the box of tricks, well, a deployment does the same thing, again, to a replication controller so it sits above replica sets and pods. 

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_10.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_11.png?raw=true)

## Services

So we've got applications, right, and we've said that in the Kubernetes world they're going to be made up of one or more pods. But we just learned that pods are mortal and can totally die. And when they do, they come back somewhere else in the cluster with totally different IPs. But it's not just when they die. Like if we scale an app and throw more pods into the mix, these all arrive with their own IPs. And then if we do like a rolling update or something, you know, like adding a new pod with a new version of code while getting rid of the old one with the old version, this ends up in a lot of IP churn. So the issue is, we just can't rely on pod IPs, and this is obviously a massive issue. So, as an example, right, let's assume we've got some microservice app with a persistent storage backend that other parts of the app use to store and retrieve data, pretty standard, right? But how is it going to work if you can't rely on those backend IPs? I mean, it's pretty darn inconvenient if the IPs change every time you push an update. But like we said, it's not even just that. Anytime you scale it up you get a bunch of new IPs, and then you lose a bunch when you scale it down. Well, playing Captain Obvious here, right, this is where services come into play. So at the highest level, right, let's say that this is a much simplified view of our app. We've got pods hosting the web front-end needing to talk to a couple of pods on the backend. Well, we slip in a service object here, and by the way, right, a service is just a Kubernetes object, like a pod or a deployment, so we define it with a YAML manifest, and then we create it by throwing that manifest at the apiserver. But once it's in place, and we'll see how it does all of this in a second, right, but it effectively sits in front of the backend here and provides a stable IP and DNS name for these pods down here. So, that's a single IP and DNS name here that load balances requests coming into those across the pods below it down here. Then, if one of the pods dies and gets replaced by another, no sweat, the service updates itself with the details of the new pod here, but keeps on servicing the same IP and DNS, and it load balances across the two pods again. The same goes if we scale the pods up or down here. All the new pods with the new IP, and the likes, get added to the service, and as if by magic, we're load balancing across four pod backends now. Then we rolling update the pods, the old ones get dropped from the service, and the new ones get added in, and it is business as usual.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_2.png?raw=true)

So that's the job of a service, a higher level stable abstraction points for multiple pods, and they provide load balancing. Now then, the way that a pod belongs to a service is via labels. And I just want to take a second to pause here and give a worthy tribute to the rolling value of labels in the Kubernetes world, because the labels, they are the simplest and the most powerful thing in Kubernetes. I mean, honestly, the power and flexibility they bring, well, it's something to behold.

So let's roll this picture back to just the two backend pods, and let's throw some labels on them, as you do, right, everything in Kubernetes gets labels. Okay, we can see we've labeled the backend pods as Prod, production, BE is for backend, and we're in version1. 3. And then up here on the Service, right, see how we've got the same labels. And it's the labels here that tie the two together. So if we had some other random pod up here that was totally different, right, like nothing to do with the backend, but we labeled it the same, then the service is going to load balance across that as well. Now, we don't want that, obviously, but you see where I'm going, right. When deciding what to load balance over, or what to be the service for, it uses a label selector that says, okay, all pods and objects with these three labels are mine, and that's how it decides what to load balance over. Now let's say we're going to update the backend service to version 1. 4. Well, we can update the service here to say, okay, just use these two labels as the label selector. Then, as we add a new pod here, it's going to match, and it's going to be load balanced on. And then the same for this one here. And as these new versions come online and the old ones still maybe stick around, we're load balancing across them all. Then we can maybe say, okay, let's just make it version 1. 4 now by adding this label in, and suddenly, we're only getting the new version. And then we can flip back easily enough, so long as we've not trashed the old pods here, and this will totally depend on how you do your updates right. But I'm thinking you can see how labels make it really easy for us to do things like that. As well, though, and it's always really hard to know what to say, and maybe what to leave out in a getting started course like this, but a quick few things that I think you'll find useful. Services only send traffic to healthy pods, so if your pods are failing health checks, then a service isn't just blindly going to send traffic to them. They can also be configured with session affinity here in a YAML file, but at the time of recording this is not on by default. You can also configure a service to point to things outside of the cluster. What else? Load balancing, okay, load balancing is purely random, by default. DNS round robin is supported and can be turned on, but you know what, beware of crappy DNS code in your apps that might not flush the cache appropriately. Your mileage may vary with that. Oh, yeah, and they default to TCP, but UDP is totally supported as well. So, yeah, services, a cracking way to provide stable IPs and DNS names in the unstable world of pods.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_3.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_4.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_5.png?raw=true)

## Deployments

So we've got our infrastructure at the bottom, the masters and the nodes, and we know that the smallest unit of work we can deploy in them is the pod, and pods have one or more containers. But we threw it out there a minute ago that we don't usually work with pods directly. And for the longest time, we've been taking this higher-level abstraction called the replication controller and using those to deploy pods. And it was a good thing, right. They added things like scaling, and self-healing, and even rolling updates, though those were a bit of a _____ bolt or an afterthought. But you know what, as an example, right, we might deploy a replication controller that says, let's have full replicas of x particular pod running. Well, we define it in a YAML file here, and we throw it at the apiserver. And, hey, presto, we have four copies of the pod on the cluster. But, then if a pod dies for whatever reason, the replication controller is still asking for 4 desired state yeah, only now there's only three, so that means red alert and all hands on deck until we're back to 4. But when I say all hands on deck, I'm talking Kubernetes here, right, like I mean me as a developer and IT guy I sleep through the entire thing. No pager and no phone call in the middle of the night for me, thanks. Kubernetes can take care of all of that hard work. Well, replication controllers are great, it's just deployments are even greater. So at a very conceptual level here, right, deployments are all about declarativeness, which is what we said before. You tell the cluster what you want things to look like, and the cluster waves its magic wand and makes it happen. And I think, honestly, right, there's no doubt that this is the best way to run real-world production environments. Now, I mean, I get it, right, that when you're playing around and you're testing, you're going to throw a few pods out there and add some replication controller action to the mix, and things are going to be a bit declarative, but you know what, there's also a bunch of imperative stuff in there as well, and by imperative, right, just in case you're not massively comfortable with the jargon, this is like, well, it's like listing out a bunch of commands or steps on what to do, so do this, and do that, and hopefully you get an end result. It's like the opposite of the declarative model where you don't say do this and you don't say do that, instead you just say, hey, look, this is what I want, you go make it happen. And for the real world, especially for production environments, I'm getting like a broken record here, I know it, but you really want to be declarative. I mean, the thing is, right, it's self-documenting, it's versioned, it's great for repeatable deployment, so spec-once, deploy-many, all of that goodness. It is like the gold standard for production environments. Plus, you know what, it's really transparent, and it's just that simple to get your head around, and that's a massive bonus for cross-team cooperation and getting new hires, and the likes, up to speed. But there's more, right. Here in the Kubernetes world, it makes roll-outs and rollbacks insanely simple. and who doesn't want that. But I'm blabbering, so back on track. Just like pods and replication controllers, deployments are proper first-class REST objects in the Kubernetes API. So, we define them in a YAML file, or JSON, if that's your thing, I'm just a YAML guy, right, but we define them in the standard Kubernetes manifest files, and we deploy them by throwing those manifests at the apiserver, same as pods and replication controllers. And in the same way that replication controllers add features and functionalities around pods, deployments do the same for replication controllers, so they take them and they add more, though, let me point this out, well, two things actually. Firstly, in the deployments world, replication controllers have been replaced by replica sets. I don't know, they're basically the same, right, just think of replica sets as next gen replication controllers. Secondly, though, with deployment we really don't get involved with the replica sets and pods. Sure, they're all there and happening in the background, but they're automagically taken care of by the deployment. And, of course, this makes life simpler for us, bingo. Anyway, the things that they bring to the game, or add, or wrap around the replication controller, yeah, they're just the things that we said before, the powerful update model and the super simple rollbacks. So, rolling updates are a core feature of the DNA of deployments, and we can run multiple concurrent versions of a deployment in a true and simple blue-green, or even a canary fashion. As well as that, Kubernetes can detect and stop rollouts if they're not working. And like we aid, rollbacks are super simple.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Deployment.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Deployment_2.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Deployment_3.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture_5.png?raw=true)

Every node or every worker out there, right, has its own set of default system pods, stuff like logging, and health checking, and DNS, and all of that.

Anyway, then we talked about pods, services, replication controllers, and deployments. But in uber-high-level, these are all REST objects inside the Kubernetes API, so you post them to the apiserver in order to create and manage them. Well, the pod is the atomic unit of scheduling in Kubernetes. There's no like running containers directly on the cluster, well, not via Kubernetes. And, you know what, doing it manually outside of the kubelet, yeah, that's not somewhere you really want to go, not at this level, at least. So we work with pods. Then on top of them we said that replication controllers let us scale them and maintain a desired state. But we also said that these are getting kind of pushed to the side a bit by the newer and more powerful deployment object. So, a deployment is really just this feature-rich abstraction that helps us manage pods. So, for instance, right, they let us define a desired state. They also let us easily change that, so scale it up and down, roll out new versions, run multiple versions side by side if that's our thing, and even rollback to previous versions, all of that magic, right. And all of it through this super-declarative interface. Oh, and I think we also said that services are useful for defining stable networking for pods, and as well as some basic load balancing.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Architecture_5.png?raw=true)

## Installing

So, Minikube is to Kubernetes what Docker for Mac and Docker for Windows are to Docker.

In fact, under the hood they're pretty similar as well. So architecturally, and let's put this one side by side, right, when we spin up Minikube, it goes away and it creates a local VM. So we're on our laptops or whatever, yeah, we download the Minikube installer, and we go minikube start. That creates a VM, then inside of the VM it spins up a single node Kubernetes cluster. Magic. But it also sets things up so that the Kubernetes API server and what have you, inside of the VM, are all available outside of it on your laptop's environment. So all I'm saying is, right, I can then use the kubectl binary that I've already got on my laptop, and I can use it to manage the cluster inside the Minikube VM. And, you know what, I mean it's totally cool to know that it's all wrapped up inside of a VM here, but really, the integration is so seamless that you probably don't even need to know that.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Installation.png?raw=true)

Inside the VM here, there's really two high-level things that we're interested in. First up, there's the localkube construct. It's a binary, actually, and it's pretty cool. This is what runs all the Kubernetes cluster stuff, yeah. So a master with the apiserver and all that, plus a node. Then, off to the side here, still inside the VM though, right, we've got the container runtime. Right now that defaults to Docker, but you can go with rkt as well. In fact, you know what, at some point I imagine we'll see rkt replace Docker as the default. And I suppose that's a bit of crystal ball reading from me, right, but it's going to happen. Rkt is just more of a natural fit with the Kubernetes architecture. But, yeah, that' the Minikube architecture. Oh, and over here, right, remember, outside the VM we've got kubectl, or kube control or kubectl, or call it whatever you want. Well, it's pretty much our Kubernetes client, and we can use that to control Kubernetes inside the VM here.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Installation_2.png?raw=true)

Zooming back out, though, the architecture stuff doesn't really matter so much. You see, this is all about a slick and smooth experience. So the stuff under the hood, really, I suppose it could all change, and it will, right, but the user experience won't. We'll always get that authentic Kubernetes experience, one that looks, smells, and feels exactly like real world production.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Installation_3.png?raw=true)

## Working with pods

Now, I know there's services and stacks and the likes, but the smallest, most atomic unit of scheduling in the Docker world is the container. Well, in the Kubernetes world, it's the pod. So, take that, and file it away in your brain as mucho important. Virtualization does VMs, Docker does containers, and Kubernetes, that does pods. And you know what, right, a pod is like somewhere in between a container and a VM. It's bigger, and I guess arguably a more high-level construct than a container, but it is nowhere near as big and clunky as a VM. So, okay, right, a pod contains one or more containers. Usually one, but multiple containers inside of a pod is definitely a thing. In fact, it's brilliant if you need to tightly couple services. Anyway, you deploy a pod to a cluster by defining it in a manifest file. Then you feed this manifest file to the apiserver, and then the scheduler deploys it to a node. And like we've said, depending on what's defined in the manifest file, that pod is going to have one or more containers. But, and here's where we get to dig a little bit deeper, irrespective of how many containers are inside the pod, the pod just gets a single IP. So it's not like an IP per container thing here. This is a one IP to one pod relationship. So, look at the example we've got here, right, two pods and two IPs, one IP for each pod. Then, if we look at Pod 1, we can see it's actually got two containers. Well, those are both available at the single IP of the pod. See how one is on port 80 and the other 5000? So actually, this IP thing, right, is really a network namespace, so every container inside the pod shares a single network namespace. So that's the single IP address that we already said, but it's also a single localhost adapter, and it's also that single set of ports. So, again, looking back at this example, right, there's no way we can set these two containers up on the same port. Normal networking rules still apply. But it's more than just networking. All containers in a pod share the same cgroup limits, they've got access to the same volumes, the same network, and even the same IPC namespaces. Now, how this all works behind the scenes with pods or infra-containers and the likes, it doesn't massively matter to us, and you know what, it's constantly changing as things like PID namespaces, and the likes, come into play. For us, right, just think of the pod as holding all the namespaces, and any containers they run just joining and sharing those namespaces.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_12.png?raw=true)

Now then, inter-pod communication is simple. That's because all pod addresses are fully routable on the pod network. Now, if you watched the installation module, which you should have done, you'll have seen me create the pod network at the end of the manual install section. Well, every pod gets its own IP that's routable on that network. This means, every pod can talk directly to every other pod, and there's no need to mess about with nasty port mappings or anything.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_13.png?raw=true)

Well, inside the pod, like if you've got multiple containers in there, well they all talk over the shared localhost interface. And if you need to make multiple containers available outside of the pod, then you do that by exposing them on ports. And obviously, like if you've got two containers inside a pod, they both cannot use the same port, no. Like we said, those normal networking rules, they still apply here.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_14.png?raw=true)

when we deploy pods it's an all-or-nothing job, right, there's no such thing as a half-deployed pod. It's either all the containers in the pod come up and then the pod itself comes up, or they don't, and the pod fails. You're never going to end up in this world where maybe one of the containers in a pod started and the other is in a failed state, but the pod's partially up. That's not how it works. And I suppose it's worth saying, right, that a pod gets deployed to just a single node, and this is just like VMs or containers, right. You can never have half of a pod on one node and half on another. One pod gets scheduled to one node.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_15.png?raw=true)

Now, the lifecycle of a typical pod goes something like this. You define it in a manifest file, YAML or JSON, take your pick, right. Then you throw that manifest at the apiserver and it gets scheduled to a node. Once it's scheduled to a node, it goes into the pending state while the node downloads images and fires up the containers. And this is important, right, it stays in this pending state until all containers are up and ready. Once that's done, it goes into the running state. Then once it's done and dusted with everything it was created to do, it gets shut down and the state changes to succeeded. Now, if it can't start, for whatever reason, it can remain in the pending state, or maybe eventually go to the failed state, which hopefully won't happen too much. And that's it for pod states, really. But, and a big but here, no pun intended, but you've got to think of pods as mortal. When they die, they die, there's no bringing them back from the dead. It's the whole pets versus cattle thing, right, and pods are totally cattle. So they die, you replace them. There's no tears, no funeral, no mound in the ground. The old one is gone, and a shiny new one with exactly the same config, but a different ID and all, magically appears and takes its place. So they're cattle, totally replaceable, that is the model.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_16.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_17.png?raw=true)

On the manifest file.

First up, we're setting the version of the API to use, v1. It's stable, it's been around since 2015, and it's fine for working with pods. Now, if you're working with some of the newer constructs, and we'll see this later in the course when we work with deployments, but some of the newer stuff is only supported with newer versions of the API and schema. Well, next up, kind. This tells Kubernetes what kind of object to deploy, obviously this one's a pod. But, again, as we crack on with the course we'll populate this with different values, Replication Controllers, services deployments, you name it. But, it's good, right. You see, instead of having a different kubectl create command for every type of object, we've just got one. Then the way that Kubernetes knows what type of object we want to deploy is right here in the manifest. Well, then we define some metadata. We're keeping it simple here, right, and we're just naming this one hello-pod, but you can pretty much put anything you want here. In fact, you know what, I'll tell you what, let's say that this one is in the prod zone, and we'll call it version v1. Alright, so as you can see, these are just key value pairs, but, they are powerful, and we'll see them a bit later. Then last, but not least we've got the spec section. This is where we define what's in the resource. So we're defining a pod, yeah, but inside of it we're saying we'll have a single container, just not any old container, we're going to call it hello-ctr, and we're basing it off of this image, and we're exposing it on this port. Now, if it was going to be a multi-container pod, we'd just define more containers below here. Well I reckon that's it, apiVersion, it's a pod, we'll call it hello-pod and give it these extra labels that we made, and we'll run this container here, fabulous.

Well, to deploy it, we go kubectl create, tell it to deploy from a file, and our file is in this directory called pod. yml. Alright, pod created, that was quick, but don't get carried away, that's just the definition on the cluster created, it might not actually be deployed yet. Well, that's alright, we can check it with kubectl get pods. And right enough, 0 of 1 pods is ready. And we can see it's currently in the ContainerCreating state, hmm. Now, hang on a second. We just did about pod lifecycles, and I do not remember anything about ContainerCreating being a valid state, so what's going on? I hope this is not some cheap course with bogus info. Well, I tell you what, let's have a look at it with this. Okay, so what have we got here? Well, we've got pod name, it's in the default namespace, it's on this node, and blah, blah, blah, ahh, Status Pending. Now Pending was definitely a legit status, so where was ContainerCreating coming from. Well, this stuff up her at the top is the pod stuff, but then further down here is the container stuff. And look, here we've got container State Waiting, Reason ContainerCreating. So, long story short, the pod status we were getting was actually the container state, not the pod state. And I'm not sure I would call that an outright bug, but yeah, it's definitely a bit weird that a get pod shows us the container state rather than the pod state. But never mind, look, down here we can see that it's pulling the image, and to be honest, that's probably done now. Yeah, so, congrats, that's your first pod up and running.

Now kubectl get pods is going to show all pods in the default namespace, and that's alright for us because we've only got one. But if you're in the real world with a boatload of them, you're going to want to filter them in some way. Now, probably the easiest way to do that is to lash the name of the pod onto the end like this, right. And if you want to see all pods in all namespaces, so like system pods and the likes, we'll just whack this onto the end.

We don't ever work directly with pods like the way I just showed. Well, what the.. Don't worry, it was good that I showed you what I did, right? I mean it covered some of the fundamentals, and it was a really easy intro into the YAML format. But, the reason we don't work directly on pods is because there are higher lever objects that expand on them and make them better. A really common one is the Replication Controller object. Now this one, right, at a high level, is a bit like a wrapper around a pod, with the sole purpose, okay, of adding desired state to the game. Okay, let's have a quick example. Assume you wanted, whatever, five replicas of a pod, and you wanted them to always be running. Alright, you'd deploy a Replication Controller that specified the pod and defined the desired state of having five replicas of that pod. Well, obviously, you throw it at the apiserver and Kubernetes would deploy it, deploy their application controller, yeah. Now, because you define the pod within the Replication Controller, part of deploying that Replication Controller actually deploys the pod, well five of them actually. Then, and here's the good stuff, Kubernetes then runs a continuous background loop that's constantly checking to make sure there's always five replicas of the pod running. And, of course, it's auto-magic and it's in the background, right. You deploy it, the Replication Controller, and then you crack on with something else. Kubernetes picks up the hard work, runs a watch loop in the background, and makes sure that the actual state of the cluster always matches your desired state.

Alright, now then, so here we've got a simple Replication Controller config. Again, we see the same four top-level constructs, version 1 of the API, this time we're defining a ReplicationController and we're calling it hello-rc, and then the spec. For now, right, we care that it's asking for 10 replicas, and we're saying select on pods with this selector label here. So this section up here is kind of the ReplicationController config. Then, though, in the template section down here we're saying use this pod template. And importantly, right, we're giving it the same app equals hello-world that we told the controller to select on up here, and then we've nested pretty much the same pod template that we used a minute ago. Now, look, I know that this is new, right, to some of you. So maybe take a second to pause the video or watch it through and maybe rewind it and watch it again to help it sink in, but I think, right, the takeaway point is this. It's a ReplicationController, we're declaring a desired state of let's have 10 of them. And when we say them, right, we mean pods, and configured like this down here. So, API, kind, metadata, and spec. Those are our four top-level objects, and we saw them a minute ago, again, in the pod manifest. Only, in the ReplicationController spec here we've embedded the pod spec as well.

kubectl describe ReplicationController

Now the image is downloaded, well, at least on one node, so let's take a look at how they are. Okay, cool, let's describe them as well, brilliant. Now if we want to update the config, so change the desired state, I don't know, to let's say, 20 replicas instead of 10. And you know what, we could be changing something else, right, like the version of the image or whatever. Well, either way, you just edit that config file, we said 20 this time, and you can change anything here in the pod template area, right. But let's save that. Then kubectl apply, and we'll give it that same file. Right, great, now don't worry about this message, it's a bit below here that we're interested in, cool. Alright, so let's check the status, and that's done, right, 20 replicas. In fact, if we do this and run this against pods, right, if we look down here we can see the age difference as well. Some of them are newer than others, so 10 old, and 10 newer. Well, you know what, I reckon that's the basics of pods and Replication Controllers.

kubectl apply -f file.yml

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Pod_18.png?raw=true)

## Services

So we've got an app running, or have we? I mean it's not like we've even seen it, not really, certainly not in action. In fact, the only evidence of the app so far are a few kubectl commands that tell us we've got some pods running. But let's show some trust here and assume our app is up and working, and it's replicated, and all thatgoodness, so that if we lose a node or whatever, we'll always have 20 instances, or however many we configured the Replication Controller for, I think it was 20. But the big question should definitely be this. How do we access it, and even see what it is? Well, on that point, Services. And I want us to think about two typical access scenarios, right, accessing from outside the cluster, like a web browser on your lappie or something, and accessing it from inside the cluster, like another set of pods wanting to talk to it. Well, guess what, yep, Services nail both of these. For starters, right, they give pods, well, you know what actually, let's back up for a second. So a Service in Kubernetes speak is a REST object, just like pods and Replication Controllers. And we'll see it in a minute, but we define them in a YAML file and throw that at the apiserver. But for us right now, we care that a Service is an abstraction. And we're going big picture here, right, but let's take a bunch of pods with our app, and they're deployed through a Replication Controller, and they are happy. But we're not. I mean, right now we have no solid way of connecting to them, because remember, pods are ephemeral, here today, gone tomorrow. So it is a big no-no to look up an individual pod IP and connect to that. Because what if we're connecting to maybe this one here, and then suddenly, poof, gone. Uh-oh, well, slap a Service in front of them like this, and boom, that is your reliable endpoint right there. So the Service here gets a single IP and DNS and port as well, right, and these never change. Let me repeat that. They never change. So this all down here can chop and change as much as it wants. Some of them can die, we can scale them up, scale them down, pretty much anything we want, but the Service in front of it, you guessed it, never changes. Just throw your requests at this, and no matter what kind of chaos and complexity is going on down here, it is all hidden nicely behind the tidy Service. And, of course, requests to the Service get load balanced across all the parts at the bottom here. But bringing it back to the two access scenarios we mentioned, access from inside the cluster and then access from outside, from inside the cluster, the other parts of our app can connect to this stable IP or DNS. And by default, right, the DNS name of the Service is just the same as the Service name. But to help us from the outside, a Service sets up a bunch of networking magic to make the pods here all accessible from outside the cluster. Speaking of which, let's dive a bit deeper. So, we've got our pods, and we know that they are unreliable. Then we layer a Service on them, and this is reliable. It gets a virtual IP that stays with it for its entire life, and it gets a DNS name and a port, yeah. Now then, that port is cluster-wide, and it's how we connect in from the outside. So we've got our nodes in the cluster, and they're running our pods. We create the service object, and that links the pods to a particular port number on every node in that cluster. And the port number is the same on every node. This means, okay, we can hit any of the nodes in the cluster on that port, and get through to our pod, our app. And there's other options as well, like integrating with your cloud provider's load balancers. But that starts to get a bit platform-specific, and we're not going there. Now then, when you create a Service, and maybe this is even going a bit too deep for this course, but let's go there anyway, right. Anytime you create a Service, you're also creating an endpoint object for it. And it's the endpoint that holds, well, all of the endpoints, so a list of all of the pods in the Service. So as pods come and go, the list of them that the endpoint maintains gets updated auto-magic. And it's great for native Kubernetes apps that know about this and how to use it. But if you don't happen to be a native app, well that's alright as well. You can hit the Service's virtual IP, and that balances over the backend pods as well. It looks lovely right, but how do we tie these pods together with the Service? Easy, labels. I absolutely love labels. So you remember the labels that we stuck against our pods before? Well, we just told the Service object to select on them, that's it. The labels here are the glue that sticks the Service to the pods, only you know what, glue is a crappy analogy, because this is actually uber flexible. So what I'm saying is, right, you can update the Service and add or remove labels here, and that's going to dynamically update which pods the Service balances over. It's totally dynamic. The Service's label selector is continuously on the watch, like 24-hour surveillance, so any changes to the Service get immediately reflected to the corresponding endpoint object. And then there's Service Discovery, okay. This is implemented in a couple of ways. First up, and best, I reckon, is DNS. As long as you're running the DNS cluster add-on, which if you follow the install methods that we've shown you, you will have, but the DNS cluster add-on implements a DNS Service in the cluster. It's all pod-based, obviously, but it implements a cluster-based DNS Service that configures the kubelet, that's on nodes, yeah, to go to it for DNS. Then it watches the apiserver for new services, and it registers them with that DNS. Long story short, Services get DNS names that are resolvable inside the cluster. Lovely, what is not to like about it? The other way, though, is through environment variables. Every pod, okay, gets a bunch of environment variables in case you're not using DNS, but this is done when the pod is created. So if you create your pods before you create the Service, then it's not going to get the variables. I mean, how can you throw variables about the Service into a pod, if you haven't created the Service yet? So I reckon you're better off with DNS.

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_6.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_7.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_8.png?raw=true)

![enter image description here](https://github.com/andreborgesdev/Thesis-Notes/blob/master/Images/Kubernetes_Services_9.png?raw=true)

Okay, so we've deployed an app, apparently, and in a pod, apparently, or a set of pods, and we did it via a Replication Controller. But we've already established, yeah, that there's an element of trust going on here, because as far as I'm aware, we haven't actually seen our app yet. So let's fix that. Well, you know what, first of all, let's show that we can't access it, at least not from outside the cluster. So these here are the public IPs of all of the nodes I've got in the cluster. So any of these here will do. And the only port that I've seen associated with the app so far is 8080, so that'd probably be my best guess as to which port to hit it on, and nothing, no joy. Okay, well, the simplest way to wrap it in a Service is to go kubectl expose. This is the iterative way to create a Service object. We're exposing our Replication Controller. We'll call this hello-svc, because we're creating a Service. The target port is going to be 8080, and we'll expose it via something called a NodePort. Okay, that's done. Well, we can describe it like this. Okay, see how it's type NodePort. This here is the Service's virtual IP, and this one here is the cluster-wide port that we can access it on, so I'll grab that actually, and we'll talk about endpoints and other stuff later. Well, if we go back to that web page again, let's change this 8080 here to the value we just copied out of the Service, and we're in business. That's our app. Hmm, a bit of a let-down probably, a crappy web page. And if you've done any of my Docker courses, you'll have seen this like a million times already. But it's okay, it's a containerized web server listening on 8080, and we have mapped it to this port here on every node in the cluster. That means we can hit any node in the cluster on this port and get to our app. And by default, right, Service ports are going to be between 30000 and 32767. But, yeah, that's our Service. Super simple, and super powerful. But the way we've shown you here with the kubectl expose command, that's just not the Kubernetes way. Obviously it works and it's fine, but Kubernetes is all about doing things declaratively. So, coming up next, we're going to see how to do all of this again, but with a YAML file, and we'll dive a bit deeper as well.

