Course Overview
Containers. They've got to be about as hot as it gets in the IT world right now. But containers need managing, oh my goodness, do they need managing. And Kubernetes, well, it's one of, if not, the hottest container management technology out there, and it's got a future as bright as any I've known, so I am pretty excited to be bringing you this course. So it's Getting Started with Kubernetes, and whether you've got an immediate and pressing need to learn Kubernetes for your job, or if you're just looking to add a string to your bow and stay ahead of the curve, this is the course for you. It's going to be fast-paced, heavy on demos, crystal clear on the theory, and definitely no death by PowerPoint, no chance. By the end, right, you'll know all the theory and practical of what I think are the most fundamental parts of Kubernetes. Now, you're not going to be experts by the end, but you will be in possession of the knowledge and skills necessary to kick-start your Kubernetes journey. Okay, well, I'm Nigel, and I'm going to be your instructor, and in case you don't know me, I'm a container-holic. I live at large in the container world, and I have done for some time. I mean, check out my long list of Docker courses. I live and breathe this kind of stuff. But you know what? I've also got a solid enterprise background. I served my time in some of the most demanding and high-pressure environments the likes of finance and government work can offer, and I've got the scars to prove it, so this is not going to be some head-in-the-clouds type of course from a guy who doesn't know the real world. Anyway, look, containers are hot, Kubernetes is hot, and I am excited to get cracking with this course.

Course Introduction
Course Introduction
Hi there, and welcome one and all. Get ready for the best 2 hours of your life. Anyway, look, I'm assuming that you're here because you see what's going on with containers. So, you see the impact that they're having and you think, you know what, I think I'll have some of that, both for yourself and for your business. Well, magic, because you've come to the right place. Now then, if you're like totally new to this stuff, and maybe you could do with a bit of a container primer, don't get me wrong, right, you are more than welcome on this course, but hear me out on this, right. I don't know, maybe you want to pop over to this course first, Docker and Containers: The Big Picture, and as well, you might want to check out Getting Started with Docker, you know, spin up your first container, and the likes. And I'm only saying this, because knowing the stuff in these courses is really going to help you swallow some of the stuff we're going to tackle on this course. So, if you think that's you, seriously, no problem, go pop over and check out those courses first. You will not be disappointed, and I promise you, you will not miss out on a single thing here, because thanks to the magic of the internet, we'll be waiting right back here for you once you're done. You will not miss a thing. Okay, so from this point on, I am assuming that you know what a container is and a bit about how they work. So here's our master plan for this course. We're going to start out with the What is Kubernetes stuff, so we'll talk about why we even have Kubernetes, and some of the things that it brings and adds to the container scene. Then we'll drill into the architecture. Now this is all going to be theory up to this point, right, but if you know my courses, you'll know it won't be boring, and by the end of it we will be well up to speed with things like masters and nodes, and pods and deployments, and dare I say, we'll be ready for the practical stuff that's coming up next. But do not skip it, okay, you'll be tested later. And the practical stuff, it might be a bit cryptic if you haven't swotted up on the theory first, so no skipping this theory class, okay. Anyway, then we'll look at how to install Kubernetes. Now I'm planning to try and cover a few different installs, right. So, for instance, we'll look at the Google container engine as a managed public cloud option, but we'll also look at how you can spin it up on your laptop, and I think we'll even take a look at a full-blown on-premises bare-metal type install, so I think some good stuff. Well, then once we can spin it up, we'll get into pods first of all. We'll see what they're all about and we'll play around with them. Then we'll hit deployments, the latest and greatest when it comes to managing applications with Kubernetes, and you'll love it. Then we'll wrap things up with a quick chat about where you might want to take your journey next. Oh yeah, and of course, at this point the world will be your oyster. You'll be like the batman or the bat-woman of the tech world. But you know what, before I get carried away, let's go find out what Kubernetes is all about. Okay, you know what, I take that back. I'm obviously too excited to get started. I forgot to say, this is me, Mr. container-holic, and if you want to reach out to me to say hi, maybe thank me for the course, who knows, right, maybe even abuse me for the course, either way, right, feel free to hit me up on Twitter. Now, okay, yeah, I'm a busy man, okay, but I do love hearing from you, and I do try my best to reply, though, I've got to say, this is not an offer for free consultancy via Twitter, but yeah, but feel free to hit me up and say hi. Anyway, this time I'm definitely ready to move on. Let's go and find out what Kubernetes is.

What Is Kubernetes?
Kubernetes: Where It Came From
Right then, what actually is Kubernetes? Now, I'm going to split this into two. I'll start with, I don't know, I guess, the DNA. I don't know if that works, but I mean things like where did it come from, and what does the name mean, and then after that we'll talk about what it actually does and why we have it. So first up, Kubernetes came out of Google. That's got kind of a movie trailer ring to it, doesn't it? Kubernetes, spawned from the bowels of Google, rising from the most hyper-scale data centers on the planet, No? Okay, never mind. But it's true enough. Kubernetes was born at Google. But, in the summer of 2014 it came of age, and it was open-sourced and handed over to the Cloud Native Computing Foundation, which is a part of the Linux foundation. And to be fair, since then it has been nothing short of a rip-roaring success, like there's no doubt it's one of the biggest and most important open source infrastructure projects on the planet. And I am warning you, right, it's future is so bright, if you stare out it for too long you're going to damage your eyes and give yourself sunburn. Well, you know what, like all the other cool projects at the moment, it's writing in Go, and if you're hands-on with the code, and the likes, it lives on GitHub at kubernetes/kubernetes. As well as that, it's actively discussed on the IRC channels, you can follow it on Twitter, this is a pretty good Slack channel, and you know what, there are a ton of meetups going on all over the planet. Ah, yeah, now, there's a pretty good chance you'll hear people talk about how it relates to Google's Borg and Omega systems. So it's no secret, right, Google's been rocking its own infrastructure on containers for years. I mean, stories of them spinning up billions of containers every week are retold the world over at meetups and the likes. So, yes, for a very long time, like way before Docker even thought of bringing containers to the masses, Google was running its search, its Gmail, GFS, pretty much the whole shebang, on containers, and lots of them. Well, pulling the strings and keeping those billions of containers in check are a couple of in-house technologies and frameworks called Borg and Omega. So, it's not a huge stretch to make the connection between those guys and Kubernetes. I mean, after all, Kubernetes is also about managing containers at scale. So, yeah, I guess it's probably natural to think that Kubernetes is maybe an open source version of one of them, but it's not. It's more like, hmm, it's more like it shares a bunch of DNA and family history with them. So in the beginning was Borg, Borg became Omega, or it spawned the idea or the genesis of Omega, and then Omega led to Kubernetes. But the three of them are separate systems, they just share a common DNA and ancestry. So I think, right, it's more proper to say that while Kubernetes was built from scratch, you know, from the ground up, it leverages a ton of stuff learned from Borg and Omega. Oh, you know what, as well, a bunch of the same guys that built Borg and Omega also built Kubernetes. Anyway, I feel like I'm waffling. What else did I want to say? We've got that it came out of Google, shares a bunch of its DNA with Borg and Omega, oh, it was open-sources under the Apache 2.0 license. Version 1 shipped way back in July 2015. Oh, yeah, the name, of course, the name. Kubernetes, right, comes from the Greek word meaning helmsman, and the logo here, obviously, that's from the helm of a ship. And this should be no surprise, right, I mean, the whole Docker and containers world is just littered with nautical references. But on the topic of the name, right, I love a good rumor, and rumor has it that Kubernetes was originally named Seven of Nine, which if you know your Star Trek, and if you don't know your Star Trek, what are you doing on this course? Anyway, if you do know your Star Trek, you'll know that Seven of Nine is a female Borg, so that linked back to Borg, yeah. Now, look, I don't know, that could be a total urban myth, but I like it, and I'm always looking for ways to link Star Trek into my courses. Now, one last thing I'll say before moving on, you're probably going to see it shortened to this quite a lot. The idea, right, being that the number 8 here replaces the eight characters between the k and the s, brilliant for lazy typers like me. Now, look, I know, a lot of this might seem like it's waffle, and you could be thinking, it's a bit of a waste of my time, and I get that, right. But it's not. I mean, for one thing, you can now look clever and in the know by shortening it to this all the time. But more seriously, I mean there are no Kubernetes experts out there that don't know this stuff. Like, I guarantee you, right, there are no folks building gigantic Kubernetes clusters who don't know that it all started at Google and now lives at kubernetes/kubernetes on GitHub. It's like a prerequisite to know this stuff, right? And you know what? Now you do. That said, though, there's absolutely no point knowing this fluffy, less-important stuff if we don't know the rest of the story, so next up, what actually is it, and what does it do?

Kubernetes: What and Why
Okay, now that we know where it came from and why it's called Kubernetes, let's talk about what it is and why we have it. Now like we said in the intro, I'm assuming that you already know about containers. Well, I'm also assuming you've got a bit of an idea about some of the challenge that they bring. As well as that, I'm hoping you've got an idea of some of the impacts they're having on how we think about the data center and use its resources. So generally speaking here, containers make our old challenges with scale seem pretty laughable, and that's putting it mildly. I mean, we've already talked about Google's billions of containers a week madness. But I know, I'll hold my hands up straightaway and say, yeah, Google is Google, and true, the vast majority of companies are about as much like Google as I am like Lionel Messi on a football pitch, so not even remotely similar. But it is true, right, that if your legacy apps hide maybe hundreds of VMs, there's a pretty solid chance that your modernized, containerized apps, they're going to have thousands of containers. And, well, we're going to need a way to manage that madness. Say hello to Kubernetes. The second thing I want to mention, and I'm fascinated by this and loving it, but we're abandoning that traditional view of the data center as a collection of computers in favor of the more powerful idea that the data center is a computer, a giant one. But what do I mean by that? Well, if we look at a typical computer, and we're keeping it high level, right, but it's got processing cores, high speed memory, slower persistent storage, and a bunch of networking. And for the most part, or at least a big part, right, app developers, they don't care which CPU core or memory DIMM their application is using, we just leave all of that up to the OS, and you know what, it works a treat, and the world of application development thrives on this. Well, with that, I suppose it's only natural to take it to the next level and treat the data center the same way. So view it as just a pool of compute networking and storage. And again, right, application developers, honestly, they're not hung up on which server their code is running on. So, how cool would it be if we could just package all the code up, give it to some kind of data center scale OS, and let that take care of which nodes to run it on. Well, say hello to Kubernetes again. But others do exist, right? I mean, for one, Mesosphere DC/OS, short for Data Center OS. That's also competing in this space, I mean it even says it in the name. But Docker Swarm is in there too. And the thing is, right, if you want to be down with all the cool kids and use the lingo of the day, right, these systems are all in the cattle business. So forget about naming your servers and treating them like pets, and the likes, these systems don't care. They deal in pure cattle. So, gone are the days of taking your app and saying, okay, run this part on this particular node here, and that part on that particular node. No, in the brave new Kubernetes world, we're all about saying, hey, Kubernetes, I've got this app, and it consists of these containers, uh, yeah, just run it for me please, and Kubernetes goes off and does the hard work for us. You know what, and I don't claim that this is my idea, right, but it is a lot like sending packages via courier services. So we package up our goods in their standard packaging, we label it, include a manifest, and give it to the courier, and they take care of everything else. All the complex logistics of which plane it goes on, which drivers to use, all of that is taken care of by them, and all they care about is that it's packaged properly and labeled up. Well, the same goes for an app in Kubernetes. Package it up as a container, give Kubernetes a declarative manifest, and it'll take care of the rest of the complexities of running it and making sure it stays running. It is a beautiful thing. Now, I don't want you to take this data center OS thing too far, right, I mean it's not a DVD install, and you don't end up with a shell prompt or a UI, and you're definitely not going to start playing solitaire or minesweeper across all the servers in your data center, it's not like that, at least yet, right. We'll see it in action soon, though, I just don't want you to take the analogy further than it's intended. Now, as well, it's early days in this space, and we're still taking relative baby steps, but there is no doubt about it, Kubernetes is out there, it's a frontrunner, and it's got a solid chance of being a winner in this space. Now, a couple of quick points to maybe answer some of the obvious kickback questions that you're no doubt thinking. First up, yeah, I know that this is a bit forward thinking, almost bleeding-edge, yeah, I don't know, right. The thing is, it's here, and it's real, so it's definitely a thing. Also, yeah, I know that most data centers are already sliced and diced into zones and the likes, so production zones firewalled off from DMZs, and dev zones, and third-party zones, and line-of-business zones, all of that jazz, right, but within each of these we've still got a bunch of compute networking and storage. And Kubernetes is happy to dive right in there and start throwing container workloads onto them. I mean it's about as platform agnostic as it can get, so it's cool with bare metal, VMs, cloud instances, open stack, pretty much anything. So long as you can install the agent, it is cool with it. What else? Oh yeah, yes, I know we're not massively bothered about which CPU core our app runs on or which DIMM it stores its working set on, but actually we do kind of care a bit when it comes to networks and maybe persistent storage. But that's fine, Kubernetes can handle that, and we'll see it soon enough. And I reckon that's about it, yeah. Kubernetes, right, a leading cluster manager or orchestrator that lets us manage containerized apps at scale. And it's a lot like a data center scale OS, we give it work to run and it makes the decisions about where in the data center to run it. And you know what? If it all seems like a bit out there, and maybe you're thinking what is this guy on about, trust me, stick around, I mean the course is only about 2 hours, and I promise you, by the end you'll be like wow, I get it, a proper light bulb moment. But here's my standard disclaimer. No, not that one, that is true as well, actually, but this is the one I want. This is a fast-moving world we're living in, and Kubernetes is a fast-moving project, but I've no doubt that every one of you are fast movers too. I'm just saying that in this brave new world, if you snooze, you lose. So crack on with the course, get on the Slack channel, follow it on Twitter, get along to a local meetup, but most importantly, right, get your hands dirty and keep playing with it. it's a skill that I'm convinced is going to pay you huge dividends in the future. So, next up, Kubernetes Architecture. Now that we're warmed up, right, this is where the proper learning really starts.

Kubernetes Architecture
Module Intro
Okay, this is the good stuff, and I hope you're excited, because at the end of this module you really are going to know about the things that make Kubernetes tick. So here's the plan. We'll start off with a view from like, I don't know, 40, 000 feet, right, because we need to have this big picture view of all of the major components before we dig into them. Well, once we've got that big-picture view, we'll take a look at masters and nodes. These are like the major infrastructure bits. Then we'll look at pods, the most fundamental, and I guess the atomic unit of deployment in the Kubernetes world, that's important stuff. Then we'll look at how services let us publish our pods or applications with more reliable networking. You know what, no, reliable networking is not the word. They let us give things well-known and stable IP addresses, and the likes, and they do a bunch more, right, but we'll get to it. Then I think we'll bring it all to a head when we look at deployment. Now deployments really are the latest and greatest way to deploy and manage our applications in Kubernetes, so it's really good stuff. Then we'll wrap the module with a quick fly-by summary of everything that we've learned. We'll probably go back and review the view that we took from 40, 000 feet, but it'll feel different, right, because we'll have all this new Kubernetes knowledge, yeah. Okay, well that's the plan, but I just want to say one thing, right. This is a getting started course, right, so we're not going to cover everything, I mean, not even close, right, and we're not going to be getting deep into the weeds. So, not everything, and not massive detail, huh. But really, don't get sad, I promise we'll absolutely cover more than you need to get on your feet and confident. I really do think you're going to love this course. So get a pen and paper ready, flip yourself into learning mode, and let's do that 40, 000-foot recce.

Big Picture View
So at the highest level, Kubernetes is an orchestrator for microservice apps that run on containers, and a microservice app is just a fancy name for an application that's made up of lots of small and independent services. But when you bring them together and they talk to each other, they create this more meaningful or more useful app. You know what, let's draw a picture, which I'm going to regret saying, because me and PowerPoint, man, we're not exactly best mates. Anyway, okay, if you've taken some of my Docker courses, this is going to look a little bit familiar, right. And if you haven't taken my Docker courses, repent. But look, right, in the real world a football team is made up of individuals. No two are alike, and each has got a different role to play on the team. Some sit back and defend, some push forward and attack, some are good at tackling, some at shooting, some at skinning people, you know the score. Nice, the score. Anyway, we've got a bunch of players with different skills and abilities, and then along comes the manager or the coach, and then he or she gives everybody a position, like a job, organizes them into a unit, and we go from this kind of mess to this beautiful formation. Something that I guess we can all say, looks like a team with a plan that can accomplish something useful. Cool, well, as well as that, the manager also makes sure that the team keeps its shape and it's in tip-top condition by subbing off injured players and the likes. Well, guess what, microservice apps in the Kubernetes world are just like this, seriously. We start out with an app made up of multiple services, each packaged as a pod, and at a massively high level at this point, obviously. But you know what, each one's got a job within the overall app. So we've got load balancers, web servers, logging, the whole picnic. And Kubernetes comes along, a bit like the coach in the football team analogy, and it organizes things so that they work together on the right networks with the right sequence, all of that hocus-pocus, and what we end up with is a useful app made up of lots of smaller moving parts. And we call this, what Kubernetes is doing, orchestration, so it's orchestrating all of these pieces to work together. Great, that's a lovely fluffy big picture again, right, but to make this all happen, well, this is our Kubernetes 101. So we start out with our app, right, we package it up and we give it to the cluster, the cluster being Kubernetes, and it's made up of one or more masters and a bunch of nodes. Now, look, apologies up front, right, if I sometimes call nodes minions. Old habits die hard, especially ones that you like. Hmm. Yeah, go on, actually back up with me for a second, right. Back in the day, these components were called masters and minions, and the world was a happy place. I mean, listen to it, masters and minions, it just makes sense, and it rolls of the tongue. But for whatever reasons, they changed it to masters and nodes, which is like, well, I mean the world is still a great place, it's just node. I mean, come on, there were loads of great ideas for new names at the time, and we chose node? Help me out here. I mean Docker has got managers and workers, and that goes pretty well, it's not as cool as masters and minions, but I'm telling you, it is way cooler than masters and nodes. I digress. Well, the masters are in charge, right, and they make the decisions about which nodes to run work on. And if you're like me, and you like to try and sound intelligent, then the bits and bobs that run on the master make up what we call the cluster control plane. So it's all that stuff that monitors the cluster, makes the changes, schedules the work, responds to events, all of that jazz, right, this is the master or the control plane. Then the nodes, minions, these run the actual work, okay. As well as that, they do stuff like they report back to the masters and they watch for changes. Now, we're in kindergarten right now. It really looks like this, ahhh. Well, you know what, we'll get into some of this later, but for now I think this is enough. So, yeah, that's our infrastructure stuff, masters and nodes. But at the start we said that we packaged up our application and we gave it to the cluster. Well, the best way to do that these days is probably as a Kubernetes deployment. So we've got our code, right, and we containerize it. Then we define it in an object called a deployment, and we'll come to this, right, but for now, all I'm going to say is to do this, we write it up, or we define it, in a YAML file. Look, it's just a manifest that tells Kubernetes what our app should look like. So, what images to use, what ports, networks, how many replicas, all that stuff, right, in a file. We give the file to Kubernetes on the master here, and the master looks at the file and deploys the app on the cluster. And then everything holds hands and sings Kumbaya, and there is our app, boom. But I'm keen to get deeper.

Masters
So a Kubernetes cluster is effectively a bunch of masters and nodes. Well, we need something to run these on, and we've said already that Kubernetes is pretty platform agnostic, so it runs on Linux, but it's just not opinionated on whether underneath that it's bare metal, VMs, cloud instances, open stack, it's just not bothered. Give it Linux, and it's cool. Now, scraping under the hood a bit, right, a master is actually a bunch of moving parts, and in the simplest and most common setups, they all run on a single server. Now, this isn't like set in stone or anything, and in fact, no doubt it'll change in the future. I mean, multi-master HA is already a thing. It's a bit complex to configure right now, but yeah, it's totally doable. And if we get the old telescope out and look further out into the future, I think it's totally reasonable to expect these master bits to get distributed all around the cluster instead of being centralized. Because, you know what, right now it is a bit of a monolith, in that we lump them all together in a big honking machine, and that's just not the way of things these days, so don't be surprised if it changes. Oh, you know what, while we're on as well, we generally consider it best practice to keep the master free of user workloads, so run your apps on the nodes, and keep the master free for looking after the cluster. It just keeps things clean and simple. Now, if we drill in a bit here and start peeling these labels off, yeah, we need to know about the apiserver right, this is a biggie, okay. It's our front-end into the master or the control plane. In fact, it's the only master component that we should be talking to. For example, it's the only one with a proper external-facing interface. And you know what, like all good things these days, it exposes a RESTful API and it consumes JSON. Now how that looks in the real world is, right, we send in our manifest files, which we mentioned already, these declare the state of our app, like a record of intent, the master validates it, and it attempts to deploy it on the cluster. Then there's the cluster store. Now if the apiserver is the brains of the cluster, then the store is definitely its memory. So the config in the state of the cluster gets persistently stored in here. Right now it's etcd. Yeah, there's been talk, like forever, about there being other options, but right now etcd is where it's at. It's battle-hardened, and you know what, in my opinion it is pretty production ready. Oh, hang on. This is bad of me actually. Apologies. If you don't know what etcd is, that's fine. So etcd is an open source distributed key value store. It's developed by the guys at CoreOS, and if key value stores aren't your things, it's a NoSQL database. But it's distributed, consistent, and watchable, and when I say consistent, right, I mean, yeah, all copies can be written to, but they talk to each other and they work out consistency. But I'm getting sidetracked. Kubernetes uses etcd as the source of truth for the cluster. So it is vital with a capital everything, right. No etcd or cluster store, no cluster. It would be a bit like taking a person and entirely wiping their memory. Alright, they might be pretty to look at and full of potential, but they're not a lot of use right here and now. So make sure you protect this store and you've got a solid backup plan for it. Alright, so what have we got under here, yeah, the controller manager. This is a bit like a controller of controllers, if you will, and dare I say, a bit of a mini monolith. But I don't want to get hung up on detail like this, because it's the kind of thing that could easily change, especially with something like Kubernetes that's evolving and improving at a rate that only software can. But right now, this component implements a few features and functions that I wouldn't be surprised if at some point in the near future they get split out into separate and even pluggable services. But right now, today, okay, we've got a bunch of controllers, node controller, endpoints controller, namespace controller, there's practically a controller for everything. And these guys all sit in a loop, and they watch for changes, the aim of the game being to make sure that the current state of the cluster matches our desired state. But like we said, right now they are all managed by the controller manager. Ah, okay, we've got the scheduler. This is a biggie, okay. It watches for new pods, and it assigns them to workers, and yeah, that's the high level, right, but it's doing it a huge disservice because it's got a ton of things to think about. So things like affinity and anti-affinity, constraints, resource management, there's loads more, but I think that's probably enough for us. Now then, because the apiserver is our front-end into the master, and it's the only component in the master that we really deal directly with, well sometimes we fall into just calling this the master. So when we say things like, I don't know, maybe issue commands to the master, or whatever, we actually mean issue commands to the apiserver, we just mix and match master and apiserver quite a lot in our lingo. I think the important thing to remember, though, is no other master components expose an endpoint for us, just the apiserver, and by default, that does it on port 443. So the master is the brains of Kubernetes. Commands and queries come into the apiserver component of the master, we can stick authentication on at this point, that's beyond the scope of this course, though. We've not covered this yet, but commands come in usually via the kubectl command line utility, but they're formatted as JSON. Then there's a bit of chatter goes on between all these components here, and depending on what's going on, commands and action items make their way to nodes over here. Speaking of which, nodes.

Nodes
So let's look at Kubernetes nodes. Now I promise I'm not going to moan about this for the rest of the course, but like I said before, they used to be called minions, and for me, that was the perfect name for them, because everyone knows that a minion is this unimportant-like underling or worker that just does what some powerful master tells it to. And when you understand how Kubernetes works, you're like, man, that totally makes sense. You see, Kubernetes is like way over on the cattle side of the pets versus cattle thing, and cattle is all about working nodes being these faceless, characterless drones that we don't really care about, and that we can swap out without even noticing. And for me, that's what a minion is, this unimportant faceless figure that just does what the master says. Then if it fails or dies, you know what, we just swap it out and it's business as usual. Anyway, let's take a closer look. Well, look, straightaway we can see that they're a lot simpler than the master, so there's only the three things the we care about, the kubelet, the container runtime, and then the kube-proxy. So first, and actually definitely foremost, is the kubelet. Let me be clear about this. The kubelet is the main Kubernetes agent on the node. In fact, it's probably fair to say that the kubelet is the node. So you install it on a Linux host, it registers the host as a node in the Kubernetes cluster, and then it watches the apiserver on the master for work assignments. Anytime it sees one, it carries out the task and then it maintains a reporting channel back to the master. If, for whatever reason, the kubelet can't run the work or maybe something goes wrong, it reports back to the master and the control plane magic on the master decides what to do. So, if a pod fails on a node, the kubelet is not responsible for restarting it or finding another node for it to run on, it simply reports the state back to the master. And, actually, speaking of reporting back, that reminds me, the kubelet exposes an endpoint on the local host on port 10255 where you can inspect it. Now, we're not going to dive into this on this course here, right, but I do think it's worth you knowing that port 10255 on your nodes, lets you inspect aspects of the kubelet. So, the spec endpoint, as we can see here, gives you some information about the node it's running on, healthz, that's a health check endpoint, and pods, this shows you running pods. And I mean there's more, but it's not for this course. Anyway, the kubelet works with pods, which we're going to learn about in a minute, but for right now, just think of a pod as one or more containers packaged together and deployed as a single unit. But because it's containers inside of pods, the kubelet needs to work with a container runtime to do all of the container management stuff, you know, things like pulling images and starting and stopping containers. But for the most part, the container runtime is going to be Docker, though, CoreOS rkt is also on the scene. So, a major part, I guess, of what the kubelet does is talk to the container runtime here. In the case of Docker, it uses the native Docker API, but yeah, it's pluggable, and if you prefer the rkt way of thinking, knock yourself out and go with rkt. Well, the last piece of the puzzle here is the kube-proxy. This is like the network brains of the node. So, for one thing, right, it makes sure that every pod gets its own unique IP. And, yeah, actually that is one IP per pod. So if you're a bit advanced and you're running pods with multiple containers in them, all of those containers are going to share a single IP. So if you want to address individual containers within the pod, you're going to be using ports, and the likes. But that's getting ahead of ourselves, right. Actually, no, you know what, on the point of getting ahead of ourselves, the proxy also does lightweight load balancing. So, hmm, getting ahead again. Among other things, right, a service is a way of hiding multiple pods behind a single network address. So let's say we got a bunch of web servers here, all talking to a backend down at the bottom. But we put this backend behind a service here, so a single IP and the likes, and well, we configure the front-end to take a look at to the service. Then it'll balance all requests across all three backends behind it here. And that load balancing is the responsibility of the kube-proxy. But, yeah, that's nodes. The kubelet's the daddy, and sorry about the blatantly male reference there, but you know what I mean, it's the main Kubernetes agent on the node. It registers the node with the cluster, and then it watches the apiserver on the master for work packages. Those work packages are basically pods, and then it reports back to the master on the status of the work that it's running. Well, sitting right next to it is the container runtime. This is usually Docker, but rkt from CoreOS gives us a choice there. Then there's the kube-proxy, the piece that makes all of the Kubernetes networking happen on the node. Good stuff. Well, we're going to switch gears right now, and focus our attention on pod services and deployments. But before we do that, I want to do like a half-time show here on desired state and the declarative model that Kubernetes uses. So here goes.

Desired State and the Declarative Model
So I want to stress the absolutely fundamental nature of two things in Kubernetes. I mean, without these, Kubernetes is just nothing, I wouldn't even be doing this course, right. Well, I'm talking about its declarative model and the concept of desired state. So, first and foremost, Kubernetes operates on a declarative model. This means we give the apiserver the manifest files that describe how we want the cluster to look and feel. We do not give it a long list of commands that it needs to run to get to that state, we just tell it what we want it to look like. Then it's up to Kubernetes to do whatever is necessary to get there. Time for a quick cheesy analogy, right, but it's a little bit like getting a contractor in and saying, right, we want a new kitchen building on the back of the house. We want it to be open plan into the living area, hook into the underfloor heating system, we want a lot of glass on the south-facing exterior overlooking the garden, we want access directly into the garage, and we want a roof garden on the top. I'm just making it up, right. But that's pretty high level, and it is describing what we want. What it's not doing is saying, ok, knock down this existing wall and hold the roof up with whatever supports you need, yeah. Build 30 courses of brick work for a double-skinned wall with pins every 18 inches, use 25 ml pipes for the underflow heating, blah, blah, blah. It is not doing any of that. Now, the analogy only goes so far, right, you probably want a lot of control over something like a new kitchen, but the point is valid. You describe what you want the cluster or the app to look like, use this image, make sure there's always five copies running, that type of stuff, and Kubernetes takes care of all of the hard work of pulling images, starting containers, building networks, running reconciliation loops, all of that, we don't have to care about it. Well, to do this, we issue Kubernetes with manifest files that describe what we want the cluster to look like. Then those manifest files are effectively a record of intent, and they constitute what we call the desired state of the cluster. Fabulous. But things go wrong, or things change, right, and it is totally possible for the actual state of the cluster to drift from this desired state. Who knows, I mean maybe a node fails, or maybe we even change the desired state. The point is, any time actual state diverges from desired state, Kubernetes gets like all, ahhh, that's not right, I must rectify, and it doesn't rest until desired state and actual state are back in sync. So as a quick example, right, let's say we've got a desired state that says we always want three instances of a web front-end pod running. Well, right now we've got three nodes, and one pod is running on each of those nodes, and that's just fabulous. We want three, and we've got three, so Kubernetes is relaxed and all chilling out. But then, horror of all horrors, one of the nodes goes down. Well, desired state still says, three pods please, but actual state says, uh-oh, only two pods, and this is like torture for Kubernetes, right, it just can't abide it. So, it kicks into action and fires up another pod on one of the surviving nodes. Great, this brings actual state back in line with desired state, and Kubernetes can relax again. And I know, right, it probably sounds really simple, but I'm telling you, it is ridiculously powerful, and it's right at the heart of how Kubernetes operates, so make sure it sinks in. We never interact with Kubernetes imperatively, or we shouldn't. We give it declarative manifest files that describe how we want the cluster to look. These form the basis of the clusters desired state. Then the Kubernetes control plane stuff is loading a load of reconciliation loops that are constantly checking that the actual state of the cluster matches the desired state. And when the two don't match, it's all hands on deck until they do. Right, well with that firmly stored in your head, let's go and look at pods.

Pods
In the VMware world, the atomic unit of deployment is the virtual machine, in the Docker world it's the container. Well, in the Kubernetes world it is the pod. So let's be 100% clear about this. Yes, Kubernetes runs containers, but always inside of pods. Thou canst not deploy a container directly onto Kubernetes. You see, a container without a pod in Kubernetes is a naked container, and Kubernetes has pretty strict views on nudity. I mean, look, I just made that up, right, and maybe I'll edit it out, I just thought it might help you remember. Now, you absolutely can run more than one container inside of a pod, but it's a bit of an advanced topic, I mean not mega-advanced, but we're not getting into it in this course. But anyway, even if a pod has only one container, I'm telling you, it's still a pod. You can't just say, oh well, since there's only one container there, might as well just deploy the container. No, not going to work. So, let's take a look at what a pod actually is. at the highest level, it's just a ring-fenced environment to run containers. So the pod itself doesn't actually run anything. It's just a sandbox, of sorts, to run containers in. So you ring-fence this area on a node, build a network stack, create a bunch of kernel namespaces, and then you run one or more containers in it. Now if you are running multiple containers in a pod, they all share a single pod environment. So, I'm talking about things like the IPC namespace, shared memory, the network stack, so IP addresses here. I mean, if you've got two containers in a pod, they both share the same IP. If they want to talk to each other, then they've got the localhost interface in the pod to do that over. As well, any volumes in there are also available to all containers in the pod. So, if you've got a use case where you need something really tightly coupled, maybe a couple of containers that need to share memory or volumes or stuff, then stick them in a single pod. But if they don't really need to be tightly coupled, and I know tight coupling is like blasphemy in some people's minds these days, so if you only need them more loosely coupled, honestly, stick them in separate pods and couple them over the network. Now, let me be crystal clear about this as well. The unit of scaling in Kubernetes is also the pod. So you want to scale a component in your app, you do it by adding and removing pods. You don't scale by adding more of the same containers to an existing pod. No, you want to scale an element up or down in your application, add or remove pod replicas. So if they're not for scaling right, they offer two or more complementary containers or application services, right, that need to be intimate. An example might be a containerized web server with a log scraper that you want right beside tailing the logs off to a logging service somewhere. In that situation, it might make sense to put these two together in a pod. And if you do that right, we tend to call the pod the main container and the log scrapper the, so-called, sidecar container, but it's just an example right. Now then, let's tie this back to the idea of pods being atomic. So the deployment of a pod is an all-or-nothing job. You never get to a situation where you're like, okay, let's scale this to another pod over here, and okay, let's stop bringing that pod up. Well, the pod's environment is created, and look, there's the first container up, so I'll tell you what, we'll mark the pod is up while we wait for the sidecar container. Nuh-uh, it doesn't happen like that. The pod gets deployed like this. It's not there, and then boom, it's there, the whole thing is up. Not there, there, not there, there, and I mean, okay, behind the magic curtain these things obviously don't just magically appear all at the same time, but the pod is never declared up and available until the whole lot is up and running. So as far as Kubernetes is concerned, it's atomic. Oh, and pods exist on a single node, right, it's atomic like that as well. You can't have a single pod spread over multiple nodes. What else? Pods are mortal. They're born, they live, and they die, that's it. They are never brought back to life, no Lazarus experiences. If a pod dies, the replication controller starts another one in its place. It is not the same pod resuscitated, resurrected, anything like that, it's a shiny new pod that just happens to look exactly the same as the one that just died. And that's the pets versus cattle thing again, right? Pods should be treated like cattle. Don't build your apps to be emotionally attached to pods so that when one dies you get sad and you try and nurse it back to life. It's not going to work like that. Build your app so that when pods die, a totally new one can pop up somewhere else in the cluster and just take its place. So they're atomic and mortal. I love the lingo. Now for the most part, we deploy pods indirectly as part of something bigger, like a deployment, but it is possible to deploy them directly by giving the apiserver a pod manifest file. Remember, that's our declaration of desired state, and it's a YAML file. Well, if we do that right, the apiserver validates it, ranks the nodes in the cluster, and then deploys it to one of them that satisfies requirements. Cool and all, but there's a pretty good chance you'll also see them deployed via replication controllers. This is another high-level construct like a deployment that takes pods and adds features around them. I think, as the name suggests, replication controllers are all about deploying multiple replicas of a single pod definition, then making sure that the required number of replica is always running. See, that's our desired natural state thing again, I can't tell you how fundamental that is, but replication controllers are being superseded a bit. You see, the same way a replication controller is a higher level than a pod so it adds more to the box of tricks, well, a deployment does the same thing, again, to a replication controller so it sits above replica sets and pods. Anyway, look, I only wanted to mention replication controllers in case you come across them. They are pretty widely used, but like I say, they're kind of being replaced by deployments. Now, we're going to mention deployments, but before we do that, let's take a look at services.

Services
So we've got applications, right, and we've said that in the Kubernetes world they're going to be made up of one or more pods. But we just learned that pods are mortal and can totally die. And when they do, they come back somewhere else in the cluster with totally different IPs. But it's not just when they die. Like if we scale an app and throw more pods into the mix, these all arrive with their own IPs. And then if we do like a rolling update or something, you know, like adding a new pod with a new version of code while getting rid of the old one with the old version, this ends up in a lot of IP churn. So the issue is, we just can't rely on pod IPs, and this is obviously a massive issue. So, as an example, right, let's assume we've got some microservice app with a persistent storage backend that other parts of the app use to store and retrieve data, pretty standard, right? But how is it going to work if you can't rely on those backend IPs? I mean, it's pretty darn inconvenient if the IPs change every time you push an update. But like we said, it's not even just that. Anytime you scale it up you get a bunch of new IPs, and then you lose a bunch when you scale it down. Well, playing Captain Obvious here, right, this is where services come into play. So at the highest level, right, let's say that this is a much simplified view of our app. We've got pods hosting the web front-end needing to talk to a couple of pods on the backend. Well, we slip in a service object here, and by the way, right, a service is just a Kubernetes object, like a pod or a deployment, so we define it with a YAML manifest, and then we create it by throwing that manifest at the apiserver. But once it's in place, and we'll see how it does all of this in a second, right, but it effectively sits in front of the backend here and provides a stable IP and DNS name for these pods down here. So, that's a single IP and DNS name here that load balances requests coming into those across the pods below it down here. Then, if one of the pods dies and gets replaced by another, no sweat, the service updates itself with the details of the new pod here, but keeps on servicing the same IP and DNS, and it load balances across the two pods again. The same goes if we scale the pods up or down here. All the new pods with the new IP, and the likes, get added to the service, and as if by magic, we're load balancing across four pod backends now. Then we rolling update the pods, the old ones get dropped from the service, and the new ones get added in, and it is business as usual. So that's the job of a service, a higher level stable abstraction points for multiple pods, and they provide load balancing. Now then, the way that a pod belongs to a service is via labels. And I just want to take a second to pause here and give a worthy tribute to the rolling value of labels in the Kubernetes world, because the labels, they are the simplest and the most powerful thing in Kubernetes. I mean, honestly, the power and flexibility they bring, well, it's something to behold. Sop labels, yeah, thank you for all that you do. Now back to the course, right. I know that might have sounded a bit weird, but when you've done a thing or two with Kubernetes, trust me, you're going to know exactly why I just did that. Labels rock. So let's roll this picture back to just the two backend pods, and let's throw some labels on them, as you do, right, everything in Kubernetes gets labels. Okay, we can see we've labeled the backend pods as Prod, production, BE is for backend, and we're in version1. 3. And then up here on the Service, right, see how we've got the same labels. And it's the labels here that tie the two together. So if we had some other random pod up here that was totally different, right, like nothing to do with the backend, but we labeled it the same, then the service is going to load balance across that as well. Now, we don't want that, obviously, but you see where I'm going, right. When deciding what to load balance over, or what to be the service for, it uses a label selector that says, okay, all pods and objects with these three labels are mine, and that's how it decides what to load balance over. Now let's say we're going to update the backend service to version 1. 4. Well, we can update the service here to say, okay, just use these two labels as the label selector. Then, as we add a new pod here, it's going to match, and it's going to be load balanced on. And then the same for this one here. And as these new versions come online and the old ones still maybe stick around, we're load balancing across them all. Then we can maybe say, okay, let's just make it version 1. 4 now by adding this label in, and suddenly, we're only getting the new version. And then we can flip back easily enough, so long as we've not trashed the old pods here, and this will totally depend on how you do your updates right. But I'm thinking you can see how labels make it really easy for us to do things like that. As well, though, and it's always really hard to know what to say, and maybe what to leave out in a getting started course like this, but a quick few things that I think you'll find useful. Services only send traffic to healthy pods, so if your pods are failing health checks, then a service isn't just blindly going to send traffic to them. They can also be configured with session affinity here in a YAML file, but at the time of recording this is not on by default. You can also configure a service to point to things outside of the cluster. What else? Load balancing, okay, load balancing is purely random, by default. DNS round robin is supported and can be turned on, but you know what, beware of crappy DNS code in your apps that might not flush the cache appropriately. Your mileage may vary with that. Oh, yeah, and they default to TCP, but UDP is totally supported as well. So, yeah, services, a cracking way to provide stable IPs and DNS names in the unstable world of pods. Next up, deployments.

Deployments
So we've got our infrastructure at the bottom, the masters and the nodes, and we know that the smallest unit of work we can deploy in them is the pod, and pods have one or more containers. But we threw it out there a minute ago that we don't usually work with pods directly. And for the longest time, we've been taking this higher-level abstraction called the replication controller and using those to deploy pods. And it was a good thing, right. They added things like scaling, and self-healing, and even rolling updates, though those were a bit of a _____ bolt or an afterthought. But you know what, as an example, right, we might deploy a replication controller that says, let's have full replicas of x particular pod running. Well, we define it in a YAML file here, and we throw it at the apiserver. And, hey, presto, we have four copies of the pod on the cluster. But, then if a pod dies for whatever reason, the replication controller is still asking for 4 desired state yeah, only now there's only three, so that means red alert and all hands on deck until we're back to 4. But when I say all hands on deck, I'm talking Kubernetes here, right, like I mean me as a developer and IT guy I sleep through the entire thing. No pager and no phone call in the middle of the night for me, thanks. Kubernetes can take care of all of that hard work. Well, replication controllers are great, it's just deployments are even greater. So at a very conceptual level here, right, deployments are all about declarativeness, which is what we said before. You tell the cluster what you want things to look like, and the cluster waves its magic wand and makes it happen. And I think, honestly, right, there's no doubt that this is the best way to run real-world production environments. Now, I mean, I get it, right, that when you're playing around and you're testing, you're going to throw a few pods out there and add some replication controller action to the mix, and things are going to be a bit declarative, but you know what, there's also a bunch of imperative stuff in there as well, and by imperative, right, just in case you're not massively comfortable with the jargon, this is like, well, it's like listing out a bunch of commands or steps on what to do, so do this, and do that, and hopefully you get an end result. It's like the opposite of the declarative model where you don't say do this and you don't say do that, instead you just say, hey, look, this is what I want, you go make it happen. And for the real world, especially for production environments, I'm getting like a broken record here, I know it, but you really want to be declarative. I mean, the thing is, right, it's self-documenting, it's versioned, it's great for repeatable deployment, so spec-once, deploy-many, all of that goodness. It is like the gold standard for production environments. Plus, you know what, it's really transparent, and it's just that simple to get your head around, and that's a massive bonus for cross-team cooperation and getting new hires, and the likes, up to speed. But there's more, right. Here in the Kubernetes world, it makes roll-outs and rollbacks insanely simple. and who doesn't want that. But I'm blabbering, so back on track. Just like pods and replication controllers, deployments are proper first-class REST objects in the Kubernetes API. So, we define them in a YAML file, or JSON, if that's your thing, I'm just a YAML guy, right, but we define them in the standard Kubernetes manifest files, and we deploy them by throwing those manifests at the apiserver, same as pods and replication controllers. And in the same way that replication controllers add features and functionalities around pods, deployments do the same for replication controllers, so they take them and they add more, though, let me point this out, well, two things actually. Firstly, in the deployments world, replication controllers have been replaced by replica sets. I don't know, they're basically the same, right, just think of replica sets as next gen replication controllers. Secondly, though, with deployment we really don't get involved with the replica sets and pods. Sure, they're all there and happening in the background, but they're automagically taken care of by the deployment. And, of course, this makes life simpler for us, bingo. Anyway, the things that they bring to the game, or add, or wrap around the replication controller, yeah, they're just the things that we said before, the powerful update model and the super simple rollbacks. So, rolling updates are a core feature of the DNA of deployments, and we can run multiple concurrent versions of a deployment in a true and simple blue-green, or even a canary fashion. As well as that, Kubernetes can detect and stop rollouts if they're not working. And like we aid, rollbacks are super simple. So I reckon that'll do for now. Deployments, yeah, they are the future in Kubernetes. They build on pods and replica sets, and they add a ton of cool stuff like versioning, rolling updates, concurrent releases, and simple rollbacks. But beware, like I said a while back, they're a bit new. Not like massively new, okay, but new enough that they're not as common out there in the wild as other objects, but things are changing, and I am confident they've got a bright future. Okay, cool, so now that we've learned like a ton, let's take a minute to look at the big picture again and reinforce everything.

Bringing It Home
Okay, so let's do our second fly-by at whatever we said it was, 40, 000 feet, was it? Anyway, we started out by saying that Kubernetes is all about running and orchestrating containerized apps, and we made the comparison to a football team, or soccer, depending on where you live. But then we got into how it all works. We start out with Linux. It's got to be Linux, but aside from that, you know what, we're not really bothered. Underneath it, we don't really care if it's honking bare metal, cloud, or whatever. Just give us Linux, and we'll give you Kubernetes. So, once we've got Linux, we slice and dice things as masters and nodes. The master is the control plane, where all of the cluster intelligence and power sits, and right now the vast majority of setups you're going to see are running a single master. But, multi-master HA is definitely a thing, and we should expect it to become more and more of a thing as time presses forward. We also said that there is talk of distributing all of this master stuff across the cluster, but we're not there yet. Speaking of which, though, right, these here are the main bits that make up the master or the control plane. And, yeah, I know, it's like quite a few moving parts, but the two that we care the most about are the apiserver here and the cluster store. So, the apiserver, right, this is really the only bit in the cluster that exposes a user-facing endpoint. So, the way we talk to the control plane and manage the cluster is through this apiserver. So we really don't want to be messing with stuff like the cluster store directly. Definitely not if you're at the level of this getting started course. Anyway, as well as the apiserver, like we said, we've got the cluster store here. This is where the state and the config of the cluster lives. Right now it's running on etcd and NoSQL key value store from CoreOS, and you know what, it's the only stateful part of the control plane. The rest of it is all stateless. But, yeah, then we've got the nodes, and you'll see references to minions from time to time. Just know that minion means node, and node means minion. And if the master is where all the control and the decision-making happens, then nodes are where the work happens. And from a complexity perspective, we can say that the worker stack here is way more simple than the master stuff. We've just got the kubelet as the main Kubernetes agent, and that manages the pods, then we've got the container runtime, like normally Docker, this is the bit that does all of the container-related heavy-lifting inside of the pods, and then we've got the kube-proxy that makes the Kubernetes networking happen. So, okay, we issue work to the cluster through the apiserver here. Then, all the pieces on the master work together, and they decide where to run the work. Then it gets dished out to one or more worker nodes who actually run it. Now as well as running it, they keep a reporting line back in case of any issues or state changes. And that's pretty much it, right. You know what, I'm not sure if we actually said this before, but every node or every worker out there, right, has its own set of default system pods, stuff like logging, and health checking, and DNS, and all of that. Anyway, then we talked about pods, services, replication controllers, and deployments. But in uber-high-level, these are all REST objects inside the Kubernetes API, so you post them to the apiserver in order to create and manage them. Well, the pod is the atomic unit of scheduling in Kubernetes. There's no like running containers directly on the cluster, well, not via Kubernetes. And, you know what, doing it manually outside of the kubelet, yeah, that's not somewhere you really want to go, not at this level, at least. So we work with pods. Then on top of them we said that replication controllers let us scale them and maintain a desired state. But we also said that these are getting kind of pushed to the side a bit by the newer and more powerful deployment object. So, a deployment is really just this feature-rich abstraction that helps us manage pods. So, for instance, right, they let us define a desired state. They also let us easily change that, so scale it up and down, roll out new versions, run multiple versions side by side if that's our thing, and even rollback to previous versions, all of that magic, right. And all of it through this super-declarative interface. Oh, and I think we also said that services are useful for defining stable networking for pods, and as well as some basic load balancing. But, yeah, congratulations, that's the theory. Hopefully it's settling in a little bit and making sense. But if it's not, don't sweat, right. It's totally normal for new stuff like this to settle in properly. Especially, right, as I know that some of you obviously hate me voice, because you're always telling me that you crank it up to like 1. 5 speed to get through it all quicker. But you know what, playing me faster like that, yeah, obviously it means you have to listen to me less, but it also gives you less time to let stuff sink in, so beware. And, I mean, look, don't come to me saying, hey, Nigel, something's not clear, and then in the next breath saying oh, yeah, and by the way, I listened to you at 1. 5 speed. I mean, come on. But, yeah, if it is still a bit vague, I don't know, maybe play the module back again, and maybe at a slower speed, then I'll sound really slow. But, you know what, I don't know, you might just want to carry on and see if things become clear as we start playing with them, instead of just looking at pictures. I really don't know, though, the choice is yours. Just don't get worried if you're feeling like you're drinking from a firehose. We all feel like that at times, and honestly, if you persevere, it usually comes good, so don't give up. Well, anyway, look, next on the cards, we're going to change gears a little bit, and we're going to actually start installing Kubernetes.

Installing Kubernetes
Module Intro
Minikube
So, you know when you see something that looks interesting, and you're like, yeah, that looks cool, but I don't know, it'd be great if I could try it out. You know like reading a sample chapter of a book on Amazon, or, yeah, like maybe taking a car for a test drive. Well, in a way, right, this is Minikube. You look at Kubernetes and you think, right, looks good, but you know what, I'd like to spin it up and have a play. Well, Minikube is a great way to spin it up and play around. And let me make a comparison here. In the Docker world, there's Docker for Mac and Docker for Windows. These are the simplest ways to spin up a local Docker environment on your laptop. It's not production, but heck yeah, it's pretty darn good for getting yourself a fully working local development environment. Or if you're like us, and just learning it, it's a cracking way to spin something up that you can build and trash with ease. So, Minikube is to Kubernetes what Docker for Mac and Docker for Windows are to Docker. In fact, under the hood they're pretty similar as well. So architecturally, and let's put this one side by side, right, when we spin up Minikube, it goes away and it creates a local VM. So we're on our laptops or whatever, yeah, we download the Minikube installer, and we go minikube start. That creates a VM, then inside of the VM it spins up a single node Kubernetes cluster. Magic. But it also sets things up so that the Kubernetes API server and what have you, inside of the VM, are all available outside of it on your laptop's environment. So all I'm saying is, right, I can then use the kubectl binary that I've already got on my laptop, and I can use it to manage the cluster inside the Minikube VM. And, you know what, I mean it's totally cool to know that it's all wrapped up inside of a VM here, but really, the integration is so seamless that you probably don't even need to know that. But I like knowing stuff, right, and I don't know, I always end up diving deeper than I planned in these things, but that's good, right. So let's have a bit of a closer look. Inside the VM here, there's really two high-level things that we're interested in. First up, there's the localkube construct. It's a binary, actually, and it's pretty cool. This is what runs all the Kubernetes cluster stuff, yeah. So a master with the apiserver and all that, plus a node. Then, off to the side here, still inside the VM though, right, we've got the container runtime. Right now that defaults to Docker, but you can go with rkt as well. In fact, you know what, at some point I imagine we'll see rkt replace Docker as the default. And I suppose that's a bit of crystal ball reading from me, right, but it's going to happen. Rkt is just more of a natural fit with the Kubernetes architecture. But, yeah, that' the Minikube architecture. Oh, and over here, right, remember, outside the VM we've got kubectl, or kube control or kubectl, or call it whatever you want. Well, it's pretty much our Kubernetes client, and we can use that to control Kubernetes inside the VM here. Zooming back out, though, the architecture stuff doesn't really matter so much. You see, this is all about a slick and smooth experience. So the stuff under the hood, really, I suppose it could all change, and it will, right, but the user experience won't. We'll always get that authentic Kubernetes experience, one that looks, smells, and feels exactly like real world production. Anyway, I'm waffling, right, so you can get Minikube for your Mac, for Windows, and for Linux. But you know what, it needs virtualization extensions enabled in the systems BIOS, so you know what, good luck using it on your public cloud provider. You know what, though, we'll take a quick look at Mac and Windows because I imagine that's what most people run on their lappies. So, here we are in the Mac world, right. First up, I'm going to install kubectl. The instructions for this, okay, they're all going to be in the course notes, but don't base your world around them. Things like this are going to change over time, and when they do, I don't know, just google it, right, and you'll get the latest and greatest. None of that, though, is going to change the big-picture walk-through we're giving you here. Okay, so that puts the kubectl binary in usr/local bin, and it makes it executable. Quick check. Okay, now let's install minikube. Okay, now then, I'm no longer a fan of VirtualBox. Yep, I have seen the light, so actually I want my minikube VM to use X-Hive. So let's use brew to install that as well. Okay, and that needs these two commands here to set the user owner as root, and then this one to allow setuid, so let it run as root, rather than my nigelpoulton user account here. And you know what, I reckon we're good to go. We installed kubectl, we've got minikube, and we've got the X-Hive VM driver, yeah, let's go. So the cool thing, right, about minikube, okay, I mean after satisfying the prereqs, but we can spin the whole thing up with a single command. So minikube start, this fires it up, but I'm going to give it the vm-driver flag here, just because it still defaults to VirtualBox on the dark side. But if we give that a few seconds, boom, minikube started. Now a couple of things, right. This line tells us that kubectl is configured to talk to minikube, so if we go kubectl config, and then current-context, right, it's talking to our minikube. And without getting too far ahead of ourselves, right, but kubectl can be configured to talk to any Kubernetes cluster, so I can use this same kubectl here to talk to my production cluster, then switch it over to minikube. All I need to do is switch contexts. Anyway, let's list the nodes in the cluster. Cool, that's our single-node minikube cluster. It's ready, and okay, so it's not the latest version. Never mind, though, remember how we spun it up with a single command? Well, we can tear it down with a single command as well. Okay, now stopping it keeps the config that we've got in there so that we can start things back up where we left off if we want to. But I'll tell you what, we'll just blow this away properly, so that's minikube delete, and how simple is all of this? But you know what, going back to that version thing, right, let's say we want to run a particular Kubernetes version, I don't know, maybe to match our production environment or something. Well, where's that start command? Okay, and if we tag this onto the end here, and we'll have 1. 6. 0. Okay, let's see that. And, you know what, while that fires up, it kind of bugs me a little bit how sometimes it's fully spelled Kubernetes and then in other places it's shorthand Kubernetes. Never mind, though, it's pulling that 1. 6. 0 version. Alright, we're looking good. Let's check that, though. Bingo. Alright, so that's minikube on Mac, super simple, and you know what, it's no doubt going to get even easier in the future. Now for Windows though. Now first up, in the Windows world, okay, things are a little bit newer, so definitely expect this to get a lot slicker very soon. Now, again, first up I'm going to get the kubectl binary, which actually is not such an intuitive task for Windows. No doubt this will change as well, in the future, but right now the easiest way I see is to point your browser to this URL. Alright, that's downloading. Now if you take a quick look at the URL, actually, you can change this bit here to specify the different versions. Anyway, once it's downloaded, copy it into your path. Now over to the Minikube Releases page on GitHub, and I'm just going to grab the 64-bit Windows installer here, and let's fire that up. Okay, just a standard installer, yeah. But look at that. There's evidence right there that this is not the finished product yet. It looks like my 6-year-old drew that in Paint. But we'll crack on. And that's it. So if I pull up a command prompt here, minikube version. Alright, I think we're good. Now in the Windows world, right, again, at the time of recording this tries to default to the VirtualBox driver. Well, remember, I am done with VirtualBox, so I am telling it hyperv, and you know what, I'm going to tell it the Kubernetes version as well here. Okay, that's ongoing. We'll give it a minute or two. Or, you know what actually, let me fire up the flux capacitor here and jump forward in time a couple of minutes. Alright, we're ready. Well, like with Mac, we've got a fully-working single-node cluster that we can throw kubectl commands at. And that's it. I love it. Ah, actually, yeah, if you just tighten minikube, right, then yeah you get a list of all the minikube subcommands, and I wanted to show you this one here, dashboard. Okay, so this is opening up the Kubernetes dashboard in a browser tab here, and we'll probably cover this later, right, but the dashboard is a great visual tool for inspecting your cluster. So we can see our node here, and if we hit the system namespace here, because the default namespace is empty, seeing as how it's a shiny new cluster. But inside the system namespace, we see, for instance, the pods that are running, these are system pods, yeah, so for running the cluster itself. And, yeah, that's it. I highly recommend, if you're new to this stuff, right, take some time to poke around in here, you'll probably learn a bunch. But, yeah, that is Minikube. I reckon, the easiest way to spin up what is pretty close to being a fully-fledged Kubernetes cluster locally on your laptop. I mean, look, yeah, you satisfy a few simple prereqs, but then it's as simple as minikube start, and in a matter of seconds you've got yourself a feature-rich, single-node Kubernetes cluster. I mean, it's not for production, right, but you want a simple local development environment or something just to play around with, and I'm telling you, minikube is the way. Well, you know what, next up we're going to spin up something a bit more production worthy inside of the Google Cloud.

Google Container Engine
Alright, then, Google Container Engine, only it's container with a K. So I want to be absolutely clear about this right from the top. Google Container Engine is Kubernetes. If you peel back the covers, or if you pop open the hood, everything you're going to see in the engine bay is Kubernetes. So K here is for Kubernetes. Now, a super-fast bit of background just to set the scene. Google Kontainer Engine is layered on top of Google Compute Engine, so GCE provides like the low-level virtual machine instances, and then GKE lashes the Kubernetes and container magic on top. And the aim of the whole game, right, is to make Kubernetes and container orchestration simple. Like we said, this is Kubernetes as a service, so all the goodness of a full-blown, feature-rich Kubernetes cluster, pre-packaged and ready for us to consume. Anyway, look, here we are in the Google Cloud console. I've already created a project, and I've set up billing. If you are new to the Google Cloud, then you're going to need to create an account, create a project, and set up billing, and it's all dead easy, right, so I'm not going to show it here. It is straight to the Kubernetes stuff for us. So we can see my project up here, ps-kube, ps for Pluralsight, and kube for Kubernetes, yeah. Well, first up I suppose we need to find the container engine, so if we come up here and then down under the COMPUTE section we can see Container Engine. Alright, so we can see two things now. First up, Container clusters. This is where we're going to be working, and as we build stuff we'll start to see it appear over here on the right. Underneath it here, though, is the Container Registry. This is Google's hosted Docker registry, but we're not bothered about that for now, though. So what we'll do is we will hit Create a container cluster here. And being super, super clear, right, when it says container cluster, it means Kubernetes cluster, look. Now, I'm not in a massively creative mood right now, so I'm good with calling this cluster-1, I'm not bothered about a description either, but actually I do care about zone. I'm in Europe, or actually I'm not sure, Brexit and all, but yeah, I think we'll go with Europe-west1-d here. In Google Cloud parlance, that is Zone 1-d in the Europe West region. The Machine type, I am feeling really tight, so I'm going to go as small as they come. But, actually, of course, right, there's a whole bunch of options to choose from. I mean, 64 CPUs and 416 GB of RAM, man, my kids wouldn't be able to eat if I chose that. Anyway, under Node image here you've got two options. Container-vm, this is the legacy choice these days, right. It's an old Debian-based image, and it's really been superseded these days by this container-optimized OS that's based on Chromium OS. Never mind all the detail, though, the container-optimized image here is the future. It's based on a way newer kernel, and it's supported by engineers at Google, so I'm having that. I'm happy with three nodes in the cluster. Oh, actually, yeah, let me be clear. That is three nodes, right, so three minions. It doesn't include masters. All of the master stuff, like the apiserver and scheduler, and all of that, right, is taken care of behind the scenes by the platform. So, when we're building stuff like we are here, we don't need to concern ourselves with the master bits. I am cool here with this about local storage being ephemeral, at least I'm cool for it for the demo, right, of course your particular requirements might be different. Now this Automatic upgrades thing here lets the platform, so Google Container Engine, keep our nodes up to date. I'm going to say No for now. In fact, let me come and look at zones here, right. Yeah, see how there's a europe1-b and 1-c and 1-d, but no 1-a. Well, a couple of years ago now, I mean it really was ages ago, they took 1-a down for supposed upgrades, and look, it's never come back. So for now, right, I'm going to manage upgrades to my nodes myself. I don't want to trust Google, look what happened to 1-a. Now, look, I'm being silly right, No, to be fair, do you know what, at the time of recording, this stuff is in beta, and you really do always want to be careful with beta features. Well, I'm going to leave Automatic repair off as well. We're not getting into networking, and I'm leaving the monitoring and logging as they are. Now then, you really can drill into the weeds here with the More option. Check all of that out. It's beyond this getting started course, though, but by all means, have a poke around yourself. And I'm going to hit, oh, no I'm not actually. I like these links here. So we can see the actual REST requests here, or the gcloud command line that will actually be used to create this Kubernetes cluster. So we can either post this REST request here to the relevant endpoint, or we can run this whopping gcloud command line, and both of them will create our cluster. Pretty cool, in my opinion. Anyway, let's create this thing. Now, I'm not a patient man, so let me hit my warp space time button here. Okay, that's all done. This is our cluster. We can see the Zone, our 3 nodes, and the total CPU and memory for the whole cluster. Then if we click the cluster name here, right, we get all the gory Kubernetes detail. So, for example, we can see our Master version, this is the apiserver endpoint IP, we can see where it is, master and node stuff, and a bunch more. Now, I really like this as well. If we hit the console icon up here, give it a second, we get a shell. Okay, so if I go like gcloud container clusters list, right, that's our cluster there. And if we hit this connect to cluster here, copy this command, and if we put it down here in the gcloud shell, that's going to configure kubectl to use our cluster. So let's list our three nodes, maybe, right, pretty snazzy I think. But you know what, that was such a walk in the park. I want to make sure that we've not missed what we've actually done. We have literally spun up a 3-node Kubernetes cluster. Okay, it's an empty cluster right now with nothing running on it, but it is a production-grade cluster. Well, okay, so this particular one is running on the cheapest of cheap micro-instances, but aside from that, like all the Kubernetes stuff of the master and the nodes, this really is production grade, and we did it without even breaking a sweat. Oh, and you know what, if we grab this endpoint IP here, and we break out a new tab, and then if we https to it, okay, back here I think, yeah, so Admin, and we'll copy this, no thanks. And I'm not bothered about you seeing my password and the likes. This will all be long gone by the time you're watching this. But if we lash UI onto the end here, look, we're at the Kubernetes dashboard. So, yeah, there we are. You can now sit back and bask in the knowledge that you know how to spin up Kubernetes clusters in the Google Cloud. But there's no rest for the wicked, I'm off to do it all again. This time, over the cloudy border into AWS land.

Installing in AWS with kops
Okay, so let's take a look at how we might get a Kubernetes cluster up and running in the world of Amazon Web Services. Now like everything else in the Kubernetes world, what I'm about to show you here is pretty new, and I don't don't mind saying, you know what, it is a bit rough around the edges. Well, first up, yes, there is more than one way to spin up a Kubernetes cluster in AWS, but I've decided to go with the kops option, kops being short for Kubernetes operations. It's pretty new, right, but I think it's got a bright future. Now when I say it's new, I mean, like, as I'm recording this, the only cloud provider it currently supports is AWS. And you know what, unfortunately there's no kops binary for Windows yet, just Linux and Max. That'll obviously change over time. Also, in order for this to work you're going to need to satisfy a few pre-reqs. Now before I list them, I don't want any of them to look at them and think, mm, yeah, you know what, I think I'll skip this thanks. Don't do that. They're not hard, and I'll walk you through most of it. So here we go. You're going to need kubectl, you're going to need the kops binary, and also the AWS CLI. I'm going to roll this video with Linux, but they're all available for Mac as well. And you know what, maybe they're all available for Windows as well by the time you're watching this. Anyway, on the AWS side of things, you're also going to need the credentials of an IAM account with these permissions. And finally, I've got a DNS subdomain hosted on AWS route 53. So, I've got an existing domain called tech-force-one. com, don't ask, and that's hosted on GoDaddy. Well, I've created a delegation for a subdomain called kubernetes, to AWS route 53. In fact, you know what, let's look at the DNS stuff first. So I've got this DNS domain, yeah, tech-force-one. com. and it's being managed here by GoDaddy. Well, I've configured this subdomain, k8s. tech-force-one. com to delegate responsibility to four AWS route 53 name servers, that's these four here. Then here in route 53 I've got it set up as a hosted zone with these four name servers, that's the four that we saw in GoDaddy, yeah. The moral of this story. I had an existing domain with a third-party provider, and I'm telling you this so that you know you don't need everything inside of AWS. So an existing domain, yeah, I've created a subdomain for my Kubernetes rig, and I've delegated that to AWS route 53. And this dig here shows that it is up and working. So that's the DNS. Sticking with AWS, though, I need the creds of an account with decent permissions for the rest of this to work. Now I've already downloaded the keys from an account, because this is a one-time operation that you can only do right after they've generated. But if you've got an IM account but you can't find the keys, well you've got this option here to create a new access key and then download it. I've already got mine, though. Okay, so DNS is good, and I've got my AWS creds, time to install some stuff. First up we'll go with kubectl. Now I'm on Linux here, right, but if you're on Mac it's just brew install kubectl. For me, though, I'm going to use this call command here to get the latest stable release. Now, all of these commands are in the course notes, by the way, so don't stress yourself trying to copy them down. Okay, I'll run these two commands here as well to make it executable, and then to put it in my path. Alright, that works. Next we need the kops binary. Again, it's a curl command. Now, you can head over to the releases page on GitHub to manually pull it down or see the latest releases and stuff, but this is good for me. Again, we want to make it executable and stick it in my path. Cool, now for the AWS CLI. This one is nicely packaged so I can install it like this. Okay, that looks good as well. And that's pretty much how pre-req is done. Now I know they're a lot, but I do expect them to get bundled and simplified in the future. Anyway, though, let's configure AWS. So this is where I need those creds. And you know what, don't worry about me showing them on the video here, they'll be long gone by the time you're watching this. Alright, well, kops need somewhere to store its config and its state, and you know what, as we're in AWS we'll just create an S3 bucket for it. So we go aws s3 mb, and we'll call this one cluster1. k8s. tech-force-one. com. Just check that that's there. Alright, next we need to tell kops about its config, right. So we export this environment variable, KOPS_STATE_STORE, and in the real world I know you're going to want to put this in your shell profile script or something, but this is what tells kops where to find its config. And that's it, we've created the bucket and we've told kops about it. Now then, the next command is the one that's actually going to create the cluster. And to do that, right, it's going to need access to our AWS. Well, right now it expects to find the AWS key in a file called id_rsa. pub, and that's in your profile's hidden ssh directory. So I'll run this command here to create that file, which doesn't exist on my machine yet. All it's doing is making a copy of the authorized keys file, yeah. Anyway, now to finally create this cluster. So we use the kops command and we say create cluster. I'm going to split this over a few lines to make it easier to read. So I'm going to say, alright, let's use aws, and you know what, we will go with the eu-west-1b zone. Alright, I'm going to tell it to use this dns-zone, and then I'm telling it to give the cluster this name. And then this ---yes flag here is important, right, okay. So you can run the command without the --yes flag, and it's going to create a cluster config for you, but it won't actually deploy the cluster. If you do it that way, you need to run another command, kops update cluster, to actually deploy it, effectively making it a two-step approach. So you create the cluster config with kops create cluster, you review it, then you deploy it with kops update cluster. You know what, though, I'm just going to wing it and go straight for the deploy. Alright, away that goes. Now, what it's actually doing here is creating a whole bunch of stuff in AWS. Now, we can control the AWS stuff with more flags on the kops command line, I just wanted to keep this example simple. But it's going to be creating things like, well, it'll create a vpc, for starters, a bunch of ec2 instances, a launch config, auto scaling groups, tons of stuff, plus, obviously, the Kubernetes stuff as well, yeah. Okay, so that's deployed. So if we grab this command here and validate it. Right, so three nodes, one master and two minions, and we can see the instance types here as well. Now, this one master and two minions is default, right, we can obviously tell the kops create cluster command how many masters and how many minions we want. And you know what, you should definitely play around with the different command options like that. But we're here, and we pretty much get the same thing from kubectl. Brilliant, that's our cluster up, shiny and new and ready for us to use, and it wasn't hard at all. And I think if we have a quick look in AWS, why not. Okay, we can see a vpc for the cluster. If we look at instances, right, we see the two nodes and the one master. What else? You know, our auto scaling groups. Right, again, one for the master and another for the nodes. And all of this, remember, was set up for us by the kops command. We didn't have to create any of this stuff manually. So pretty cool, yeah. That's a Kubernetes cluster up and running in AWS. But if you're just playing around, right, getting rid of it is just a simple kops delete cluster command. Give it the cluster name, and tell it, yeah, I really want to, and away that goes. And you know what, that's creating a Kubernetes cluster in AWS. Way better than it used to be, but still a few rough edges that need to be smoothed out, and they will be over time, right. Well, next up, our last install video, the manual install.

Installing Manually with kubeadm
Okay, last one, the manual install. And I know I already sound like a broken record on this, but of course, there's a zillion and one ways to install Kubernetes the manual way. For this lesson, though, I've decided to show you kubeadm. It's a core project alright, and I get the warm fuzzies over it. I reckon it's got a pretty good road map. And yeah, like kubectl, there's like a zillion and two ways to pronounce it. So there's kube admin, kub admin, kube adam, kub adam, kube adm, kub adm, take your pick. And if anyone tells you you're pronouncing it wrong, then tell them they're free to go and get a life. Now then, again, please don't hate me for this, but right now, like while I'm recording this, kubeadm is pretty new. But, the folks that are involved with the project are on record as saying they're trying hard not to change things like command line arguments. So what we're going to see here is going to be pretty solid for a while. Now, we're going to go with a pretty simple install, the idea being that that'll be enough to bootstrap you on kubeadm, and then you're in a better position to go and check out the command reference and do a bit of poking around. So, for this lab, right, I'm going to go with three machines. For me, their instance is in AWS, Ubuntu 16. 04 Xenial, but that's not hard and fast. You can go with bare metal physicals, local VMs on your lappie, take your pick of public clouds. Really, kubeadm is pretty agnostic. Anyway, I'm going to build node 1 here as the master, and then I'm going to add in nodes 2 and 3 as workers. And right now, okay, these are the pre-reqs or the steps that we're going to take. All three nodes are going to get Docker, the kubelet, kubeadm, kubectl, and the CNI. Now a quick pit stop, right. Docker is the container runtime, kubeadm is the tool we're going to use to build the cluster, kubelet is the Kubernetes node agent, kubectl is the Kubernetes client, and Kubernetes CNI installs support for CNI networking, now the CNI being the container network interface, or the spec or model for Kubernetes networking, yeah. Well, then we'll initialize a new cluster. We'll create an overlay network for the pods, and we'll add some worker nodes. That's the plan. Alright, then, so this is node1. Now I'm going to run these commands here, right. They're all in the course notes, but they just make it so that we can install the right packages from the right repos. But remember, though, I'm doing this on Ubuntu 16. 04 xenial. The packages and repos will be different if you're using a different distro. So, yeah, this bit, I guess, is a bit OS-specific. The rest less so. Okay, now let's install those packages we mentioned. We want Docker, kubeadm, because we're going to use that to spin up the cluster, kubectl, because we couldn't live without that, the kubelet, and the cni. Okay, now while that's installing, I want to make a point about Docker. Docker supporting Kubernetes is always a bit interesting, right. Now you might be totally fine going with the latest version from docker. com, but then again, you might not. So the way we're doing it here is we're getting a specific version from the repo we added right at the very top. Okay, so that's done, and the best way to see versions is just to run it again. Okay, right, so 1. 12. 6 of Docker, and 1. 6. 1 of everything else except the CNI. Well, that's the pre-reqs, now let's create that cluster. So if you know your Docker Swarm mode at all, then you're going to feel right at home with this. We just go kubeadm init, and of course, right, there's a tone of flags that let you be opinionated about stuff like networking, yeah. But this on its own is enough to start a new cluster. Alright, that's pulled all the required images, created the cluster-related system pods, and we should have our single node cluster, though, our first order of business is to grab these three commands here. I'm root, so if I just drop down to a regular user, and let's run them. Now all they're doing is copying the Kubernetes config file from etc/kubernetes, putting that in my HOME directory, changing its ownership to me, and then exporting an environment variable that tells Kubernetes where to find this config. You'll probably want to make that environment variable a more permanent part of your shell in the real world. Anyway, now we should be able to use kubectl. Okay, that works, but see this NotReady here? Well, if we go kubectl get pods ---all-namespaces, and ---all-namespaces just says show me the system pods as well, Right, there's the reason for the NotReady, 0/3 on the kube-dns pods. Now, they're not going to come up at all until we've created a pod network. So for that, and there's a world of choice out there for Kubernetes pod networking, but I'm going to go with a multi-host overlay from Weaveworks. So, if we go kubectl apply, this is the shiny new way of installing plug-ins or add-ons, tell it to use a file, and this one. Now, then, there were some pretty big changes that happened with Kubernetes 1. 6, right, so the 1. 6 config file here is specifically for Kubernetes 1. 6 and higher. If you need to roll with older versions, then you just leave off this 1. 6 bit here. Alright, that looks good, so let's take another look at those nodes. Alright, we're ready now. Check the DNS pods though. Right, 3/3 and now up. So, yeah, the kube-dns pods, right, these will not come up until the pod network is in place. But we're still just one node, right. Let's add a couple of minions. And to do that, we need the cluster's join token, which actually, if I scroll back up here to the cluster creation, yeah, we get the full command that we need. So we grab that, and we can just run it over here. Oh, nice, on node2. A bit of color always brightens things up, yeah. Well, first up, right, I've got to point out I've already installed the pre-reqs that we've listed in the beginning, so Docker, kubeadm, kubectl, all of that good stuff. Every node we want to join is going to need those. So let's paste that in, and that's added. Do the same again on node 3. Alright, looking good. Back to the master and list those nodes again. Hey, cue fireworks and celebrations, that is our 3-node manually installed cluster ready to rock-and-roll. Though, do remember, right, it is just a single master 0 HA cluster, so be aware of that. Well, you know what, time to stick your feet up for a minute while we do a quick recap of the installs we've covered.

Module Summary
Time for the highlights. I think we made a lot of noise about there being a lot of ways to install Kubernetes, but we talked about four. First up, we talked about Minikube, which is all about spinning up a local Kubernetes development environment on something like your laptop. If you know Docker for Mac or Docker for Windows, then Minikube is pretty much the same, only for Kubernetes not Docker. So it spins up a VM, then inside that VM it runs a small Kubernetes rig. You can then point your kubectl client at it, and play with Kubernetes all day long. Then we looked at Google Container Engine. This is part of the Google Cloud, and it's Kubernetes as a service. So get yourself an account on the Google Cloud, and it's dead simple to fire up a production-grade cluster. And remember, all of the master or the control plane stuff is taken care of for you. Then we looked at how we might spin something up in AWS using the kops tool, short for Kubernetes operations. It's not quite as integrated as the Google Container Engine is with the Google Cloud, but saying that, kops does take care of spinning up all of the AWS resources that you need, like instances, VPCs, and auto scaling groups. So it is pretty integrated, and it's really simple. Then we wrapped the module up with a manual install using kubeadm. I think, after satisfying a few pre-reqs, we could spin up a cluster with a couple of commands. There was kubeadm init, that creates the new cluster, then we add nodes or minions with kubeadm join. Now, of course, there's a ton more ways to install Kubernetes, including tools and products from some really good third parties. But the ones we covered here are all native tools, and they're the ones that I think should have a long and a bright future. Okay, well, coming up next, we're going to take a look at the atomic unit of scheduling in Kubernetes, Pods.

Working with Pods
Pod Theory
Right, we've got the theory down, and we know how to spin up a cluster, so the next logical thing is pods. So this is how we're going to roll with this module. It'll be pretty simple and pretty short. We'll lay down the theory, then we'll get our hands-on. But this is the cool thing. For the hands-on part, we're going to deploy a Kubernetes app, a really simple one, right. I think like the equivalent of Hello World, something to signify you coming out as a Kubernetes newbie. Anyway, look, to frame our discussion around pods, or to give us some perspective, in the VMware world or the HyperV world, or just the virtualization world in general, the atomic unit of scheduling is the virtual machine. You want to deploy some work in those worlds, then you stamp it out on a VM. In the Docker world, that atomic unit is the container. Now, I know there's services and stacks and the likes, but the smallest, most atomic unit of scheduling in the Docker world is the container. Well, in the Kubernetes world, it's the pod. So, take that, and file it away in your brain as mucho important. Virtualization does VMs, Docker does containers, and Kubernetes, that does pods. And you know what, right, a pod is like somewhere in between a container and a VM. It's bigger, and I guess arguably a more high-level construct than a container, but it is nowhere near as big and clunky as a VM. So, okay, right, a pod contains one or more containers. Usually one, but multiple containers inside of a pod is definitely a thing. In fact, it's brilliant if you need to tightly couple services. Anyway, you deploy a pod to a cluster by defining it in a manifest file. Then you feed this manifest file to the apiserver, and then the scheduler deploys it to a node. And like we've said, depending on what's defined in the manifest file, that pod is going to have one or more containers. But, and here's where we get to dig a little bit deeper, irrespective of how many containers are inside the pod, the pod just gets a single IP. So it's not like an IP per container thing here. This is a one IP to one pod relationship. So, look at the example we've got here, right, two pods and two IPs, one IP for each pod. Then, if we look at Pod 1, we can see it's actually got two containers. Well, those are both available at the single IP of the pod. See how one is on port 80 and the other 5000? So actually, this IP thing, right, is really a network namespace, so every container inside the pod shares a single network namespace. So that's the single IP address that we already said, but it's also a single localhost adapter, and it's also that single set of ports. So, again, looking back at this example, right, there's no way we can set these two containers up on the same port. Normal networking rules still apply. But it's more than just networking. All containers in a pod share the same cgroup limits, they've got access to the same volumes, the same network, and even the same IPC namespaces. Now, how this all works behind the scenes with pods or infra-containers and the likes, it doesn't massively matter to us, and you know what, it's constantly changing as things like PID namespaces, and the likes, come into play. For us, right, just think of the pod as holding all the namespaces, and any containers they run just joining and sharing those namespaces. Now then, inter-pod communication is simple. That's because all pod addresses are fully routable on the pod network. Now, if you watched the installation module, which you should have done, you'll have seen me create the pod network at the end of the manual install section. Well, every pod gets its own IP that's routable on that network. This means, every pod can talk directly to every other pod, and there's no need to mess about with nasty port mappings or anything. Well, that's inter-pod communication, what about intra-pod? Well, inside the pod, like if you've got multiple containers in there, well they all talk over the shared localhost interface. And if you need to make multiple containers available outside of the pod, then you do that by exposing them on ports. And obviously, like if you've got two containers inside a pod, they both cannot use the same port, no. Like we said, those normal networking rules, they still apply here. And I'm hoping you get the picture, right. It is about the pod. The pod gets deployed, the pod gets the IP, the pod is what it's about. Now then, when we deploy pods it's an all-or-nothing job, right, there's no such thing as a half-deployed pod. It's either all the containers in the pod come up and then the pod itself comes up, or they don't, and the pod fails. You're never going to end up in this world where maybe one of the containers in a pod started and the other is in a failed state, but the pod's partially up. That's not how it works. And I suppose it's worth saying, right, that a pod gets deployed to just a single node, and this is just like VMs or containers, right. You can never have half of a pod on one node and half on another. One pod gets scheduled to one node. Now, the lifecycle of a typical pod goes something like this. You define it in a manifest file, YAML or JSON, take your pick, right. Then you throw that manifest at the apiserver and it gets scheduled to a node. Once it's scheduled to a node, it goes into the pending state while the node downloads images and fires up the containers. And this is important, right, it stays in this pending state until all containers are up and ready. Once that's done, it goes into the running state. Then once it's done and dusted with everything it was created to do, it gets shut down and the state changes to succeeded. Now, if it can't start, for whatever reason, it can remain in the pending state, or maybe eventually go to the failed state, which hopefully won't happen too much. And that's it for pod states, really. But, and a big but here, no pun intended, but you've got to think of pods as mortal. When they die, they die, there's no bringing them back from the dead. It's the whole pets versus cattle thing, right, and pods are totally cattle. So they die, you replace them. There's no tears, no funeral, no mound in the ground. The old one is gone, and a shiny new one with exactly the same config, but a different ID and all, magically appears and takes its place. So they're cattle, totally replaceable, that is the model. And I think we're probably done with the theory for now. So the highlights to take away are definitely, 1) that pods are the smallest unit of scheduling in Kubernetes. 2) you can have more than one container in a pod. Okay, one is probably the most common, and having more than one is a bit more of an advanced topic, but it's ideal for containers that need to be tightly coupled, so I don't know, sharing memory or volumes or something. 3) pods get scheduled on nodes. You can never end up with a pod spanning multiple nodes. And 4) they get defined declaratively in a manifest file. Well, we throw that manifest file at the apiserver, and the scheduler assigns it to a node. Cool, let's go try it out.

Deploying Your First Pod
Now I'm going with this cluster here, three nodes. This one here is the master, and we can see this particular cluster is in AWS. Well, you know what, the AWS part doesn't matter. Kubernetes is Kubernetes, so you can be following along on Minikube or GKE or whatever you want, right. Now, if you've not got kubectl, you need to go back to the lesson on installing and get it. It's basically the Kubernetes client binary, and it's vital. So, as we start getting up close and personal with Kubernetes, it is literally going to be your best friend. Anyway, following the Kubernetes mantra of composeable infrastructure, we define pods in manifest files. And I guess, like we've said enough times already, we feed those into the apiserver and the scheduler instantiates them on the cluster. Wow, here's the pod manifest file we're going to use. This one is in YAML format. You can go with JSON, though, if that's your thing. And I know, alright, this one is super simple, but that's on purpose, right. I want to give you a nice gentle intro. So right off the bat, there's four top-level resources you're going to need to know. First up, we're setting the version of the API to use, v1. It's stable, it's been around since 2015, and it's fine for working with pods. Now, if you're working with some of the newer constructs, and we'll see this later in the course when we work with deployments, but some of the newer stuff is only supported with newer versions of the API and schema. Well, next up, kind. This tells Kubernetes what kind of object to deploy, obviously this one's a pod. But, again, as we crack on with the course we'll populate this with different values, Replication Controllers, services deployments, you name it. But, it's good, right. You see, instead of having a different kubectl create command for every type of object, we've just got one. Then the way that Kubernetes knows what type of object we want to deploy is right here in the manifest. Well, then we define some metadata. We're keeping it simple here, right, and we're just naming this one hello-pod, but you can pretty much put anything you want here. In fact, you know what, I'll tell you what, let's say that this one is in the prod zone, and we'll call it version v1. Alright, so as you can see, these are just key value pairs, but, they are powerful, and we'll see them a bit later. Then last, but not least we've got the spec section. This is where we define what's in the resource. So we're defining a pod, yeah, but inside of it we're saying we'll have a single container, just not any old container, we're going to call it hello-ctr, and we're basing it off of this image, and we're exposing it on this port. Now, if it was going to be a multi-container pod, we'd just define more containers below here. Well I reckon that's it, apiVersion, it's a pod, we'll call it hello-pod and give it these extra labels that we made, and we'll run this container here, fabulous. Well, to deploy it, we go kubectl create, tell it to deploy from a file, and our file is in this directory called pod. yml. Alright, pod created, that was quick, but don't get carried away, that's just the definition on the cluster created, it might not actually be deployed yet. Well, that's alright, we can check it with kubectl get pods. And right enough, 0 of 1 pods is ready. And we can see it's currently in the ContainerCreating state, hmm. Now, hang on a second. We just did about pod lifecycles, and I do not remember anything about ContainerCreating being a valid state, so what's going on? I hope this is not some cheap course with bogus info. Well, I tell you what, let's have a look at it with this. Okay, so what have we got here? Well, we've got pod name, it's in the default namespace, it's on this node, and blah, blah, blah, ahh, Status Pending. Now Pending was definitely a legit status, so where was ContainerCreating coming from. Well, this stuff up her at the top is the pod stuff, but then further down here is the container stuff. And look, here we've got container State Waiting, Reason ContainerCreating. So, long story short, the pod status we were getting was actually the container state, not the pod state. And I'm not sure I would call that an outright bug, but yeah, it's definitely a bit weird that a get pod shows us the container state rather than the pod state. But never mind, look, down here we can see that it's pulling the image, and to be honest, that's probably done now. Yeah, so, congrats, that's your first pod up and running. But, I mean, how can we see it, like is it a web server, what is it? I don't even know. Well, we'll get to that in the next module. Before we get to it, though, we're going to play around here a bit more.

Deploying Pods via Replication Controllers
Now kubectl get pods is going to show all pods in the default namespace, and that's alright for us because we've only got one. But if you're in the real world with a boatload of them, you're going to want to filter them in some way. Now, probably the easiest way to do that is to lash the name of the pod onto the end like this, right. And if you want to see all pods in all namespaces, so like system pods and the likes, we'll just whack this onto the end. Alright, now then, bombshell time. We don't ever work directly with pods like the way I just showed. Well, what the.. Don't worry, it was good that I showed you what I did, right? I mean it covered some of the fundamentals, and it was a really easy intro into the YAML format. But, the reason we don't work directly on pods is because there are higher lever objects that expand on them and make them better. A really common one is the Replication Controller object. Now this one, right, at a high level, is a bit like a wrapper around a pod, with the sole purpose, okay, of adding desired state to the game. Okay, let's have a quick example. Assume you wanted, whatever, five replicas of a pod, and you wanted them to always be running. Alright, you'd deploy a Replication Controller that specified the pod and defined the desired state of having five replicas of that pod. Well, obviously, you throw it at the apiserver and Kubernetes would deploy it, deploy their application controller, yeah. Now, because you define the pod within the Replication Controller, part of deploying that Replication Controller actually deploys the pod, well five of them actually. Then, and here's the good stuff, Kubernetes then runs a continuous background loop that's constantly checking to make sure there's always five replicas of the pod running. And, of course, it's auto-magic and it's in the background, right. You deploy it, the Replication Controller, and then you crack on with something else. Kubernetes picks up the hard work, runs a watch loop in the background, and makes sure that the actual state of the cluster always matches your desired state. Right, now, before we do anything, though, let's get rid of our pod with a kubectl delete, and we called it hello-pod. Alright, now then, so here we've got a simple Replication Controller config. Again, we see the same four top-level constructs, version 1 of the API, this time we're defining a ReplicationController and we're calling it hello-rc, and then the spec. For now, right, we care that it's asking for 10 replicas, and we're saying select on pods with this selector label here. So this section up here is kind of the ReplicationController config. Then, though, in the template section down here we're saying use this pod template. And importantly, right, we're giving it the same app equals hello-world that we told the controller to select on up here, and then we've nested pretty much the same pod template that we used a minute ago. Now, look, I know that this is new, right, to some of you. So maybe take a second to pause the video or watch it through and maybe rewind it and watch it again to help it sink in, but I think, right, the takeaway point is this. It's a ReplicationController, we're declaring a desired state of let's have 10 of them. And when we say them, right, we mean pods, and configured like this down here. So, API, kind, metadata, and spec. Those are our four top-level objects, and we saw them a minute ago, again, in the pod manifest. Only, in the ReplicationController spec here we've embedded the pod spec as well. Magic. Well, a great thing about Kubernetes is that we deploy everything with pretty much the same command. So we go kubectl create again, tell it a file, and this time we'll have the ReplicationController file. And off that goes. Now the image is downloaded, well, at least on one node, so let's take a look at how they are. Okay, cool, let's describe them as well, brilliant. Now if we want to update the config, so change the desired state, I don't know, to let's say, 20 replicas instead of 10. And you know what, we could be changing something else, right, like the version of the image or whatever. Well, either way, you just edit that config file, we said 20 this time, and you can change anything here in the pod template area, right. But let's save that. Then kubectl apply, and we'll give it that same file. Right, great, now don't worry about this message, it's a bit below here that we're interested in, cool. Alright, so let's check the status, and that's done, right, 20 replicas. In fact, if we do this and run this against pods, right, if we look down here we can see the age difference as well. Some of them are newer than others, so 10 old, and 10 newer. Well, you know what, I reckon that's the basics of pods and Replication Controllers. Now in the next module, we're going to see how we can use another Kubernetes resource, the service object, okay. And this will let us easily connect to our pods. But first up, a quick recap of what we've learned in this module.

Module Summary
Now I'm going to keep this quick, because I'm keen to crack on. So the main things we learned here. We said that the pod is the smallest deployable object in the Kubernetes API. I think we said it's equivalent to the VM in the virtualization world, or a container in the Docker world. We also said that deploying a pod is an atomic transaction, so an all-or-nothing deploy. Either everything in the pod goes and the pod is considered up and deployed, otherwise, it's not, even if only a tiny part of it didn't go. We reminded ourselves that we define pods and other Kubernetes objects in YAML or JSON files, and then we feed those to the apiserver. We use the kubectl command to do that, but you can obviously talk to the API in other ways. Then, we dropped the bombshell. I had just shown you how to define and deploy a pod, but then I said, yeah, we never actually do that. Instead, we always use higher-level constructs like ReplicationControllers. So then we switched gears and looked at them. And we said, right, that a Replication Controller is a great example of Kubernetes implementing the desired state model. So, in the Replication Controller you define a desired state. For us that was 10 replicas of a particular pod. Then, once we deploy that, Kubernetes works its backside off to make sure that the actual state of the cluster always matches that desired state. But you know what, even though Replication Controllers are the business for horizontally scaling pods, even if you're deploying a single lonely pod that you never plan to scale, you should still deploy it via a higher-level construct like a Replication Controller. And I think that resolves. And you know what, as cool as that's been, we've not actually seen what our pod is even doing yet. Well, that is coming up next with Services.

Kubernetes Services
Module Intro
In this module, we're going to take a look at the Kubernetes Service object, the way in Kubernetes to expose your app to the outside world, plus a bit more. And we're going to go like this. We'll cover off the theory of what a Kubernetes Service does and how they work. Then we'll slap one onto the app that we've deployed, but we'll do it the iterative way. Then we'll do pretty much the same again, just this time the declarative way. And it will be useful to see it done both ways, right. Then, we'll mention a real-world example or two, and we'll wrap the module up with a quick summary. So, let's go wrap our heads around the theory.

The Theory
So we've got an app running, or have we? I mean it's not like we've even seen it, not really, certainly not in action. In fact, the only evidence of the app so far are a few kubectl commands that tell us we've got some pods running. But let's show some trust here and assume our app is up and working, and it's replicated, and all thatgoodness, so that if we lose a node or whatever, we'll always have 20 instances, or however many we configured the Replication Controller for, I think it was 20. But the big question should definitely be this. How do we access it, and even see what it is? Well, on that point, Services. And I want us to think about two typical access scenarios, right, accessing from outside the cluster, like a web browser on your lappie or something, and accessing it from inside the cluster, like another set of pods wanting to talk to it. Well, guess what, yep, Services nail both of these. For starters, right, they give pods, well, you know what actually, let's back up for a second. So a Service in Kubernetes speak is a REST object, just like pods and Replication Controllers. And we'll see it in a minute, but we define them in a YAML file and throw that at the apiserver. But for us right now, we care that a Service is an abstraction. And we're going big picture here, right, but let's take a bunch of pods with our app, and they're deployed through a Replication Controller, and they are happy. But we're not. I mean, right now we have no solid way of connecting to them, because remember, pods are ephemeral, here today, gone tomorrow. So it is a big no-no to look up an individual pod IP and connect to that. Because what if we're connecting to maybe this one here, and then suddenly, poof, gone. Uh-oh, well, slap a Service in front of them like this, and boom, that is your reliable endpoint right there. So the Service here gets a single IP and DNS and port as well, right, and these never change. Let me repeat that. They never change. So this all down here can chop and change as much as it wants. Some of them can die, we can scale them up, scale them down, pretty much anything we want, but the Service in front of it, you guessed it, never changes. Just throw your requests at this, and no matter what kind of chaos and complexity is going on down here, it is all hidden nicely behind the tidy Service. And, of course, requests to the Service get load balanced across all the parts at the bottom here. But bringing it back to the two access scenarios we mentioned, access from inside the cluster and then access from outside, from inside the cluster, the other parts of our app can connect to this stable IP or DNS. And by default, right, the DNS name of the Service is just the same as the Service name. But to help us from the outside, a Service sets up a bunch of networking magic to make the pods here all accessible from outside the cluster. Speaking of which, let's dive a bit deeper. So, we've got our pods, and we know that they are unreliable. Then we layer a Service on them, and this is reliable. It gets a virtual IP that stays with it for its entire life, and it gets a DNS name and a port, yeah. Now then, that port is cluster-wide, and it's how we connect in from the outside. So we've got our nodes in the cluster, and they're running our pods. We create the service object, and that links the pods to a particular port number on every node in that cluster. And the port number is the same on every node. This means, okay, we can hit any of the nodes in the cluster on that port, and get through to our pod, our app. And there's other options as well, like integrating with your cloud provider's load balancers. But that starts to get a bit platform-specific, and we're not going there. Now then, when you create a Service, and maybe this is even going a bit too deep for this course, but let's go there anyway, right. Anytime you create a Service, you're also creating an endpoint object for it. And it's the endpoint that holds, well, all of the endpoints, so a list of all of the pods in the Service. So as pods come and go, the list of them that the endpoint maintains gets updated auto-magic. And it's great for native Kubernetes apps that know about this and how to use it. But if you don't happen to be a native app, well that's alright as well. You can hit the Service's virtual IP, and that balances over the backend pods as well. It looks lovely right, but how do we tie these pods together with the Service? Easy, labels. I absolutely love labels. So you remember the labels that we stuck against our pods before? Well, we just told the Service object to select on them, that's it. The labels here are the glue that sticks the Service to the pods, only you know what, glue is a crappy analogy, because this is actually uber flexible. So what I'm saying is, right, you can update the Service and add or remove labels here, and that's going to dynamically update which pods the Service balances over. It's totally dynamic. The Service's label selector is continuously on the watch, like 24-hour surveillance, so any changes to the Service get immediately reflected to the corresponding endpoint object. And then there's Service Discovery, okay. This is implemented in a couple of ways. First up, and best, I reckon, is DNS. As long as you're running the DNS cluster add-on, which if you follow the install methods that we've shown you, you will have, but the DNS cluster add-on implements a DNS Service in the cluster. It's all pod-based, obviously, but it implements a cluster-based DNS Service that configures the kubelet, that's on nodes, yeah, to go to it for DNS. Then it watches the apiserver for new services, and it registers them with that DNS. Long story short, Services get DNS names that are resolvable inside the cluster. Lovely, what is not to like about it? The other way, though, is through environment variables. Every pod, okay, gets a bunch of environment variables in case you're not using DNS, but this is done when the pod is created. So if you create your pods before you create the Service, then it's not going to get the variables. I mean, how can you throw variables about the Service into a pod, if you haven't created the Service yet? So I reckon you're better off with DNS. Anyway, look, this is supposed to be a getting started course, so I'm calling a halt to the theory for now. All we really need to know is that Services give us a stable networking endpoint for our pods, and they load balance across the pods as well. Plus, they let us access our pods from outside of the cluster. Alright, magic. Let's go see them in action.

Creating a Service the Iterative Way
Okay, so we've deployed an app, apparently, and in a pod, apparently, or a set of pods, and we did it via a Replication Controller. But we've already established, yeah, that there's an element of trust going on here, because as far as I'm aware, we haven't actually seen our app yet. So let's fix that. Well, you know what, first of all, let's show that we can't access it, at least not from outside the cluster. So these here are the public IPs of all of the nodes I've got in the cluster. So any of these here will do. And the only port that I've seen associated with the app so far is 8080, so that'd probably be my best guess as to which port to hit it on, and nothing, no joy. Okay, well, the simplest way to wrap it in a Service is to go kubectl expose. This is the iterative way to create a Service object. We're exposing our Replication Controller. We'll call this hello-svc, because we're creating a Service. The target port is going to be 8080, and we'll expose it via something called a NodePort. Okay, that's done. Well, we can describe it like this. Okay, see how it's type NodePort. This here is the Service's virtual IP, and this one here is the cluster-wide port that we can access it on, so I'll grab that actually, and we'll talk about endpoints and other stuff later. Well, if we go back to that web page again, let's change this 8080 here to the value we just copied out of the Service, and we're in business. That's our app. Hmm, a bit of a let-down probably, a crappy web page. And if you've done any of my Docker courses, you'll have seen this like a million times already. But it's okay, it's a containerized web server listening on 8080, and we have mapped it to this port here on every node in the cluster. That means we can hit any node in the cluster on this port and get to our app. And by default, right, Service ports are going to be between 30000 and 32767. But, yeah, that's our Service. Super simple, and super powerful. But the way we've shown you here with the kubectl expose command, that's just not the Kubernetes way. Obviously it works and it's fine, but Kubernetes is all about doing things declaratively. So, coming up next, we're going to see how to do all of this again, but with a YAML file, and we'll dive a bit deeper as well. Alright, let's do this properly. Well, we'll clean up first, though, by getting rid of that service.

Creating a Service the Declarative Way
What did we call it? Oh, yeah, of course we did. Anyway, please go away. Alright, as always, we've got a service manifest file here, right. Now this is known territory for us these days. So first up Services have been around since forever, so they're in the version 1 API and schema. Well, we're telling it this one is a Service, and we're giving it a name and a label. Then we're actually defining it. So we tell it type NodePort, and we mentioned that already a minute ago, but there's at least three major options that we could go with here, and I think it's worth mentioning them, right. Well, first up, okay, we call these service types. And the first one is ClusterIP, and it's the default. This one gives the service a stable IP within the cluster, so it only makes the service available to other nodes in the cluster. Then there's NodePort that we're going with. This one takes that ClusterIP, and it adds a cluster-wide TCP or UDP port on top of it. It's what we saw a minute ago where it assigned a random port above 30000, and tied the service to that port on every node in the cluster. Think of it as a basic way of exposing the Service to the outside. We saw it in our web browser, yeah, which in case I didn't mention, actually, my web browser is totally outside of this cluster. It's on my laptop in the Home lab. The cluster is in Amazon Web Services. Okay, well, then there's a load balancer. Well, this one takes the NodePort and the ClusterIP, and then it adds another layer on top, this time a cloud-native load balancer. Now there's other ingress modes, and what have you, that also wrap around NodePorts in the same way, and that's really what NodePorts are for, feeding into these high-level constructs. But, we can definitely use them in this raw format to access our app. And, you know what, because this is just a getting started course, we're not going to get into the specifics of cloud A's load balancer versus cloud B's or anything like that. But, conceptually, right, it is a bit like those Russian dolls. At the core or the very center is ClusterIP. That makes the pods available on the stable IP, but only from within the cluster. Then you can add a NodePort and wrap that around the ClusterIP so that our pods are available from outside the cluster. After that, we can add or wrap yet another layer to the outside to integrate it with your favorite cloud provider's load balancer, though, your mileage may vary there, and not all cloud load balancers are necessarily created equal. Anyway, you know what, we're going with vanilla NodePort. Now this port bit here is what our app is exposed on in its container, so the container that's running inside of all of the pods that we're going to have, exposes port 8080. This then gets mapped through to the NodePort, that port above 30000, on all nodes in the cluster. In fact, actually, we can do this here and specify the NodePort. Right, that works for me. Anyway, we're telling it TCP, which actually is the default, so we could have left it out. But you can tell it UDP here if you need to. And last, but definitely not least, we've got our selector. Now, this has to match our pods in our application controller, otherwise none of this is going to work. But I think we're good with that file. So before we deploy, let's just run this to make sure our selector label is going to match our pods. Yeah, that looks okay, yeah. So, just the usual again, we go kubectl create, tell it we've given it a file, and we give it that service file, and we're good. So, again we can throw our usual get and describe commands at it, uh-huh. And this is all looking good. I think for us, both outputs we can see the NodePort value again at 30001. Well, if we go grab that browser tab again, and you know what, let's hit Refresh again here to show that the old one from before is not working anymore. We deleted it, remember, so we could do this declarative one. Okay, and this time we went with 30001. And we're back in business. So same result as last time, right, only this time we did things the proper Kubernetes declarative way. Now, then, I've got a quick note to self here saying, mention the endpoint object. So let's just rewind a bit, right. Remember when we said, when we create a Service, that also creates an endpoint object that holds all of the pods that the Service matches. Then we said, this gets updated on the fly whenever pods come and go. Well, we can see it, as well, with the normal kubectl commands. Remember, it's the same name as the Service. Bingo, these are all of the pod IPs that match the Service's selector label. And you know what, on that note, I reckon we're done. But we're not quite ready for a review yet. Before we do that, I want to just look at one of the many ways that services can be really amazing in the real world.

In the Real World
Now, everything that we've seen is cool, yeah, but I'm a technophile. I love technology just because it's technology. Like, when I watched Star Trek as a kid, my favorite episodes were always the ones that just had the most technology in them. I was never really bothered about story lines; it was just all about the tech. But, sadly we don't live in a world where we can sit and play around with cool technologies all day. They've got to bring some value to the business, I don't know, keep it running, make it agile, make it resilient, there's a long list. Well, I want to take a minute to run through a really common real-world example, making updates to apps. We all know that it's a fact of life that we've got to roll out updates to applications. Bug fixes is a major reason, but new features and new business functionality is another. Well, let's look at this, and we're going to see the power of labels here. The sweet, sweet power of labels. Anyway, look, this is our app, right. It's any business app, and I know it's super simplified. But, it's deployed on a Kubernetes cluster, it has a bunch of pods managed by a Replication Controller. In front of it, we've got our trusty Service. Now that's selecting on pods with labels that say app=biz1 and zone=prod. And see how the pods here have got the same, though, they've got a third as well, showing the version. Now it doesn't matter that the Service is only selecting on two of these three. It'll still grab them, and it will still load balance over them. However, if the Service itself was also selecting on another label, and the pods did not have it, I don't know, let's say this example here, then it would not work. The pods wouldn't match the query. But the Service matching on two of the labels that the pods have, that's totally fine. Now, let's say we need to push a new version of the app, but obviously we want to do it as smoothly as possible without interrupting access, some kind of zero-downtime update. Okay, so the first thing we do, I guess, is introduce the new version here. And if we look at it, it's the same as the old one, well, except that it's got the updated code, yeah. But, it's pod-based, and it's deployed by a Replication Controller. And look at its labels. They also match the Service. So, because the label selector on the Service here is constantly being watched, the control plane sees the additional matching pods, and it updates the endpoints and all of the load balancing. Now, as if by magic, their load balancing across the lot. And that's great, right, but some kinds are hitting the old version, yeah. Well, after a given amount of time, and that's arbitrary, right, you decide, but as soon as you're comfortable that the new version here is working as expected, pulling the old one out is as simple as updating the label query on the service here to match this new additional tag. And look at that, suddenly the older versions don't match anymore, everyone is getting the new version. But, and this is vital, right, the old version is still there, we're just not using it anymore. So, if for whatever reason we suddenly have an issue with the new version and we need to switch back, and I'm teaching you how to suck eggs here, I'm sure, but we just flip the label selector on the Service here, and we're back to the old version. I freaking love it, and it can be used for all kinds of things, blue-greens, canaries, you name it, right. This is so simple, but so powerful. Like I say, I love it. Now let's go quickly and review what we've learned in this module.

Summary
So what have we learned? Services are the business. Yeah, but a bit more than that. I think we've learned that they provide us a reliable networking endpoint for our own stable pods. We know that pods come and go, right, and we shouldn't rely on their IPs. Well Services, they don't come and go. They get an IP, a DNS, and a port, and they never change. But, they let all the stuff like the pods, and what have you, behind them, change as much as they want, and they protect us and our apps from all of that constant churn of pods. But, they also expose our pods to the outside world. We looked at a basic, and a pretty crude way of exposing a set of pods through a NodePort Service. That was where the Service was configured on a single port across every node in the cluster, and then mapped that through to our pods. And that was great, but I think we said that NodePorts are really for feeding into high-level objects like load balancers. So, you can create the Service type of load balancer, and if you're on AWS or Google Cloud or whatever, you can have your service hook in with a cloud-native load balancer. But we didn't look at that, because we're a getting started course, and I think what we've provided you here is enough to get you started. And to be honest, right, it does save me picking a cloud provider and then having you guys all complain at me that I picked the wrong one. But take that the way I intended, right. I really do appreciate you taking my courses, and I'm all about making them as useful as possible for you. Really, I am. But, that's me done for now on Service. Next up, our last module, and it's the good stuff, Kubernetes Deployments.

Kubernetes Deployments
Module Intro
Okay, last module, and we're going to be building on the things that we've learned about Pods and Replication Controllers and Services. So, if you have not been following along, or I don't know, maybe you don't already know these, then really, you're going to need to go and learn about them and then come back. Okay, this is the plan. We'll nail the theory, then we'll create a brand-new deployment, then we'll update it, I think like a rolling update and maybe a rollback or two, and then we'll celebrate. So let's go start with the theory.

Deployment Theory
Kubernetes Deployments. I love them. Now right here at the top, so up front square, and properly central, right, we need to know that Kubernetes deployments are all about rolling updates and seamless rollbacks. It is the whole reason they exist, rolling updates and simple rollbacks. In fact, I'm going to park it up here, and we're going to come back to it later, but you need to know that. And, of course, right, like pods and everything else we've seen, they are full-on objects in the Kubernetes API. Now, this is getting a bit boring now, but we create them as a YAML or as a JSON file, and we give them to the apiserver to deploy. Okay, so remember that we said Replication Controllers kind of wrap around pods? So we've got our pods at the core running our apps inside of containers, and Replication Controllers, they take them to the next level. We're talking about things like scalability, resiliency, desired state, yeah. Hey, we want 50 of this pod, no matter what, and Kubernetes takes care of it, like in the background it runs all of the watch loops and stuff to make sure that the actual state of the cluster meets our desired state. Brilliant. Well, deployments take things even further, and they wrap around Replication Controllers. And like we've got parked up here, it's all about bringing this rolling update and rollback stuff. Only, okay, this Replication Controller here, well, in the world of deployments it's actually a replica set. Now, for all intents and purposes, they're the same thing, right. Well, I mean there's a few subtle differences, but we don't really interact with them directly like we did with Replication Controllers, just because we're working directly with the deployment instead. But, yeah, as far as we care, they're just a newer version of a Replication Controller. But be aware of the difference. I mean there's always going to be the tech trolls out there who inflate their own self-worth by pulling you up on stuff like that. I mean, I should know, I'm one of them. No, I'm just kidding, it doesn't bother me. Anyway, deployments manage replica sets, and replica sets manage pods. And like we're showing up here, again, but I want you to fully appreciate this, deployments are all about the updates and the rollbacks. Speaking of which, right, let's just step back for a second, and we'll see how we handled updates before deployments. So back in the day, you remember, when Oracle ruled the world and legacy Unix stuff like AIX, and what have you, were all the rage. Maybe not that long ago, but, before we had deployments, right, we'd deploy our pods through a Replication Controller, sweet. Well, then to update things we'd create a new Replication Controller, so with a new name, I don't know, maybe slap a version number on the end or something, but we were creating a new YAML file with a different name. Then we do kubectl rolling update, and we tell it to use this new file. And we can set a bunch of parameters to control the update, and all of that, right, but this is the old way so we're not going there. But that was it, right, Kubernetes would then take care of the update. And it was, I mean it was okay, it worked, but to be honest, it was a bit Frankenstein, a bit bolt-on. I mean, it was all handled on the client side instead of the control plane, for one. Rollbacks, they were a bit basic, and you know what, there wasn't any proper audit trailing or anything. So, I mean, yeah, it definitely worked, it was just a bit of a kludge, definitely not as good as what we're going to see with a deployment. So, with a deployment we create a YAML file, and yet, you know what, we throw it at the apiserver. That gets given to the deployment controller, and your app gets deployed on the cluster. Now, behind the scenes, you get a replica set and a bunch of pods. The deployment creates all of that for us, right, automagic. And, of course, Kubernetes is running its background loops and making sure that whatever is on the cluster matches what we ask for, all the usual stuff. Then, you want to push an update, like a new container image or something, fine, you make changes to the same YAML file, and just push it to the apiserver, and we'll do it all in a minute, right, so bear with me on the high level. Anyway, in the background, Kubernetes creates another, so a new replica set, so we've got two now. And as it winds this new one up, it winds the old one down, I don't know, maybe one pod at a time, you can specify that. But what we get is a nice smooth rolling update. And you can keep on doing that with more and more updates, right. Just keep updating that YAML file, which in the real world, right, you're going to want to be managing in a source code repository. But, and this is important, in the background, all the old replica sets, they stick around, they don't get deleted. I mean they're not managing any pods anymore, they're wound down, but they're a great way for deployments to revert back to a previous version. So, for a rollback, we just do pretty much the opposite. We wind up one of the old replica sets, and wind down the current one, really simple. And there's more, right, there's intelligence built in. You can say things like, wait however many seconds after the pods are up before you mark it as healthy. And there's readiness probes, and all kinds of magic, right. All in all, a lot better and more feature rich than Replication Controllers. Well, with all that in mind, let's go and get our hands dirty with our first deployment.

Creating Your First Kubernetes Deployment
So we're going to create a brand-new Kubernetes deployment from a YAML file, and you can do the same iteratively, right, using the kubectl run command. But you know what, we are the real deal now that we're on to deployments, and being the real deal, I want to go declarative from the start. That said, though, by all means, have a play around with the kubectl run command in your own time. Anyway, look, before we start, let's get rid of the things that we've deployed already, so getting rid of this Replication Controller here, that's going to get short of all of our pods as well. We'll check that, yeah. Now, we've got a service object running as well, but you know what, I'm going to leave that up, because I'm a tough guy. Well, actually, I'm not, but we're going to be deploying this same app again, just this time through a deployment object. But the point, right, because it's the same app with the same port and labels and stuff, then the service that we've already got deployed is just going to keep working for that. Alright then, so we've got a YAML file again, and it's in the course notes if you want a copy, but look at this, something new at the top. Exciting. Well, deployments are a bit new, right, I mean, they're not, I mean, not like super new, but at the time of recording, we do need to specify this beta API extension. Like if you go v1, it's just not going to work. Now, as well, as in when the v2 API comes out, feel free to use that, deployments will be in there as well. Anyway, look, then we're telling it that this is a deployment, so send this to the deployment controller please, and we're giving it a name. Then, for now, right, the rest just looks like a Replication Controller, we're saying let's have ten replicas of this pod down here. So if we come out, and if we do the usual create operation, though I want to say here, right, technically we can say kubectl create deployment here, and then emit the kind equals deployment in the manifest file, either way works, I just think defining stuff like this in the YAML file is way more declarative. Anyway, look, that's done. Let's have a quick look at it. Alright, the usual detailed description, which I don't actually want to look at just yet. I mean, feel free to pause the video and take a look yourself, but we're going to cover a bunch of this stuff in a minute, so I'm saving my lecturing until then, though I do want to show you the replica sets. Okay, we've just gone one right now. That's because we've just done the initial create or the initial rollout of the deployment. But, we can see that it gets the same name of the deployment, plus some hash here of the deployment manifest, or at least maybe of the pod template bit. And we can get more detail with the usual describe command. Right, now in order to access it we need the customary service object, but remember, we kept ours running. Now, if you're following along or not quite following along exactly, so let's say you didn't keep the service object running or maybe you're doing a clean deployment or something and you want to access it from outside of your cluster, you're going to need to deploy your service object in front of it. But ours are still running, remember, so if we hit a web browser here, we're going to want a node IP from here, and it was port 30001, Yo Pluralsighters!!! And at this point we'd like to take a break and give a special thanks from my family, my kids can eat thanks to you taking this course, so you've done some good today just by watching this, well done. Anyway, look, let's take this to the next level with some updates and rollbacks.

Updating a Deployment
So we've got our app deployed via a deployment, capital D Deployment, and we can access it through a node port service. I'll tell you what, we're getting good at this. But the business has come to us with a change, and we've got to update the app. Okay, so I'll tell you what, let's assume we've updated the code and we've built and tagged a new Docker image, that's already done. So the next thing to do is push that new image into production. Now, clearly I'm side-stepping a bunch of real-world stuff like CI/CD workflows, and the likes. The image already exists on Docker hub, and we are ready to modify our deployment YAML. So, the first thing we want to do is come down here and update the image tag. Now don't pay attention to how I'm tagging this. I've just made a quick update that I've tagged as edge. So dead simple here, yeah, that's going to change the image that the deployment will use. And I want to be clear about this image, right. Please, please, please, do not think that it is in any way a safe image, it's not. I built it a while ago from my Docker DevOps course, I think, integrating Docker with DevOps automated workflows. Anyway, look, I haven't touched it since, so it is 100% guaranteed to be bull of vulnerabilities and other nasties. Please, don't leave it running for any length of time if you're using it to follow along. Anyway, that's going to change the image that our app uses, but we're going to make a few more changes while we're in here. So we're going to come up here, and right under replicas we're going to add this minReadySeconds map, and I'm going to say 10. Now what this is doing is it's saying, wait for 10 seconds after each new pod comes up before you mark it as ready and move on to the next one. Alright, well then I'm going to be specific about how I want the update to happen, so I'm going to nest this strategy map in here as well. Now some of the values that we're setting here are the defaults anyway, right, but I wanted you to see how they look and how you can change them. So we are telling it, right, we want to do a RollingUpdate strategy. Others do exist, and no doubt, more will be added as things mature, but then we're saying let's do the update by taking one pod down at a time and never having more than one extra pod. Now, by extra pod, I mean we're asking for 10 up here, that's our desired state, and then while we're rolling through the update, never surge to more than 1 above 10, so never more than 11 while we're doing the update. And I think we should be good. So we go kubectl apply, we give it the same file, and remember, you'll be using some form of version control tool in the real world, hopefully, but we're going to slap this record here onto the end, and I'll tell you about that in a second, but away it goes. Now then, it's going to take a bit of time to complete. Remember, it's going to iterate through our pods one at a time, but it's also going to wait 10 seconds after each one. Remember we told it minReadySeconds 10? Well, as well as that, if it's not downloaded the images before, it's going to take a few seconds to pull the image down on each node. So I think while it's running we can go kubectl rollout status deployment, and then the name of our deployment. And there we are, partway through, in fact, nearly finished. So if we Ctrl+C that and then run a get deployment, it might even be done. Right, DESIRED 10 and 10 UP-TO-DATE. And you know what, I'm not sure about this 9 AVAILABLE, right, I think it might be a bug. I've seen it before, and I know that if you dig into the replica sets and what have you, everything is up, and nothing is in the failed stat, so I'm going to just ignore that for now. Well, if we look at the deployment history with kubectl rollout history deployment, and then our deployment, okay, two revisions or versions. Version 1 was our initial deploy, and then version 2 here is the change that we just made. And check out this lovely CHANGE-CAUSE right here. That's the command we run to invoke it. And it's in there because we slapped the record flag onto the command. If you don't use record, then it's going to look like version 1 up here with no CHANGE-CAUSE. So, I highly, highly, highly recommend that you use record with your deployments. It's great for an audit history. Anyway, we've done our rolling update, so based on what we said earlier about replica sets, I reckon we should have two of them by now. Right, the one with no pods under management at the top, that's from our initial deploy. Remember, it got scaled down when we did the update. And the one at the bottom that's got 10, that got scaled up as part of the update. Well, this makes it dead easy to rollback, though, actually, you know what, we've not even checked the update yet. What am I doing? Let's describe the current state of the deployment. Okay, where is it. Right, there's the updated image, the edge tag. And if we hit our browser again and refresh that, okay, Yo Kubersighters this time, I should trademark that. But that's our update right. It used to say Yo Pluralsighters, now that's Yo Kubersighters. Anyway, back to the simple rollback I was mentioning. Because we've got two versions, and effectively two replica sets, right, we can just go kubectl rollout again. This time it's undo, it's a deployment still, we tell it which deployment, and I'm going to say ---to-revision=1 here. That's going to take us back to revision 1 that we saw a second ago. Quick check for typos, and away that goes. And it's going to do the same thing that the update did, right, so it'll go through each pod one at a time, and wait 10 seconds after each. So another cheeky get deploy, we can mess the name up because it's our only deployment. Okay, it's done two already, and we can say that it's burst up to 11, one above the desired 10, yeah. And we can watch it with a rollout status as well. Alright, that'll just tick through until it's done. So you know what, I'm going to annoy the laws of physics here, and I'm going to speed time up for you. Okay, we're done, and you know what, it's been fun. But I reckon we're coming to the end. But don't go yet, right. I want you to stick around for a couple of minutes while I quickly recap what we've learned here about deployments. But then, after that we've got one more super short module where I give you a few ideas of where to go next in your mastery of Kubernetes, so stick with me.

Module Summary
Well, we've learned a ton, and I reckon this last module was the best. So right at the top we said that deployments are where it's at these days in Kubernetes. They're the latest and greatest, and you'll get a ton of use out of them. We also said, they're all about the rolling updates and the simple rollbacks. So they wrap around replica sets, which are just NexGen Replication Controllers, yeah, and they add that powerful update and rollback goodness. And like everything else, they are objects in a Kubernetes API and we should be looking to look at them declaratively. Yeah, the kubectl run command creates deployments, but the right way, and the grown-up way, is to do things with a YAML or a JSON file. That way you define your desired state and give your file to the apiserver. Then you just sit back and let Kubernetes do the hard work. What's not to like about that? Also, though, we said you create your initial deployments with kubectl create and give it your manifest. Then you want to go check that manifest into a source control tool. After that, when you need to make changes, make them to the same config file, version check it, and deploy it with kubectl apply. But we said to use the ---record flag. That way we get a nice little audit history of what's happened to the deployment. Now behind the scenes, or in the background, when you're updating your deployments, new replica sets get spun up for updates. And the old ones, they get spun down, but they don't get deleted. Instead, they stick around and make it really easy for us to rollback with the kubectl rollout undo command. And there's more, I mean, there's loads more, but that's all we've got time for. Stick around for 2 more minutes, though, while I give you some quick pointers about where you might want to go next. Cheers.

What Next?
What Next?
Okay, you've made it, well done. And as always, a massive thanks from me. In fact, I'm getting all emotional. I just want to tell you how much I love you, because oh. No, just kidding. Seriously, though, from my side of things, I do put an insane amount of effort into these courses, literally months per course, and it's all, right, so that you guys can get as much out of them as possible. But honestly, if you had any idea how hard I work, and you know, how much I hate PowerPoint, man. But, yeah, really, a massive thanks from me. And I really hope it's been time well spent for you. If it has, shout me out on Twitter. I'd love that. Even if you don't like it, you know what, tell me why. But, it's been a getting started course. We have literally just scratched the surface. And even with what you've learned here, it could still be daunting to look at the docs or the API and think, man, where do I go next. So, from a mastering Kubernetes perspective, I think I've hammered home that you want to be doing things declaratively, like doing stuff in manifests. I mean, you can totally go the iterative route, and that's fine, I mean it's pretty good actually for messing about and finding your way around, but in the real world, especially if you've got aspirations of running this stuff in production, you really want to go declarative. I mean, nice YAML config files and the likes, they're just way more readable than honking long command lines. And they're self-documenting, and they lend themselves to config management repos and stuff, so it's just the proper, grown-up way to do things these days. Now, on that, actually, you're going to need a bit of YAML or JSON. I'm a YAML guy myself, and I reckon learning a bit of YAML will do you really well. But all you really need to know about are maps and lists, so if this stuff is totally new to you, get yourself on Google and learn YAML maps and lists. You'll grok them in no time. On the infrastructure or deployment side of things, I definitely recommend locking into high availability of the control plane bits. It's kind of early-ish days on this front, so things are changing, but for real-world production use cases, you are going to want to do as much H/A as possible. So, as an example, the clustor store, that's the only stateful component of the Kubernetes cluster, so it makes massive sense to back this up and deploy it in an HA config. Now right now it's based on etcd, and you can totally deploy etcd H/A. I mean you can even deploy it and manage it outside of Kubernetes, that's totally doable, and you know what, it might be a requirement for your production. Though, I will say this, right, if you're going to do that, get your testing done first, because while there's currently no reasons why it can't be done, you should go in with your eyes wide open. I mean, I don't know, probably 99. 99 whatever percent of real world testing and the likes, is all done with etcd running on the same nodes as the rest of the control plane stuff. So be aware, right, you are choosing the less-travelled path if you choose something else. Your mileage may vary. On the Kubernetes feature side of things, honestly, there's an absolute boatload more than just pods deployment and replica sets. Go and explore the docks and stuff. I definitely recommend looking at LoadBalancer Services and other ingress options. There's some interesting stuff going on in that sort of area. Finally, you might also want to take a look at at some of the stuff the folks at CoreOS and the wider ecosystem are doing. And I do mean things like CoreOS Tectonic, but I also mean the Core rkt project as well. The Kubernetes community are fans of that. And, of course, there's Docker. If you don't know Docker so well, I have no shortage of courses I can recommend there. Aside from that, though, and don't get me wrong we could talk about this stuff all day, but for now, I reckon that's good, just practice, practice, practice. So thanks again for watching. I'm around all the time on Twitter and the likes, and I'd love to hear from you. though, I mean I'm mega busy, so I'm sorry I can't offer free professional services via tweets, sorry about that. But definitely reach out and tell me what you liked, and maybe what you didn't, and maybe what I should do in the future as well. Ping me. And on that note, keep calm, and kubectl.