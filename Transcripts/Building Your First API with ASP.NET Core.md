# Building Your First API with ASP.NET Core

## Course Overview

NET Core. ASP. NET Core and Entity Framework Core signify the direction Microsoft is taking with their developer frameworks. Both have been built from the ground up. In this course, you'll learn how to build an API with ASP. NET Core that connects to a database via Entity Framework Core. We'll build an API for getting information and manipulating it concerning cities and their points of interest. We'll start out with the basics. An introduction to ASP. NET Core, including how such a project is structured. Then we'll implement all the functionality you'd expect from an API, getting, creating, updating, and deleting resources. You'll learn about features and concepts that are essential to ASP. NET Core. We'll also dive into Entity Framework Core, the O/RM of choice for. NET Core applications. After this course, you'll have a good understanding of how to build an API from scratch with ASP. NET Core. Let's dive in.

## Getting Acquainted with ASP.NET Core

## Introduction

Hi there, and welcome to the Getting Acquainted with ASP. NET Core module from the Building Your First API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. This course is a beginner course that'll teach you how to create an API with ASP. NET Core 1 or 2, with Visual Studio 2015 or 2017. The API will connect to a database via Entity Framework Core. We're starting from scratch, and we'll end up with a fully functioning API that'll expose information on cities and their points of interest, as well as allowing us to create, update and delete these. What you can see on screen now is the finished project and the API's functionality. You'll notice that we will start out with a simple, easy codebase, and we'll keep on improving on that codebase. So in each module, the code will get a bit better. That means that in the beginning of the course, we might do things that we'll switch out for better solutions later on in the course once we learn about them. It's all about gradual improvement. The first few modules are about introducing ASP. NET Core and its most important concepts, and building the API itself: getting, creating, updating and deleting resources, using an in-memory data store. Then in the last two modules, Entity Framework Core is introduced and the in-memory data store is replaced by a database. This is on purpose because when building an API, the persistence logic shouldn't matter as far as the outer facing API contract is concerned. That allows us to focus on each specific part of the application, one by one, instead of introducing everything at the same time. In this first module we'll dive into the big picture. We'll learn what ASP. NET Core,. NET Core and the. NET Standard are and how they are related. And we'll quickly dive into code already. We'll start a new ASP. NET Core web project, and we'll look into how it's structured. We'll learn how requests are handled, and how we can manipulate that with middleware--a key concept in the. NET Core world. But before we can really start, let's have a look at the prerequisites for this course.

Tooling
As this is a beginner course, you don't need to know anything about ASP. NET Core or Entity Framework Core. Everything will be explained as we progress through the course. But knowledge of C# is a requirement. As far as tooling is concerned, you've got a few options. The first option is Visual Studio 2015 Update 3. This can be used to build ASP. NET Core 1. 0 projects with the x-project based project system. If you don't have a licensed Visual Studio 2015 version, you can also use the free Visual Studio 2015 Community Edition. The second option is Visual Studio 2017. You can use Visual Studio 2017 to create ASP. NET Core 1 projects using the more familiar csproj/MSBuild-based system. And if you want to create an ASP. NET Core 2 project, you'll need version 15. 3 at least. A free version exists as well--the community edition. ASP. NET Core applications can also be developed using other IDE's like Visual Studio Code, which also runs on OS X and Linux, and Visual Studio for Mac. As far as the code for creating our API demo project is concerned, there's about a 95% overlap between ASP. NET Core 1. x and ASP. NET Core 2. That means that almost everything code-related you'll learn in this course can be applied to both versions. When there are differences, we'll switch between Visual Studio 2015 with ASP. NET Core 1 and Visual Studio 2017 with ASP. NET Core 2 to show them. As we're setting up our environment and solution in this module, it's this module where we'll already encounter most of the differences. The exercise files contain three demo flavors--one for use in Visual Studio 2015 with ASP. NET Core 1, one for use in Visual studio 2017 with ASP. NET Core 1, and another one for use in Visual Studio 2017 with ASP. NET Core 2. The reasons for choosing a specific version or environment over another one are many. As far as the IDE is concerned, this might depend on the IDE licenses you've got at your disposal, or on the functionality the IDE and tooling offer. The experience in Visual Studio 2017 is much more streamlined than when working with Visual Studio 2015. In Visual Studio 2015, the tooling never got out of preview stage. Personal preference is also important. If you prefer to use Visual Studio Code, by all means, you should. As far as code is concerned, it might seem like a no-brainer to go for ASP. NET Core 2. New versions often offer improved performance, better security, and an extended API surface. And this is the case with ASP. NET Core 2. Often, the extended API surface is one of the main reasons to choose a newer version. It might contain functionality you need but wasn't available in an older version. On the other hand, the support lifecycle can also be important. Not all versions are supported for the same amount of time, which might lead to choosing an older version. We're getting to the support lifecycle differences soon. In any case, the different exercise file flavors have got you covered. To create requests to the API we'll build and to see the response messages, we'll use Postman. Postman is a free tool you can use to, amongst other things, create and send HTTP requests and inspect the response. Bundled together with the exercise files are all HTTP requests we'll send in this course. So you can just import them in Postman and use them. Saves you a bunch of typing. As an alternative, you can use Fiddler, or any other tool of choice that allows you to create HTTP requests and inspect HTTP responses. And you'll need a browser of choice as well. You can use this up to a certain point to send those requests, but without a plugin or developer tools, it'll become hard to create the requests we need and inspect the response message. That's all that's needed to get started. Let's have a look at ASP. NET Core's big picture.

ASP.NET Core: The Big Picture
What IS ASP. NET Core? Let's start at the top, and drill down. ASP. NET Core is an open-source and cross-platform framework for building modern cloud-based internet connected applications. Think about web applications, but also APIs for those web apps or mobile apps. And even apps for the Internet of Things. As it's open source, you can find the source code and contribute to it at github. com/aspnet. It can run on Windows, Mac and Linux. And you can also develop on those three systems by using Visual Studio, Visual Studio Code, Sublime with a plugin, or other editors depending on the OS you're running. Important to know is that it was rethought from the ground up. It's not an updated version of ASP. NET 4. ASP. NET 4 and previous versions, weren't that lean and granular. They were based on the System. Web assembly, which often contained a lot of functionality you might not need. That's very different from what ASP. NET Core is. It's is based on a set of granular NuGet packages, and that has the benefit of allowing us to use just those packages we need. That results in a smaller application surface area, which has a few benefits--tighter security, reduced servicing, and improved performance, to name a few. With ASP. NET Core 2, these were improved upon further. So that's in essence what it is. Now, what do these ASP. NET Core applications run on? Well, an ASP. NET Core application can run on the full. NET Framework or on. NET Core. So, it needs one of those. The full. NET framework is the one we're all used to. Before getting into why we would choose one or the other, let's look into what. NET Core is.. NET Core is a modular version of the. NET Framework designed to be portable across platforms for maximum code reuse and code sharing. It's a subset of the full. NET Framework, and it provides key functionality to implement the app features you need and reuse this code regardless of your platform target. So rather than having different versions of. NET for different platforms that lacked some functionality depending on the platform, it's the same. NET Core that runs on all these platforms. When. NET Core 1 was released, the APIs offered were still quite limited, but they've been vastly extended in version 2. As far as platforms are concerned, for web apps, those includes Windows, Linux and Mac. Others types of Microsoft platforms that can be targeted with. NET Core include traditional Windows desktop apps, Windows devices and phone apps, and Xamarin apps. It's called modular because it is released through NuGet in smaller assembly packages, rather than one large assembly that contains most of the core functionality, so the same principle as is used for ASP. NET Core. Then there's one more term you might have read about--. NET Standard. What's that? Well,. NET Standard is not a framework you can download or anything like that. It's a standard, and that standard defines a common base layer that a platform should support.. NET Core is Microsoft's implementation of that standard. So that's. NET Core, the. NET Standard, and ASP. NET Core, and how they are related to each other. So now, let's look into why we'd choose one or the other. As mentioned, an ASP. NET Core app can run on the full. NET Framework or on. NET Core. So that is a choice we'll have to make. If we run it on the full. NET framework, we gain the ability the use dependencies that target the full framework. You can't just use any assembly that targets the full. NET framework in. NET core. But as. NET Core matures, there's less need to target the full. NET Framework. And that's especially thanks to the larger API surface area. NET Core 2 offers which allows the most common dependencies to be easily ported. And that's important because by targeting the full. NET framework, we lose out on a lot. Running on it would mean we'd lose the modularity and small memory footprint of. NET Core. We lose the performance improvements and the cross-platform capabilities. And, in fact, any reason there was to create. NET Core is something we won't take advantage off were we to use the full framework. So, unless you really, really need a full. NET framework feature, the suggested choice to make is to run ASP. NET Core on. NET Core. And with that, we know what we'll be working with. Let's see how we can get our machine ready to start developing.

Demo: Downloading and Installing .NET Core
Microsoft has made the installation process really easy for us. Let's open up a browser of choice, and navigate to microsoft. com/net/core. If you haven't installed your preferred flavor of Visual Studio yet, this is a good time to do so. The latest available version of the. NET Core SDK can always be found here, and currently that's 2. 0. Depending on when you're watching this course, you can expect the versions to be different.. NET Core, by design, has a pretty fast release schedule. The SDK download also includes all the necessary tooling. If you're looking for older versions, you can find them by clicking downloads, and then selecting. NET Core. Let's scroll down a bit, and there we can find the link to the. NET Core release archive. Let's click that. You can see there're two types of releases--LTS, or Long Term Support releases, and Current releases. Let's click that link. Long Term Support (LTS) releases are supported for three years after the general availability of the Long Term Support release or one year after the after the general availability of a subsequent Long Term Support release. Current releases, on the other hand, are likewise supported related to the parent LTS release, but they're also supported for three months after the general availability of a subsequent Current release. So, depending on the support cycle you require, you might want to go for an LTS release instead of a Current release because it is important to know that Current releases don't necessarily become LTS releases. Let's go back. Important to know is that for Visual Studio 2015, the tooling was still a separate download, and it never got out of the preview stage. So if you're using Visual Studio 2015, you'll need to install the tooling next to. NET Core 1. 0. If you're using Visual Studio 2017, you're good to go with just the SDK download for your preferred version. Also important to know is that these versions can all be installed next to each other. Let's navigate back. After downloading the file, all that's required is running it, and the installation process will start. I already did that on my machine. And once that's done, we can open Visual Studio and start a new project. So we choose File, New Project, and in our list of templates, we should now see. NET Core project templates, as is indeed the case. Once you see those, you're ready for the rest of the course. Let's immediately start with our first ASP. NET Core project.

Demo: Starting a New ASP.NET Core Project
In this demo, we'll create a new ASP. NET Core project from scratch, and we'll inspect its structure. We've still got Visual Studio open from the last demo where we choose to create a new project. We're going to create an ASP. NET Core application, so let's filter on that so we only see the templates related to what we want to do. Here, we see that there's two different options--an ASP. NET Core web application running on. NET Core, or one running on the full. NET framework. We just learned the difference between these two and concluded that. NET Core is the go-to option unless you have a really good reason to use the full. NET framework. So, let's choose. NET Core, and let's choose a name. As we're going to create an API that will return information about cities, CityInfo. API sounds like a good name. The solution itself will be named CityInfo. And I'm going to put that in my Pluralsight demo folder. We click ok, and then we can choose the template we want to start from. Depending on the template we choose, additional files and dependencies will be added. Some starter samples might be included, and some configuration might be filled out. We're building an API, so the obvious choice might be to just start from the Web API template. That's the easy way to start. But due to the fact that it will already contain a sample controller and some code and dependencies related to building an API, it's also not the best way to understand what's going on. Moreover, we'd probably end up with references and files we don't really need. So what I like to do for almost all projects and applications is to start from the empty template. This not only allows us to add exactly what we need and nothing more than that, but it's also the perfect starting point to explain the ins and outs of building an API with ASP. NET Core. So, let's choose that. Once we click ok, NuGet restore will download the dependencies used by the empty template. So yes, even the empty web project template already contains some code. Let's have a look at that. Consider this part of the demo a run-through of the basics, but don't worry if it feels like a lot to grasp at once. We'll revisit all of these concepts multiple times throughout the course. At the end of the course, all of this will feel very familiar. We've got a source folder and one new project in it, our new ASP. NET Core project--CityInfo. API. If we look at that, we see that a program class and startup class have been added. Let's have a look at those. Program is the starting point of our application. This will look familiar to those of you who've worked with console applications beforeâ€¦ and that's because asp. net core is a console application. More specifically, it's a console application that calls into ASP. NET specific libraries. The Main method you see onscreen here is responsible for configuring and running the application. In our case, we'll be running a web application, so we need to host that application. For that, an instance of WebHostBuilder is initialized. Now, as we're hosting a web application, we need a web server. ASP. NET Core is completely decoupled from the web server environment that hosts the application. It actually ships with two different HTTP servers--WebListener, which is a Windows-only web server, and Kestrel, a cross-platform web server. Kestrel is the default, signified by the UseKestrel statement. But we're in Visual Studio, and that uses IIS Express as its default web host. The line UseIISIntegration signifies that IIS Express functions as a reverse proxy server for Kestrel. If you intend to deploy your application on a Windows Server, you should run IIS as a reverse proxy server that manages and proxies requests to Kestrel. If deploying on Linux, you should run a comparable reverse proxy server, such as Apache, to proxy requests to Kestrel. Now you can directly self-host your application using Kestrel alone. But using something like IIS as reverse proxy brings a lot of advantages. For example, IIS can filter requests, manage the DSL layer and certificates, make sure your application restarts if it crashes, and so on. UseContentRoot specifies the content root directory used by the web host. The content root is the base path to any content used by the app, such as its views and its web content. By default the content root is the same as the application base path for the executable hosting the application. If you want, you can provide an alternative location by passing it in the root content statement. But by default, it's the current directory, which is okay for our purposes. An important fact is that the content root isn't the same as the web root. We can see the web root here. Its default path is content root/wwwroot. The web root of an application is the root location in a project from which HTTP requests are handled. And then there's the line UseStartup. This specifies the startup type to be used by the web host. That's the other class that was added when we created the empty web application. We'll look into that shortly. Build then builds an IWebHost instance to host our web application. And with that, we've configured the webhost and returned an IWebHost implementing instance. So all that's left is calling. Run on that instance, which will run the web application, and it blocks the calling thread until the host shuts down. So, quite a lot of explanation for a few lines of code. But it's quite important to know these basics. If you're working with ASP. NET Core 2, this code can remain as-is, but it can also be simplified. So let's have a look. By default, when you start a new empty web project, the program class will look like this when targeting ASP. NET Core 2. The main method just contains one statement, BuildWebHost. And that statement calls into CreateDefaultBuilder on an IWebHost instance, followed by the Startup type to use and. Build(). CreateDefaultBuilder will create a default builder with all the options we just ran through. So at first sight, this code is a shortcut for what we just ran through. But it actually does a little more than that. As ASP. NET Core is open source, we can have a look at the actual code that will be executed. Let's open GitHub. We see that statements like UseKestrel, UseContentRoot, and UseIISIntegration can be found in the implementation. But there're other statements as well. Default files and variables for configuration are set up, as is logging. We will look into these later on in the course. Now, when you see code like this, you might be inclined to remove the lambda as it seems unnecessary. The code would then look as such, and it would do the same. It looks a lot cleaner, but the thing is that the BuildWebHost lambda by convention is used by Entity Framework Core's tooling when using Entity Framework Core 2. 0. Removing it will lead to errors when using certain commands, like when adding migrations through the add-migration command. So it's best to leave the lambda as-is. We'll look into migrations in the module on EF Core. Alright, let's switch back, and let's have a look at the Startup class. The Startup class is the entry point for an application. At this moment, there're two methods in there. ConfigureServices is used to add services to the container and to configure those services. That container is used for dependency injection. We'll get to that later on in the course. But what are services in the. NET Core world? A service is a component that is intended for common consumption in an application. So that really is quite broad. There're framework services, like identity, MVC, EF Core services, but there're also application services, which are application-specific. For example, later on in the course, we'll introduce a component to send mail. That's an application-specific service which we will make available for dependency injection by registering it in this ConfigureServices method. And then there're some services that are built-in, like the ApplicationBuilder and a Logger. There's currently no code in ConfigureServices, but regardless of that, those built-in services are available for injection. So the services we add here can later be injected into other pieces of code that live in our application. One example is the injection in the Configure method we see on-screen. An instance of an IApplicationBuilder interface implementing class, one that implements IhostingEnvironment and one that implements ILoggerFactory are provided by the container. Don't worry if this doesn't sound familiar yet. We'll look into the built-in dependency injection system in a lot more detail later on. ConfigureServices is, by the way, an optional method. But most if not all applications will require at least some services that aren't automatically registered already. And the second method is the Configure method. This one is called after the ConfigureServices method because it uses services that are registered and configured in that method. It's used to specify how an ASP. NET Core application will respond to individual HTTP requests. This is where, later on, we'll have to let our application know it has to use MVC for handling http requests. At this moment, every HTTP request will result in Hello World! being printed out on screen. Let's set a few breakpoints so we can see the flow, and let's run this. I'm going to put one in Main, one at ConfigureServices, and one at Configure. Let's run. So, first, we hit the Main method of our Program class. This is where our web host is built. Up next, ConfigureServices is called, and after that, as expected, the Configure method is called. Once app. Run is reached, we will see Hello World! printed out onscreen. And as expected, that's exactly what we see. So why is this the case? Well, it's the case because that's how we configured HTTP requests to be handled in the Configure method. So maybe now is a good time to have a look at that request pipeline in a bit more detail.

The ASP.NET Core Request Pipeline and Middleware
Whenever an HTTP request comes in, something must handle that request so it eventually results in an HTTP response. Those pieces of code that handle requests and result in responses make up the request pipeline. What we can do is configure that request pipeline by adding middleware, which are software components that are assembled into an application pipeline to handle requests and responses. If you've worked with an older version of ASP. NET, you know that modules and handlers took care of this for us. In ASP. NET Core, the roles of both modules and handlers have been taken over by middleware. A few examples of middleware that can be added to this pipeline are diagnostics middleware or middleware to handle authentication, and even MVC itself is a piece of middleware that can be added to the request pipeline. Now let's look at such a request. The ASP. NET request pipeline consists of a sequence of request delegates called one after the next from one piece of middleware to the next. Each of those has the opportunity to perform operations before and after the next delegate. And that eventually results in a response we need. Important to know is that each component chooses whether to pass the request on to the next component in the pipeline or not. So the order in which we add middleware to the request pipeline matters. A good example would be authentication middleware. If the middleware component finds that the request isn't authorized, it will not pass it on to the next piece of middleware, but it will immediately return an Unauthorized response. It's, thus, important that the authentication middleware, in case of our example, is added before other components that shouldn't be triggered in case of an unauthorized request. Let's see how we can add middleware to the request pipeline with a demo.

Demo: Configuring the ASP.NET Request Pipeline
In this demo, we'll configure the ASP. NET Core request pipeline. We're going to set up basic diagnostics by showing a developer-friendly exception page when an exception is thrown. But we only we only want to show that in the developer environment. And when I say we are going to do that, well, the people who design the empty web project template already did a lot of work for us. Let's have a look at this statement-- app. UseDeveloperExceptionPage. This configures the request pipeline by adding the developer exception page middleware to the request pipeline. So now when an exception is thrown, this piece of middleware will handle it. The middleware is part of the microsoft. aspnetcore. diagnostics assembly. Most middleware can be found in separate assemblies, a testament to the modularity of ASP. NET Core. Let's make sure we throw an exception so we can see what happens. So instead of showing Hello World!, our application will now throw an exception on each request. As we've configured the request pipeline to use the developer exception page, we see a nice error page with a lot of information that might be useful to us when developing the application. So we see the exception that's thrown. And if we continue, we see the developer-friendly exception page. Let's close this because there's something else we need to talk about. We're going to skip on the loggerFactory for now. That's coming up later. But right below that, there's another line of code--if (environment. IsDevelopment()). This signifies that the developer-friendly exception page middleware will only be added when we're running in the development environment. So maybe this is a good place to talk about separate environments because we might want to execute different pieces of code depending on a specific environment. And this is a good example of that. The developer exception page is not something we want to allow everyone to see, right? We only want this during development, but not in a production environment. Let's have a look at the Debug tab of the project properties. Here we can find the ASPNETCORE_ENVIRONMENT variable. It's set to Development currently. ASP. NET Core references a particular environment variable, hosting environment, to describe the environment the application is currently running in. Three values are used by convention--development, staging, and production. But you can add your own if you want to. Mind you, an environment isn't the same as a type of build. You might roll out a debug build to a staging environment for example. All in all, it's a pretty nice addition. We're now able to easily write code for different environments. Back to our code. To programmatically access this value, we can use the IHostingEnvironment service, which provides the core abstraction for working with environments. It's provided by the ASP. NET hosting layer and can be injected. And as we see, it's already injected in this method. Let's have a look at that. Through the env variable, we now have access to information on our hosting environment--ApplicationName, ContentRootFileProvider, ContentRootPath, etc. We want to make sure the developer exception page middleware is only added when we're in a development environment. And, otherwise, we want to do something else like catch the exception and optionally log it. And there's middleware for that as well, the exception handler middleware. We can pass in parameters to the UseExceptionHandler method like a friendly error page to show. But for now, let's keep on that. We just want to learn the principles of working with different environments. We're currently in the development environment, so let's change that to production. And let's give it a try. Our exception is thrown. By the way, this exception is thrown twice. That's because this code is excecuted on each request. The first time you see that exception, well, that's actually a request to show the favorite icon of the web page. And we see exactly the same as before. So what happened here? Well the problem is (and this is quite important) when you're working with different environments, the new name is only picked up after the web server has restarted. So what we actually need to do is restart IIS Express or Kestrel to be more exact. The easiest way to do that, just restart Visual Studio. So let's do that. And let's give that another try. Our exception is thrown. And this time the exception handler middleware is triggered. The exception details aren't shown, and as we didn't specify a friendly error page to direct to, we end up with a blank page. Let's have a look at the developer tools. You can access those by pressing F12 on your browser. In here we see two errors, Failed to load resource, 500 (Internal Server Error), once for the favorite icon and another time for the root page. This is the result of the exception that was thrown. But, wait a minute. We just learned that the exception handler middleware also logs the exception. So where is that log then, and how? Well, as you can guess, that loggerFactory that's injected in the Configure method has something to do with that. ASP. NET Core has built-in support for logging. We'll get to that once we actually start building our API. And with that, we are already at the end of this demo. We now know how to configure the ASP. NET requests pipeline and how to work with different environments. Time for the module summary.

Summary
In this first module, we learned about the basics. We started out with the big picture. ASP. NET Core is an open-source and cross-platform framework for building modern cloud-based internet connected applications. Important to know is that it was rethought from the ground up. It's not an updated version of ASP. NET 4. The keywords are tighter security, a small footprint, cross-platform, and performance. These are improved upon further with ASP. NET Core 2. 0. An ASP. NET Core application runs either on the full. NET Framework or on. NET Core. That last one is the go-to option.. NET Core is a modular version of the. NET Framework designed to be portable across platforms for maximum code reuse and code sharing. It's also an implementation of. NET Standard--a standard that defines a common base layer that a platform should support. Then, we created our first ASP. NET Core web application. We learned that the Program class is the starting point of our application. The Main method of that class is responsible for configuring and running the application. In that Main method, the IWebHost instance is built, which hosts our web application. As we've seen, when using ASP. NET Core 2. 0, this can be simplified by calling into WebHost. CreateDefaultBuilder. That sets up the webhost with default options, and automatically sets up configuration files and logging, both of which we'll look into later on in this course. The Startup class is the entry point of our web application. ConfigureServices is used to add services to the container for dependency injection, and to configure those services, while the Configure method is used to specify how an ASP. NET application will respond to individual HTTP requests. Lastly, we learned how to configure the HTTP Request pipeline by adding middleware to it. While doing that, we dived into working with different environments. We used middleware in the development environment to show a developer-friendly exception page, while we used the regular exception handler middleware for other environments. And with that, the basics are covered. In the next module, we'll dive into creating our API and returning data.

Creating the API and Returning Resources
Coming Up
Hi there, and welcome to the Creating an API and Returning Resources module from the Building Your First API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. We ended up with a bare bones web project in the previous module. Now it's time to start building our API. We learned that we can add various types of middleware. One of this is the ASP. NET Core MVC middleware, which we'll need to build our API. Once we've got that up and running, we'll learn how to return data or (to use the more correct terminology) resources from our API and how to interact with that API by sending HTTP requests. We'll also learn how we can configure the form of the response that's sent back to the consumer of the API. That's quite a lot of content coming up in this module, so let's dive in.

Middleware for Building an API
What do we need to start building an API? Well, we know from the previous module that we're going to have to add some middleware to the HTTP request pipeline in the Configure method and that we'll have to add the framework services to the container in the ConfigureServices method. If we take one step back to the previous version, those of you who've worked with that will know that when building HTTP services, we choose Web API. And to build a client-facing web application, views and all, we choose ASP. NET MVC. This has changed. In ASP. NET Core, there is no separate Web API framework anymore. The old ASP. NET MVC and Web API functionalities are now unified in one framework--ASP. NET Core MVC. It's positioned as a rich framework for building web apps and APIs using the model-view-controller design pattern. This makes sense. After all, MVC is a design pattern that was used both by ASP. NET MVC and ASP. NET Web API. The middleware we'll have to configure and add to the pipeline is thus the ASP. NET Core MVC middleware. Let's clarify the MVC pattern.

Clarifying the MVC Pattern
MVC is an acronym, and it stands for model-view-controller. Looking at the definition, it's an architectural software pattern for implementing user interfaces. There's some confusion on what it exactly is and does because different interpretations on what it should be exist. Essentially, though, it's meant to encourage loose coupling and separation of concerns, which in turn allows better testability of your application and the ability to reuse parts of it. Mind you, it isn't the full application architecture. In a typical n-tier architecture--presentation layer, business layer, data access layer, more often than not with a service layer in between--MVC is used in the presentation layer. And that's about it. Hence, the implementing user interface part of the definition. If you've ever worked with Angular, well, in some regard you could see that as a client-side MVC implementation. It consists of three parts. The model handles the logic for your application data. A model in this sense can contain code to retrieve or store data at that level. In some implementations, often when the MVC pattern is only used at the top level of your architecture, the model doesn't contain any logic at all. It's another component of your application that handles this. For example, the business layer. In other implementations, a view model, which does contain code to retrieve or store data, is in fact used as the model. The view represents the parts of the application that handle the display of data. This might be HTML, for example. And the controller then handles the interaction between the view and the model including handling user input. If we look at the dependencies between these components, we see that both controller and view depend on the model, and the controller also depends on the view. That's one of the key benefits of this separation. In other words, the controller chooses the view to display to the user and provides it with any model data it requires. But with all this talk about user interfaces, how does this map to building an API? Well that view in API terminology is the representation of our data or resources. More often than not these days, that's JSON. And user interaction? Well, the user or consumer of an API would typically be another application. So when a request is made by an API consumer, an action on the controller will be triggered. The controller sends any input data, query string parameters for example, to the part of the application responsible for business or data access logic. And it returns a model to the view. That view in this case doesn't contain any logic like a Razor or HTML-based you might have, but it's a representation in JSON, for example, of the returned data. And with that, it's time to fire up Visual Studio.

Demo: Adding the ASP.NET Core MVC Middleware
In the first demo of this module, we'll add the ASP. NET Core MVC Middleware to our application. We're in the Startup class. And the first thing we'll have to do is add a reference to MVC as the services and middleware are defined in an external dependency. The NuGet dialog is one way of doing this, but I want to show you a pretty nifty feature in Visual Studio--NuGet integration. There's integrated NuGet support in our IDE. If you know the name of the method you need, that's a good way to add the necessary references. I happen to know that, to add the MVC framework services, we need to call the AddMvc extension method in the ConfigureServices method. If we do that, we get a prompt stating that this extension method seems to be available in the microsoft. aspnetcore. mvc package, and we can now add the dependency directly from here. You need to know this in advance. It won't just show up in IntelliSense. Another option as just said is using the NuGet dialog to look for the ASP. NET Core NuGet package and add the reference. Let's do that. If you've been working with Visual Studio in the past few years, you know how this works. We right-click on the project, and we choose Manage NuGet Packages. The package we need is microsoft. aspnetcore. mvc. And let's install the latest stable release. MVC has been added. The necessary packages have been restored. When using ASP. NET Core 2, by default you don't have to manually add this reference. We'll look into that right after this demo. Back to the startup class. And in ConfigureServices, services. AddMvc now works. On to the Configure method, where we'll have to add the MVC middleware to the request pipeline. For that, we call app. UseMvc(). We add this to the request by applying after having added the exception handler middleware to the pipeline. So the exception handler middleware can potentially catch exceptions before handing the request over to MVC and, more importantly, handle exceptions and return the correct response when an exception happens in the MVC-related code we'll write. From this moment on, the MVC middleware will handle http requests. That also means that we can now get rid of the code from the previous module where we threw that exception. We're currently still working in the production environment after the last demo of the previous module. So let's set this back to the development environment so we see developer-friendly errors in case something happen. Let's restart so the new value gets picked up by the webserver, and let's run. We get a blank page. Let's have a look at the developer tools, and we see that what we actually got back was a 404 Not Found status code. So we added to the MVC middleware, but it doesn't do anything yet. It can't find any code to execute that will result in a representation of data being returned. There is no code yet. And there's no routing configured to allow for that. So, that brings us to the next demo, in which we'll add our first controller to return data, or rather, resources, like cities and points of interest for cities. But first, let's talk about the change ASP. NET Core 2 introduced as far as adding references is concerned.

ASP.NET Core 2 Metapackage and Runtime Store
ASP. NET Core 2 introduced two new concepts--Microsoft. AspNetCore. All metapackage and the Runtime Store. Let's look into these. In the demo, we just added an ASP. NET Core related NuGet package to our solution for the first time. As we learned, ASP. NET Core is modular, i. e., it's made up of a lot of different, small assemblies, rather than one big System. Web assembly as it used to be in the old ASP. NET. But that comes with its own set of issues. It's not always easy to find out in which package the functionality you need can be found. Moreover, keeping an eye on using the correct versions of these assemblies can become quite cumbersome. But that's the way it is for ASP. NET Core 1 applications. For ASP. NET Core 2, this was solved with the Microsoft. AspNetCore. All metapackage, which is referenced by default for new ASP. NET Core 2 applications. With a metapackage, you can add a list of packages by just referencing one package. The Microsoft. AspNetCore. All metapackage for ASP. NET Core includes all supported ASP. NET Core packages, all supported Entity Framework Core packages, and some internal and third-party dependencies used by ASP. NET Core and Entity Framework Core. If we have a look at this metapackage at NuGet, we can see that the list is quite big. If we scroll down a bit, we can find the MVC package we just added, but also a lot more packages by the ASP. NET Core and Entity Framework Core teams. In other words, instead of having to look for these packages and trying to find out where a certain piece of functionality lives, the reference to the Microsoft. AspNetCore. All metapackage ensures a reference to all these packages. And that includes IntelliSense support in Visual Studio. That begs the question, Where do these packages come from? Well, in ASP. NET Core 2, the concept of the runtime store was introduced. When you install ASP. NET Core 2, on your machine a folder will be created. And this folder contains packages required for running ASP. NET Core applications. If you remember the Global Assembly Cache from when using the full. NET Framework, well, it's more or less back. And it has advantages--faster deployments because less files have to be deployed, lower disk space use because multiple applications can use that same store, the packages are only on your disk in one location instead of in each application folder. Lastly, improved startup performance, because the packages are precompiled. Let's have a look at that folder on my machine. You can find it underneath program files, dotnet, store. We see it contains all the packages we also saw at the NuGet page of Microsoft. AspNetCore. All. So that sounds good and easy, right? But it comes with a trade-off. Important to know is that when using this approach and publishing the app, the required assemblies are no longer copied to your application folder. On the left, you can see the output of the solution we'll end up with at the end of this course, on ASP. NET Core 1, after publishing. On the right, there's the output after publishing when working with the Microsoft. AspNetCore. All package and the runtime store on ASP. NET Core 2. So that's a lot less. The Microsoft packages are no longer copied when publishing ASP. NET Core 2 applications by default. So that's a trade-off because that means to use this approach, you'll have to install the runtime and thus the runtime store on each machine you want to publish to. ASP. NET Core 1 applications were self-contained by default. There was no need to install the runtime on the machine you'd publish to. ASP. NET Core 2 applications by default are no longer self-contained, but framework dependent. The correct version of the runtime must be installed on the machine you want to publish to. To be honest, I'm not sure I feel very positive about that approach. There's definitely advantages to it during development. But not making ASP. NET Core 2 applications self-contained by default can hurt when publishing the application. Here's the good thing though. This approach is the default for ASP. NET Core 2 applications, but it's also optional. So if you want, you can still just add the references you need, one by one, instead of the reference to the metapackage. It's up to you to choose the approach that best fits your requirements or use case. And with that, we can continue.

Demo: Returning Resources (Part 1)
We're building a CityInfo. API. So a CitiesController is a good controller to start with. Just as before, we could use a built-in template. But let's not do that as to better understand what's going on. I'm just going to add a new class and name it CitiesController. Let's start by adding a folder for our controllers and the CitiesController class. We want to let this class derive from controller, from the Microsoft. AspNetCore. Mvc namespace. Then we'll need an action that returns data. At this moment, we'll keep this fairly simple. We want to return JSON as the representation format of our data. For that, we can use the JsonResult class. This returns JSONified version of whatever we pass into the constructor of JsonResult. We want to return a list of cities, but we currently don't have a city model class, so let's start with two anonymous objects giving it an ID and a name. When consuming HTTP services like our API to get data back, we should send an HTTP request with HTTP method Get to a URI that routes to this method. That's a few different concepts to explain. Let's start with the request itself. Any type of client that consumes an API sends an HTTP request to that API. You can use a browser for that. It's easy enough for GET requests as that's what a browser sends to a URI. But we'll run into restrictions quite fast when we want to start sending requests to create or update resources. So we'll use Postman for that as that allows us to create HTTP requests and inspect the responses. So let's open up Postman. In Postman, we can create our requests. As mentioned in the first module, I created collections of all the requests we'll use throughout the course. If you download the exercise files, you can just import them. Let's open the first one. So here we can choose the HTTP method to use for the request and the URI. Later on, we'll also work with headers and the request body. But for now, a GET request with all the defaults will do. Let's run the application and then send this request. The request was sent, and we get back a 404 Not Found result. This is because the MVC framework doesn't know how to map this URI, api/cities, to the action on the controller we just created. It's the principle called routing that handles this. So let's look into that.

Learning About Routing
Routing matches a request URI to a method on the controller. So once we send an HTTP request, the MVC framework passes the URI and tries to map it to one of our controller-driving controllers and the corresponding methods on that controller. There're two ways it can do this--convention-based or attribute-based. Let's look at an example of convention-based routing. For this, we have to configure these conventions. We can do that by passing in these routing conventions to the UseMvc extension method. An example can be seen on screen. This would map the URI cities/ index to the Index method on a controller named CitiesController. But these types of convention-based mappings are typically used when you're using the MVC framework to build a web application with HTML-returning views. For APIs, the ASP. NET Core team recommends not using convention-based routing, but attribute-based routing instead. Attribute-based routing, as the name implies, allows us to use attributes at controller and action level. We provide these with an optional template. And through that template, a URI is matched to a specific action on a controller. For this, we use a variety of attributes depending on the HTTP method you want to match. Let's have a look at the most common ones for the functionality a typical API would want to expose--reading, creating, updating, and deleting resources. First up, reading resources. The correct HTTP method for this is GET. In code, we use the HttpGet attribute on the action that contains the code to read and return a resource. A sample URI would be api/cities for a list of cities and api/cities/1 for a city with an ID 1. Then there's creating a resource. POST is used for this signified by the HttpPost attribute also at action level. In the action, the code to create a resource should reside. A sample URI is api/cities to create a city. For updating resources, two options are available. The first one is PUT, which should be used for full updates. All fields of the resource should be overwritten or set to their default values. The HttpPut method is used at action level, and a PUT request to api/cities/1 would update the city with ID 1. But you don't always want to fully update a resource. In fact, more often than not, you'll need partial updates to update only one or two fields instead of all of them. That's what a PATCH action is for signified by the HttpPatch attribute. A sample URI is the same as for PUT, api/cities/1 to partially update the city with ID 1. The last of the HTTP methods is the DELETE method to delete a resource. The HttpDelete attribute should adorn the action that deletes the resource. A sample URI is, again, api/cities/1 to delete a city with ID 1. And, lastly, there's a special attribute that doesn't match to a specific HTTP method--the Route attribute. This is used at controller level to provide a template that will prefix all templates of the action level attributes For example, we see that all our sample URIs start with api/cities. The Route attribute could be used at controller level with api/cities as its template value. Like that, we don't have to provide that value in the templates of all the action level attributes. So far for the theory. Now let's have a look at how we can use these. In this module, we'll look into the Route and HttpGet attributes, and we'll encounter the other ones further on in the course.

Demo: Returning Resources (Part 2)
We know we want to respond to a GET request, so let's use the GET attribute. We want this controller action to be executed when we send a request to api/cities. So that's what we pass in as a routing template. Let's build and run. And let's send the request to api/cities via Postman. And this time, we get back our two cities. So that's nice. We've just created our first API action that actually works. Let's go back to our controller. A controller typically contains all the actions, for example, to get a single city or maybe to create or update one. How these are consumed should be consistent, i. e., we want to consume all our resources by starting with API. And all the resources in this controller start with /cities. We can use the Route attribute at controller level for this so we only have to define this once and not on each action. Now the template we pass in contains the plural noun cities, and that's actually the prefix of the Controller class name. We can refer to that by putting controller in straight brackets in the template. That will then match the Route api/cities to the CitiesController. But to be honest, I'm not a big fan of that approach. If we were to have refactoring of our code later on and rename the Controller class, the URI to our cities resource would automatically change as well. This can be considered an advantage for websites. But for APIs, it's more of a disadvantage in my opinion as a resource URI should remain the same regardless of what the underlying class is named. That's of no importance to the consumer of the API. But that's just my opinion. You're obviously free to disregard this if you want. Let's change it back to api/cities, and let's give this a try. Let's send a request again, and there we go. But we're still just returning some JSON directly from anonymous objects. That isn't necessarily a realistic scenario. And in most architectures, you'd work with model classes, POCOs, or DTOs that are then serialized to JSON. Let's improve on our architecture in the next demo.

Demo: Improving the Architecture with Model Classes
So in this demo, we'll get rid of returning JSON directly. Instead, we'll create model classes. Let's start by adding a Models folder and one class, CityDto. Let's give it an ID, a name, and a description. There's an important distinction I want to make here. Maybe we're running a bit ahead of ourselves, but it's quite important. What is returned from or accepted by an API is not the same as the entities used by the underlying datastore. Currently, we're going to be working with in-memory data as we are focusing on the API itself. So the in-memory datastore will simply work on these DTO classes we are now creating. But later in the course, we'll add Entity Framework Core to the mix. The Entity classes that that will work on will be different from these classes. For example, a CityDto might contain calculated fields like NumberOfPointsOfInterest. A PersonDto class might contain a full name property, which is a concatenation of first name and last name. These are fields that aren't saved as such in a database. They're calculated on the go and won't be part of the city Entity class. And the other way around exists as well. In the next module, we'll learn how to create resources. The model accepted by such a method doesn't contain an ID for a city as it will be the responsibility of the underlying store to create that. So that'll be another class as well. It won't be the cityEntity class we'll use with Entity Framework. Anyway, let's continue. We'll need some data to return. So for now, let's add an in-memory data store containing a list of cities. We'll change this out for a persistent database store once we learn about Entity Framework Core. Let's add a new class, CitiesDataStore. Let's give it a property, Cities, a List of CityDto. And let's copy/paste in a bit of code to initialize the dummy data. By the way, a little piece of trivia, Antwerp in Belgium is where I live, and one of the symbols of our city really is a big cathedral that was never finished. And it was supposed to have two towers and look a bit like the Notre Dame in Paris as you can see on the pictures here. But the city ran out of money. That may or may not say something about this city. But anyway, let's ensure we have a static property on our CitiesDataStore, and let's name that Current. It returns an instance of CitiesDataStore. This makes sure we can keep working on the same data as long as we don't restart. The syntax you see here is new to C# 6. It's the auto-property initializer syntax. This allows assignment of properties directly within their declaration. For read-only properties like this one, it makes sure the property is immutable. Back to our controller. We can now switch out the code we have here by getting the cities from the current Cities store. And let's give this a try. Our GET request to api/cities now returns the three cities from our CitiesDataStore, and while we're at it, we can continue with implementing an action to return a single city. Let's add a new action, GetCity. Let's decorate it with the HttpGet attribute. The URI to this resource is api/cities followed by the ID of the city we want to get. To work with parameters in our routing templates, curly brackets are used. Now as api/cities is part of the route template for our controller, we can just use ID between curly brackets as a parameter for the GET attribute. That ID parameter, that's an integer. And it should be in the parameter list of the action. This ID parameter will automatically be set to the ID from the URI. And let's look for the city with that ID and return a JsonResult with the city in JSON format in the response body. Let's build, run, and fire up Postman. And let's try getting a city by appending the ID to the URI. If we pass in ID 1, we get back New York City. And if we pass in another ID, we get back the matching city. But what would happen if we tried to send a GET request to an un-existing resource? For example, api/people? We get back a 404 Not Found, so that's good. And if you remember from the previous module, we got back a 500 Internal Server Error when we threw an exception. These are HTTP status codes, in this case, automatically returned by the framework. Yet if we try and get a city that doesn't exist but lives at the URI that can be routed to an action, like api/cities/5, we get back a 200 OK status code with a null result. The framework doesn't automatically return a 404 because our URI can be routed. But null isn't the correct result. If a city is not found, we'd expect a 404 Not Found status code. We should be able to check for cases like this, which our code currently doesn't, and we should correctly return status codes. Let's learn why this is so important.

The Importance of Status Codes
We already learned that to consume the API, the consumer sends an HTTP request using an HTTP method like GET. He then gets back an HTTP response. This response contains a status code, and it's really important to get these right because these status codes are the only thing a consumer of the API, like a client-side web app, can inspect to know if a request worked out as expected or if something went wrong, and if something went wrong, whether it's the fault of the consumer or the fault of the API itself. Imagine you're sending back a 200 OK status code for each and every request. The consumer would assume his request worked out as expected even if it went wrong. And imagine that when something goes wrong, we'd only send back a 500 Internal Server Error. The consumer would never know if it's his fault something went wrong, for example, sending a GET request for a city that doesn't exist, meaning he can correct it and try the request again, or if it's the server's responsibility, in which case the client shouldn't try and correct a mistake because the mistake isn't his. There're a lot of status codes, and an API doesn't necessarily have to support all of them. But let's look into a few common ones. There're five levels of status codes. Level 100 status codes are informational and weren't part of the HTTP 1. 0 standard. These are currently not used by APIs. The level 200 codes mean the request went well. The most common ones are 200 for a successful GET request, 201 for a successful request that resulted in the creation of a new resource, and 204 for the successful request that shouldn't return anything like a delete. Level 300 status codes are used for redirection, for example, to tell a search engine a page has permanently moved. Most APIs don't have need for this. Then there're the status codes that tell the consumer he did something wrong, level 400 client error. 400 means Bad Request. The request, you, as a consumer of the API sent to the server is wrong, for example, the JSON the consumer provided can't be parsed. 401 Unauthorized means that no or invalid authentication details were provided. And 403 Forbidden means that authentication succeeded, but the authentication user does not have access to the requested resource. 404 Not Found means that the requested resource doesn't exist. And 409 is used for conflicts, for example, an edit conflict between two simultaneous updates. And, lastly, there's level 500 server errors. Often only the 500 Internal Server Error is supported. This means that the server made a mistake, and the client can't do anything about it other than try again later. Let's see how we can implement this.

Demo: Returning Correct Status Codes
In this demo, we'll learn how to return correct status codes with our responses. We're back in our CitiesController, and we need to return a response with a specific status code. Currently, we're still returning a JsonResult here. Let's look at the JsonResult. If we look at the definition, we see that JsonResult is an ActionResult, which formats the given object as JSON. And ActionResult, well, that's a default implementation of IActionResult. IActionResult defines a contract that represents the result of an action method. But this JsonResult, well, that's not always what we want to return. First of all, APIs don't necessarily work with JSON. They should return what a consumer asks in the Accept header of the request if that format is supported. But more importantly, it's not always simply a bunch of JSON you want to return. We just learned about the importance of status codes. We need to be able to return a response with a specific status code. Now this is possible with JsonResult as well. Let's put our result in a temporary variable, and if we look at that temporary variable, we can see we can set its status code. For example, do 200, and then we just return our temporary variable. But this way of working can become quite cumbersome. ASP. NET Core contains a set of built-in helper methods on the controller that all create an IActionResult. There's Not Found when a resource is not found. Bad Request when a request is malformed. And we'll encounter a few more of these. Let's change the action signatures to return an IActionResult instead. First, let's try and get the city we passed in ID. If it doesn't exist, we return a 404 Not Found by using the Not Found method. Otherwise, we want to return a 200 OK containing the city in the response body. That can be done with the OK method passing in the city. If an exception that isn't handled happens, the framework automatically returns a 500 Internal Server Error. One of the other advantages of using these methods rather than JsonResult is that the results aren't necessarily returned in JSON format. If we support other formatters in our API, the consumer can decide how he wants to see the results. We're covering that later on. Now we can also change the action above. Rather than returning a JSON result, we now want to return a 200 OK. There's no Not Found here because even an empty collection is a valid response body. The resource, Cities, was found. It just happens to be an empty list. Let's build and run and try that request from the previous demo again. So we'll send a GET request to api/cities/5. That request returned a null value previously, which wasn't correct. This time we do return the correct status code, a 404 Not Found. Before we end this demo, I'd like to cover one more thing. We've used Postman to create our requests. But as mentioned before, a browser is also an HTTP client that sends HTTP requests. Let's try getting a city that doesn't exist directly from the browser. We know this returns a 404 Not Found, but we have to open the developer tools to see this. There's an alternative we can use, status code pages. Let's open the Configure method of the Startup class again. ASP. NET Core contains status code pages middleware. If we add that to the request pipeline, a simple text-based handler for common status codes is added. We can add this by calling UseStatusCodePages on app. And let's try that again. That's already nice. Instead of an empty page, we get a 404 Not Found on an un-existing resource. And if we try our un-existing city, we get that same status code in text onscreen. And this text message will also be visible in Postman. Whether or not you want to support this is up to you of course. But if you want to do that, well, this is how. There's more to returning the correct status codes than what we've seen in this demo. Imagine working with child resources, i. e., the points of interest for a city. What's the correct status code to return if the parent city isn't found? And what's the correct one if the parent city is found but the collection of points of interest is empty? Let's check that out in the next demo, Returning Child Resources.

Demo: Returning Child Resources
In this demo, we'll combine what we've learned up until now to return the points of interest for a city including the correct status codes. First, let's add a new class PointOfInterestDto. And let's give it an ID, a name, and a description. Then let's extend the CityDto class so it includes a collection of points of interest. It's a good idea to always initialize this to an empty collection instead of leaving it at null as to avoid null reference issues. Typically you'd do this in the constructor, but we can use C# 6's auto-property initializer syntax for this as well. And we can also calculate the calculated field, NumberOfPointsOfInterest. And let's add some dummy data to our datastore for these points of interest. Let me just copy/paste that. And there we go. Now we can add a new controller to look into that extra check we have to make. Let's add a new class and name it PointsOfInterestController. Let's make sure it inherits Controller. And we can add the Route attribute. Now we have to make a decision. If we want to allow access to the points of interest regardless of the city, to Route would be along the lines of api/pointsofinterest. But a point of interest doesn't necessarily make sense without a city. It's a resource that's a child of another resource. So I like to reflect this in the URI, i. e., points of interest of a city should be accessible to api/cities/cityId/pointsofinterest. That means we start Controller Route with api/cities, and then we add new actions. First, GetPointsOfInterest. We decorate it with the HttpGetAttribute and pass in cityId/pointsofinterest. This means that combined with the Route attribute, this action will be executed when a consumer sends a GET request to api/cities/cityId/pointsofinterest. So we accept this cityId as a parameter. And this is where we need to check something else. We just learned in the previous demo that an empty collection shouldn't result in a 404 Not Found because the resource can be found, it's just an empty list. In this case, however, we should return a 404 Not Found more specifically when the city with passed-in cityId doesn't exist because in that case, the points of interest can never exist either. So we look for the city, and if the city is null, we return Not Found. If the city can be found, we return OK passing in the points of interest. Likewise, an extra check should be added for getting one specific point of interest. We add a new action first, GetPointOfInterest. The HttpGetAttribute's template this time contains two IDs, the cityId and the ID of the point of interest. We also accept these two as parameters. First, we try and find the city. We can just copy/paste that. If the city doesn't exist, we return Not Found. Then we try and find the point of interest with the passed-in ID on that city object. If it's not found, we return Not Found. And, otherwise, we return the point of interest with an OK 200 status code. Let's try this out. First, let's get the points of interest for a city that exists. The city with ID 1. And we get back Central Park and the Empire State Building, two points of interest in New York City. If we try a city ID that doesn't map to a city in our datastore, we should get a Not Found. And indeed that's the case. Then let's try getting one point of interest. We pass in a city ID and a point of interest ID, and this works as expected. We get back New York City Central Park. If we now change the city ID to 4, we again should get back a Not Found. So far, so good. And we also get a Not Found if we try and get a point of interest for a city that exists but doesn't have a point of interest with that ID. For example, we try to get a point of interest with ID 10 for New York City. That doesn't exist. And indeed a 404 Not Found. And if we select the single city, we should now get back the city, the amount of points of interest, and the points of interest themselves. And that's indeed the case. And with that, we've added support for child resources including the checks required to return the correct status codes. But when we select the city, we also get back the points of interest. That's because we coded it like that. Our in-memory datastore works with the DTO classes. And those contain points of interest for each city. The consumer of the API has no say in this, but as you know, later on in this course, we'll introduce Entity Framework Core. And that allows us to easily choose whether or not we want to load child entities from the underlying database. So at that moment, we learn how to let the consumer of the API decide whether or not to return the points of interest when requesting a city resource. Now let's have a look at the result formatting. If we look at the casing of the JSON we get back, we see we get it back in camel casing. The first letter is lowercased. And that's good. Most modern consumers expect this type of casing like an Angular application--property names starting with a lowercase letter. But if you happen to have worked with the old ASP. NET Web API, you'll know that this approach is different from what we're used to. By default, the property names were serialized as they were defined in our classes. In this case, that would mean with an uppercase first letter. So if you're creating a new API to replace an old API built with Web API, your consumers might expect the casing to be the same as how the property names were defined on the city class in this case. We can change this with the help of serializer settings. Let's have a look at that in the next demo.

Demo: Working with Serializer Settings
In this demo, we'll look into how we can work with SerializerSettings. ASP. NET Core by default deserializes from and serializes to JSON. But how exactly do we configure how that serialization happens? Well remember that we can use the ConfigureServices method in Startup that's used to configure the services made available through the container. Well we can configure MVC like that. Here we've added the MVC services. On that we can call AddJsonOptions. This expects an action to set up these options, so we can add that with a lambda expression. From the options parameter, we can access the SerializerSettings. It's the ContractResolver we're looking for because what we actually want to do is overwrite its default naming strategy. By default, it's set to change property names so they start with a lowercase letter, and we don't want that. So let's try and get that ContractResolver, and let's cast it to a default ContractResolver. I just had to add a using statement to use this default ContractResolver. It was a using statement to Newtonsoft. Json. Serialization. So in case this look familiar, yes indeed this is JSON. NET, ASP. NET Core uses JSON. NET by default for handling JSON. So now we've got the castedResolver, and we want to access the NamingStrategy. And what we want to do here is set that to null. From this moment on, JSON. NET will simply take the property names as they are defined on our class. Let's give that a try. This is still the request from the previous demo you see onscreen. So with lowercase first letter for each property name, now let's try and send that request again. And, indeed, this time around, the first letter is uppercased just as it's defined on the City class. As I said, you might need this or you might not. I'm guessing that for most new applications, the default, i. e., lowercase letter to start with, will be what's required. So I'm going to comment this out again. But like this, you now know how to access and manipulate the JSON SerializerSettings. And that brings us to Formatters and Content Negotiation. Somehow the MVC framework must know what format to serializer to or deserialize from. And it does that with input and output parameters. The default format is JSON. But that's not always what we want. Let's have a look.

Formatters and Content Negotiation
A key concept when developing an API like this is that of content negotiation. This is the process of selecting the best representation for a given response when there are multiple representations available. When you're building an API that only your one client application has to consume, you might be okay with always getting back your representations in JSON format. But if you're building an API for consumption by multiple clients, some of which you have no control over, chances are that not all of these clients can easily consume JSON representations as a response. Some might be better off with XML or another format. The consumer can request a specific format by passing in the requested media type through the Accept header. For example, if the Accept header has a value of application/json, the consumer states that if your API supports the requested format, it should return that format. If it has a value of application/xml, it should return an XML representation, and so on. If no Accept header is available, or if it doesn't support the requested format, it can always default to its default format. In most cases today, that's JSON. So we're dealing with output here. ASP. NET Core supports this via output formatters. An output formatter deals with output. The consumer of the API can request a specific type of output by setting the Accept header to the requested media type like application/json. But if there's output, well, there's also input. We're running a bit ahead of ourselves with that. We don't provide input in a request body yet. That's what we'll do once we start creating and updating resources in the next module coming up right after the next demo. So this does seem like a good place to mention this already because next to output formatters, ASP. NET Core also supports input formatters. An input formatter deals with input, for example, the body of a POST request for creating a new resource, or input to media type of the request body is identified through the content-type header. Let's see how we can configure these.

Demo: Formatters and Content Negotiation
In this demo, we'll learn how to work with formatters and content negotiation. By default, only the JSON input/output formatters are used. Let's have a look at that request for the list of cities we already have. We never passed in an Accept header value, yet we get JSON back. Our API defaults to that. If we add application/json as the Accept header (and it's good practice to always add an Accept header because you want to be clear about what you want from the API; you might not have control over what the people who build the API might decide on as its default data format in the future), we should also get back JSON as is indeed the case. Now let's request XML, by changing the Accept header to application/xml. And let's send that request again. And we still get back a JSON. That's because no XML output formatter was configured, so the API just defaults back to JSON. We can change that. Let's open the ConfigureServices method of our Startup class. AddMvcOptions allows us to configure the supported formatters for our API. InputFormatters signifies the list of input formatters, and as you can guess, output formatters signifies the list of output formatters. The XML formatters are part of the Microsoft. AspNetCore. Mvc. Formatters. Xml assembly. And what we're looking for is the XmlDataContractSerializerOutputFormatter. That's the one we want to add to the list of supported formatters. So we call Add, and we pass in a new instance of that XmlDataContractSerializerOutputFormatter. And as we learned, if you already know the name of the class you want to use, you can add a dependency like this through the integrated NuGet support in Visual Studio. Otherwise, you can also use the NuGet dialog. So I'm adding the package, and there we go. As we know now, this step isn't necessary if you're using ASP. NET Core 2 with the Microsoft. AspNetCore. All metapackage reference. Likewise, for input, the XmlDataContractSerializerInputFormatter can be used. Now if you'd want to remove a formatter, like the default JSON formatters, you can use the "Clear" to clear all formatters or "Remove" to remove a specific one. But let's not do that. We do want to keep on supporting JSON, right? Let's build this, run, and try that request again. First, let's give it a try without an Accept header. And we get back JSON. The default formatter is the first one in the collection. So far, so good. Now let's try it with an Accept header, application/xml. And this time we get back XML from our API. And that's it for this demo. And it leads us nicely into the next module, where we'll learn how to manipulate resources. There, we will use the content-type header for providing the media type of a request body. But first, let's check out the module summary.

Summary
We started out by clarifying the MVC pattern--model-view-controller. The model handles the logic for our application data. The view represents the parts that display data, which in case of an API that's the data typically in JSON format returned from API calls. And the controller handles the interaction between view and model. This pattern leads to more reuse and better testability. We added the MVC middleware and learned we can use an HttpGet to return data from our API. And it's routing that takes care of mapping a request URI to a method on our controller. Next to GET, there's also POST for creating, PUT for updates, PATCH for partial updates, and DELETE for delete. All of these are coming up right after this summary. For APIs, attribute-based routing is advised over convention-based routing. We also learned about content negotiation. That's the process of selecting the best representation for a given response. Technically, ASP. NET Core handles this without output formatters by looking at the Accept header of the request which contains the desired media type. If input has to be provided, an input formatter takes care of that by looking at the content-type header. And with that, this module is done. But we're currently just returning data from our controllers. Very few APIs only require that. Most also require the ability to manipulate data. Let's check that out in the next module.

Manipulating Resources
Coming Up
Hi there, and welcome to the Manipulating Resources module from the Building Your First API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. After the previous module, we now know how to get resources from our API. In this module, we'll take it one step further. We'll look into resource manipulation. That means we'll learn about creating, updating, and deleting resources, all pretty common functionality for an API. And as for now, manipulating resources, that means we should also validate the input. Let's get started with the first demo, creating a resource.

Demo: Creating a Resource
In this demo, we'll learn how to create a new point of interest for a city. We know from the previous module that we'll have to use POST for that, so let's dive in. We've already touched upon the fact that what we send to the API and what we get back from the API isn't exactly the same as what is stored in the data store. The further along we get in the course, the more this will shine true, especially once we add Entity Framework Core to the mix. But at this moment, when creating a resource, it already starts to become obvious. We've got a point of interest DTO here currently used by our in-memory data store, and also returned when getting a city or list of cities. That's not a full separation yet, but looking at the point of interest DTO class, it becomes clear to see why we shouldn't use this for our POST. It has an ID. That ID, that's something the server or API is responsible for, or at least in the application we're building. So it's not something that can be chosen by the consumer of the API. With that in mind, that also means that this ID shouldn't be part of the DTO used for creation. So let's add a new class, point of interest for creation DTO without that ID. Now that our system's where the consumer is responsible for choosing the ID, so it is a valid use case, and if that's the case it would lead to the point of interest DTO and the point of interest for creation DTO to contain the exact same fields. But even in those cases, I'd suggest to keep these separate. It leads to a model that's more in line with the API functionality, making change or refactoring afterwards easier. And when validation comes into play in the next demo, you might want validation on input, but that's not necessarily the same validation, if at all, that's needed for output. So, use a separate DTO for creating, for updating, and for returning resources. On to our points of interest controller. We'll want to add a new action and decorate it with the HTTP POST attribute. Let's name it Create Point of Interest. Too, our template, contains the city ID first, followed by points of interest. So, we accept the city ID as a parameter. And then we need to be able to get the content of the request. This request body will contain the data for the point of interest, which we want to deserialize into a point of interest for creation DTO. That's what a From body attribute is for. There's a few things that can go wrong with POST requests. Things we'll have to check for. A new possible status code comes into play here. The consumer might send a bad request in such a way that the request body cannot be deserialized into a point of interest for creation DTO. If that's the case, the point of interest parameter will be null, and that's a mistake the consumer makes. So we want to let the consumer know this by returning a Bad Request 400 status. Also, the consumer might send a request to a resource URI that doesn't exist, adding a point of interest to a non-existing city. So we check for that, and we return Not Found if the city doesn't exist. Let me copy paste that over from one of the methods we created earlier. If both check out, we can add the new point of interest, but we need to calculate the ID for the new point of interest, and our data store currently works on our point of interest DTO, not on a point of interest for creation DTO, so we'll have to map that DTO to a point of interest DTO. We can calculate the ID by running through all points of interest across cities, and getting the highest value. We add one, and we use it when creating the point of interest we're going to add to our in-memory data store. For that, we also copy over the values of our input to our new object, and we add it to the city's points of interest. Now, as you might notice, this is quite cumbersome. Mapping can easily lead to errors, and running through all cities and their points of interest just to get an ID, well, that isn't ideal either, performance-wise on one hand, and on the other hand, this code doesn't take possible errors when multiple consumers try to get an ID at almost the same time into account. But just as with most things in this course, we start somewhere, and we keep on improving on what we learn throughout the course. So later on, we'll be able to work with auto-generated IDs, and we will switch out this manual mapping to another component. But for now, this will do. Lastly, we also need to return something. For POST, the advised response is a 201 Created response. Helper methods exist, and the one you want is the Created At Route Helper method. This one allows us to return a response with a location header, and that location header will contain the URI where the newly-created point of interest can be found. If we scroll up a bit, we see that this would have to refer to the route to our action to get a single point of interest for a city. We can give that a name so we can refer to it. Let's name it, Get Point of Interest, and then let's scroll down again and paste this in the Created At Route method. We first paste in Get Point of Interest, the Get Point of Interest Route template, needs a city ID, and ID of our point of interest. So for that we paste in an anonymous type with those values. And we also paste in the newly-created point of interest, which will end up in the response body. That should be it. Let's give this a try. What we want to do is create the POST request to City ID forward slash Points of Interest. So let's add a point of interest to the City With ID tree, Paris. There's a nice cemetery there named PÃ¨re Lachaise, which is where Jim Morrison and Oscar Wilde are buried. Jim's grave is quite small, a bit run-down, even, but nice in its way, and Oscar actually has a big tomb full of lipstick of all the people who come there and kiss the tomb. So, it's definitely worth a visit, and this means that each self-respecting city info API should contain that info. And that is what we paste in, in the request body. We paste it in as JSON, but we need to tell this to our API somehow, so it knows how to deserialize this. That's what the content type header is for. In our case, with the value of application JSON. Postman can detect the format of the body, and it will automatically add this content type header. And this brings us back to the previous module, where we talked about input and output formatters. This tells the framework that it should chose to chase an input formatter to deserialize the request body. Let's send this. We get back a 201 Created response, with the newly-created point of interest in the body, and if you look at the header collection, we see that there's now a location header so the consumer knows how to access this new point of interest. Let's give that a try. And indeed, we see our newly added point of interest. We now know how to create a resource. But, when creating resources, one of the things you'll probably want to do is validation of your input, and we haven't done that yet. So, that's coming up next.

Demo: Validating Input
In this demo, we'll learn how we can validate input by working with ModelState. Let's have a look. Let's start by sending a request with invalid input. We'll send a POST request, but instead of providing input that can be serialized into a point of interest for creation DTO, we send an empty body. I've put a break point in the code we ended up within the last demo so we can see what happens. We've hit the break point. Let's have a look at that point of interest parameter. It's null. deserialized into a point of interest for creation DTO. So it's null, and that's invalid input. We check for that, and we return a bad request. So, so far so good. If you now look in Postman, we see that we indeed get back a 400 Bad Request. But there's more than one type of invalid input. Let's try sending a request without a name, with a property that doesn't exist, invalid property, and with a fairly long description that I got from the Pirate Ipsum generator. Point of interest isn't null anymore, so this passes True or Null check without any problem. The invalid property is ignored. We can live with that. Name is null, description is very long. And that's not really realistic. More likely than not, we'll run into an error later on, depending on whatever rules we might have on our data store. Not null columns or max length restrictions, for example. Those will typically result in an exception, but we don't want our API to just hit an exception and return a 500 Internal Server Error. We want to check these rules on our model, our DTO classes, after all, it's a mistake made by the client, not by the server, so it shouldn't return a 500 Return Server Error. It should return a 400 Bad Request. Let's dive into code. We haven't got any rules yet. We could check for null values or string length manually. And then return a bad request. But that will become cumbersome quite fast. There's a better way: data annotations. Let's open our point of interest for creation DTO model class, and apply one. Required is a good one to start with. They can be found in the System. ComponentModel. DataAnnotations name space from the System. ComponentNull. AnnotationsAssembly. And let's have a look at that. Here we see a list of data annotations we can use. There's attributes to check for valid credit card numbers, email addresses, minimum and maximum length, and so on. So with these attributes, we can already get pretty far as far as validation is concerned. Let's add a maximum length for name and description. Say, 50 for name, and 200 for description. But this is just part of the story. We also need a way to check if these rules are adhered to, and this is where the ModelState comes into play. Back to our point of interest controller. We're in Create Point of Interest. ModelState is a dictionary. It contains both the state of the model, and model-binding validation. It represents a collection of name and value pairs that were submitted to our API, one for each property. And it also contains a collection of error messages for each value submitted. Whenever a request comes in the rules we just applied to our model, the DTO, are checked. If one of them doesn't check out, the ModelStates Is Valid property will be false. And this property will also be false if an invalid value for a property type is pasted. So it's a great value to check to see if we can continue with what's in our action. Let's do that. We do need to keep the null check as the ModelState will be valid if the body can be deserialized to the expected type. Let's try this. I'm going to delete my break points, we don't need them anymore. So, let's send that request we sent earlier again, and this time we do get back a 400 Bad Request. The rest of our action isn't executed. There's more we can do. Let's open the DTO again. These attributes allow us to specify an error message. Let's try that for the required attribute of the name field. To return these errors to the client, we pass in the ModelState when we return Bad Request. And let's try that again. And this time, we see we get back our customized error message, and also the default error message for max length. But what about more complex validations? Rules we can't check with the built-in data annotations? Well, we will have to write code for that ourselves, but we can add errors to our ModelState so they are returned when we return a bad request with ModelState included. Say we don't want our consumers to input the same description as was given as Name. There's no attribute to handle that, so we'll write that check ourselves. If description and name are the same, we add a model error. We have to give this a key. This can be a property name, but it doesn't have to be. This way of working comes from the fact that the same framework is now used for building APIs, as well as client-facing web apps. For those, adding the correct property name results in validation errors being applied to fields in the view bound by that property. So, they're nicely displayed in line. Let's try this by sending a POST request with the same values for name and description. So, name and description are the same, we send it, and this time we get back a 400 Bad Request containing our validation error message. Now, to be honest, I am not always a big fan of the approach taken here. Annotations mix in rules with models, and that's not really good separation of concerns. And having to write validation rules in two different places, the model and the controller, for the same model doesn't feel right either. But this is the default approach as taken by the old ASP. NET for some years now, and also by ASP. NET Core. So, I did want to cover it. However, if you're building more complex applications, it might be a good idea to check out something like FluentValidation, which offers a fluid interface to build validation rules for your objects. From version 6. 4 and onwards,. NET Core is supported. And that covers input validation. Let's continue with updating a resource.

Demo: Updating a Resource
There's two ways of updating a resource: a full update and a partial update. In this demo, we'll cover full updates. For that, we use the PUT HTTP methods. We're back in our point of interest controller. As we learned previously, it's a good idea to use different models for create, update, and delete. Even if they, at this moment, contain the same properties. So let's start by adding a DTO for updating a point of interest. The same rules apply as for creating one, so we'll add those as well, a name field, that's required, with a max length of 50 and a description field that has a maximum length of 200. Then, the action on our controller. Let's name it Update Point of Interest. For full updates, we'll decorate it with the HTTP PUT attribute. Though our template contains the city ID first, followed by points of interest, and then followed by the ID of the point of interest we want to update. So, we also accept City ID and ID as parameters. And then we need to be able to get the content of the request, for which we'll use the From body attribute again. That's it for the signature. Then, just as in POST, we check for null, we check for that additional validation rule, and we check in the ModelState is valid. So I'll quickly paste that in, and there we go, the exact same code as in our POST methods. And, by the way, this kind of drives the point home that having to have all these rules repeated over and over again in the model or in the controller actions, is something that can be improved upon. It's okay for the simpler cases, but for more complicated cases, FluentValidation might be a good option. Next, we check if we can find both the city and the point of interest we want to update. And if any of those fail, we return Not Found. First, let's find the city, and if it's null, return Not Found. And then let's try and find the point of interest, and we also return Not Found if that point of interest can't be found for this city. And then we can update. And this is where this full update principle comes into play. According to the HTTP standard, PUT should fully update the resource. That means that the consumer of the API must provide values for all fields of the resource, save for the ID, as that one is already coming from the URI. If the consumer doesn't provide a value for a field, that field should be put to its default value, which conveniently, it will have in the inputted object. We must thus always update all fields. In our case, that's just a name and a description. And lastly, we return a tool for NoContent. That means that the request completed successfully, but there's nothing to return. You can also return a 200 Okay, containing the updated resource, but typically for updates, you'd return NoContent as the consumer already has all the information. After all, it's the consumer who sends the content that had to be changed. Let's try this out. We'll send a request PUT to update New York City's Central Park name and description. We get back a 204 NoContent, so the update was successful. Now let's get this point of interest. And indeed, it's been updated. And now let's try out that full update principle, by not providing a value for description. So, that's a new PUT request with only a name in the body. We get back a 204 NoContent, so the update was successful. Now let's get that point of interest again, and the description was set to null. This is completely correct according to the HTTP standard, but it might not be ideal for consumers of the API. Imagine a case where a resource has 20 fields. You only want to update one of those fields. That would mean that the consumer would first have to get the full resource, including all fields and values, from the API, and then send them back with the PUT but with one field changed. That's quite a lot of overhead. It would be nice if there was some sort of way to send a Change Set over the wire, only containing those fields that have to be updated, and various JSON documents combined with the patch methods. Let's check that out.

Partially Updating a Resource
The PATCH HTTP method is for partial updates. But what should the body of a patch request look like? It needs to contain the fields that have to be patched, and the value those fields need to get. And if you take it one step further, it also needs to contain an operation that has to happen, for example, a patch request that copies the value of one property to another property. Luckily, there's a standard format for this. The standard is named Json Patch RFC 6902. And what it does is define a JSON document structure for expressing a sequence of operations to apply to a JSON document. In our case, that document to apply the sequence of operations to is the JSON we get back when requesting a resource. In essence, the consumer of the API will create a JsonPatch document as the body of the batch request, adhering to this standard. Such a body is a list of operations, like and at, replace, copy, et cetera operation, including the respective field names and values. Our API should accept such a patch document, and then apply all those operations to the resource. Let's have a look at an example of such a request body. It starts with straight brackets, signifying an array. And that array is a list of operations that have to be applied to the resource. We see two operations on screen. First one is a replace operation, signified by op. Path signifies path to the property, name in this case, and value signifies the new value that should be given to the name property, new name. The second operation replaces the description with value new description. Other operations are supported as well. In fact, there's a lot of cases that are supported by JsonPatch. You can add to an array, remove from an array, replace one value by another, copy or move a value, change nested properties and so on. It's a really powerful standard. Going over all of these would definitely lead us too far. The most important thing to remember is that a JsonPatch document is essentially a list of operations, like, at, remove, replace, etc, that have to be applied to the resource, allowing for partial updates. Let's check that out with a demo.

Demo: Partially Updating a Resource
In this demo, we'll look into how we can partially update the resource with Patch. We're back in our points of interest controller where we left off. Patch is signified by the HTTP PATCH attribute. At Route template is the location of the point of interest, just as with the full update. We then need an action, let's name it Partially Update Point of Interest, that accepts these two parameters. And then we need the input from the request body as a third parameter. So, we can again use From body, and then we need a class to which we can deserialize that list of operations. That's a JsonPatch document from the Microsoft. AspNetCore. JasonPatch namespace. Such a document works on a specific type, so what is that type that we want to patch? There's two ways we can go about this, point of interest DTO or point of interest for update DTO. A point of interest DTO, well, that has an ID. And you do not want to allow an update to change the ID of what you're updating. That would no longer match with the ID we pass in in the URI, so if you go this way, we need yet another validation check. The other way is applying the patch document to the point of interest for update DTO. That already includes the validation annotations, and it doesn't have an ID, but it does mean we need an extra transformation. We need to get the matching point of interest and map it to a point of interest for update DTO, before applying the patch. I personally prefer that approach, as it's much more in line with what we've done until now. But, if you don't like to work with different DTOs for different types of actions, just remember to check that the patch doc doesn't try to change the ID. First things first, we check if the input isn't null, and if it is, we return a bad request. Then we check if the city and point of interest can be found. If not, we return Not Found. This is getting quite familiar by now. And then it's time to apply the patch document. It's a patch document that works on a point of interest for update DTO, so we need to transform our point of interest we got from the store to a point of interest for update DTO. And to this variable, point of interest to patch, we can then apply the patch document. Applying a patch document can be done by calling Apply To on the patch document, passing the object to patch. But, something might be wrong with the patch document. We could have created a patch document to change a property that doesn't even exist, for example. We want to know that, because if that's the case, the consumer has made a mistake, and we need to return a bad request. Well, if we pass in the ModelState, in the Apply To methods, any errors of that type will be added to the ModelState's error collection. So we do that, and afterwards, we check this state and return Bad Request if it's not valid. If it is valid, the update was successful. So, the rest of the code is exactly the same as for our regular update. We copy over the property values and we return NoContent. Let's give that try. We're going to start simple, with a patch request to change the name of New York City's Central Park. As we learned, a patch request's body contains a JsonPatch document, an array of operations. To change the name, we pass in replace as the operation, name as the path, and a new value for the name. And we get back at 204 NoContent. That means the request was successful. Let's try and get New York City's Central Park. We see that the name has indeed been replaced. Let's try another one. A JsonPatch document consists of an array of operations, so we can add another operation. That's replace as operation value, description as the path, and the new value for the description. We again get back a 204 NoContent. And if we get New York City Central Park, we see that both name and the description have been updated. Now let's try sending an invalid request, an update for a property that doesn't exist. So we paste in an invalid property as the path. And this time, we get back a 400 Bad Request, including the validation error on the JsonPatch document. Replace of a value means removing the value first, and then replacing it with a new value, so the error we get first is that the property at the unexisting path could not be removed. So, this is looking good. But, we're missing something. What about our validation rules? Let's try a request that would lead to a validation issue. We're going to clear the name. This is a required field. We have a required annotation on the name property, so we do not want this to succeed. So, we send a JsonPatch document containing one operation, remove the name. And we get back a 204 NoContent, so it succeeds. And let's get this point of interest now. And it looks like the name has been cleared. That's not good at all. So, why is this? Let's go back to our code. After having applied the patch document, we check if the ModelState is valid. And apparently it is valid, otherwise the rest of the code wouldn't be executed. This ModelState contains errors on the inputted model, and the inputted model, well, that is in the point of interest for update DTO. The inputted model is a JsonPatch document. So, as long as that document is valid and the patch document can be applied, the ModelState will be valid. So what we need to do, additionally, is check if the point of interest for update DTO is still valid after applying the patch document. First, let's add that extra validation rule we couldn't achieve with validation annotations. Name and description must be different. Then, let's validate it. To do that, we can use the TryValidateModel method, pasting in the now-patched point of interest for update DTO instance. This triggers validation of this model. And any errors will also end up in the ModelState, so after that we check ModelState again, and return bad requests containing the errors if it's not valid anymore. Okay, let's try that again. Let's try and remove that name. This time, we get back a 400 Bad Request, and the expected error message. And let's try one more. If we try updating name and description to the same value, we should also get back a bad request. And that is indeed the case. So with that, we've learned about patching for partial updates. As we've seen before the demo, it can also be used to manipulate arrays, to copy or move values, and so on. It also works on dynamic types, allowing you to actually manipulate your objects by adding or removing properties at run time. So, it's really powerful. There's one more HTTP method we have to look into, DELETE, for deleting resources. So let's have a look at that.

Demo: Deleting a Resource
In this demo, we'll learn how to delete a resource. And deleting resources tends to be one of the easier things to do. We start out on our points of interest controller, and we add a new action. Let's name it Delete Point of Interest. For delete, we decorate the action with HTTP DELETE attribute. Just as for update, this has to route to the specific point of interest we want to delete, as route template. So, the parameter list consists of a city ID and an ID for the point of interest. Just as before, the first thing we do in our action, is check if the city and point of interest for that city exists. And if they don't, we return Not Found. If they do, we can effectively remove the point of interest. The response has no body, so that's a 204 NoContent. And that's it already. Let's check this out. Let's send a delete request to New York City's Central Park. We get back a 204 NoContent, so that checks out. Let's just make sure by trying to get all points of interest from New York City, and we see that indeed, Central Park has been deleted. And with that, we've covered everything about manipulating resources. Let's have a look at the summary.

Summary
We started this module by learning about POST to create a resource. When the resource has been successfully created, a 201 status code response is what we should return. That was the first time we submitted a request body to our API. The format of it is designated by the content-type header. When talking about input, it should be validated, and that's what we learned next. We can use data annotations on our DTO classes, and then we can check for these by using the ModelState. That's a dictionary containing both the state of the model and model-binding validation. Whenever a request comes in, the annotations applied to the model, the DTO are checked. If one of them doesn't check out, the ModelState's valid property will be false. For those rules that can't be checked with data annotations, we can add a manual check, and call At Model Error on the ModelState if necessary. And then, we looked into updating a resource. PUT is for full updates, PATCH is for partial updates. Partial Updates are often preferred, but they require a way to send through a list of changes. That's where the JsonPatch standard comes in. It's essentially a set of rules for expressing a sequence of operations to apply to the Json resource we want to update. After a successful update, a tool for NoContent status code, or alternatively a 200 Okay status code, should be sent back with the response. DELETE is for deleting resources, and that warrants a 204 NoContent status code as a successful response. So what else does a good API need? Well, I'd say a certain amount of logging is definitely required. ASP. NET Core has a built-in logger service that we can use for that, but before we can use that, we must look into one of the most important ASP. NET Core concepts, working with dependency injection and services. That's coming up next.

Working with Services and Dependency Injection
Coming Up
Hi there, and welcome to the Working with Services and Dependency Injection module from the Building Your First API with ASP. NET Core course at Pluralsight. My name is Kevin, and I'll guide you through the rest of this module. We've built our API, but we're not done yet. All APIs require a certain amount of logging, and ASP. NET Core has a built-in logging service. But to use that logging service, we must inject it into the class that needs it. So, first of all, we'll learn about ASP. NET Core's built-in dependency injection system. And then we'll inject a logger to this. And we'll also learn how to log to a file. Next to built-in services like the logger, we can also create, register, and use custom services. And that's what we'll do next. We'll make sure that when a user deletes a point of interest, the administrator of the system is notified of this. We'll separate that functionality out in a custom mail service. And that also means we'll have to store the mail address to send that mail to somewhere. So that allows us to look into configuration files. Let's dive in.

Inversion of Control and Dependency Injection
So let's talk about IoC inversion of control and dependency injection. We'll start with an example. Imagine a class that uses different services. For example, when we implement logging in the upcoming demo, our PointsOfInterestController will use a logger service. That's a concrete type specified at design time. This means that the PointsOfInterestController has a dependency on the logger. We typically new it up in the constructor. And let's say it also depends on one or more other services. Problems arise when you need to replace or update that logger. The controller source code will have to change, and it also makes the controller harder to test. You can't just replace the logger with a mock version of the logger as we're using the concrete type. And the class needs to manage the lifetime of the dependency. In short, this is tight coupling. And that's where the inversion of control pattern comes to the rescue. It delegates the function of selecting a concrete implementation type for the controller's dependencies, the logger in our example, to an external component. Great, but how do we do that? And that's what the dependency injection pattern helps with. That's, in fact, a specialization of the inversion of control pattern. The dependency injection pattern uses an object (the container) to initialize objects and provide the required dependencies to the object. We've talked about that container before. You register services with the container, and the container is responsible for providing an instance of it to the class that needs that instance. It manages the lifetime of the dependency. Let's have a look at a code sample. So this is what it looks like. We've got a backing field to hold the dependency, but this is not a concrete implementation. We're expecting a class that implements the ILogger interface, in this example, tied to the controller. The constructor has changed as well. This is constructor injection. The constructor of our controller needs an instance of a type that implements ILogger, again, no longer a concrete type, an interface. The container injects the dependency for our class. The net result is that our class is decoupled from that responsibility and from that concrete type. The dependencies can be replaced or updated with very few or no changes at all to the code in our class. And the class itself can easily be tested because those dependencies can be mocked by providing a mocked version of ILogger. Even though a lot of architectures in the old ASP. NET versions contained some form of dependency injection through Ninject, for example, it wasn't built in. In ASP. NET Core, it is. Some services are built in and, by default, registered with the container, like the logger. Others can be added, and that's typically done in the ConfigureServices method of the Startup class. Both framework services and our own application services can be added. In fact, we already did that in the second module when we registered the MVC framework services. Let's see how we can inject one of the built-in services, a logger, into a controller in the demo.

Demo: Injecting and Using a Logger
In this demo, we'll inject the logger into the PointsOfInterestController we previously created. The logger is a built-in service, so we don't have to add it to the container in the ConfigureServices method. What we do need to do is configure one or more loggers so the service knows what to log and where it should log it to. We do that in the Configure method of the Startup class. An ILoggerFactory implementing instance is already injected. And that gives us access to a logger factory. We need to add one or more providers so it knows where to log to. The AddProvider method allows this, but there're shortcuts to this. Actually, we already see one on-screen--AddConsole. This will log to the console window. What I like to do is log statements to the debug window, which is quite convenient when developing. And we can do that by calling AddDebug on the loggerFactory. Let's try that one. NuGet integration tells us we need the Microsoft. Extensions. Logging. Debug package, so let's add that. And there we go. We can choose the minimum level we want to log to the debug window. We can choose to log only critical failures, only debug, error, information, none, trace, or warnings. Let's keep it at the default, which means information level or more serious. As you remember from the first module, in ASP. NET Core 2, the CreateDefaultBuilder call in the Program class takes care of adding both the console and debug providers so there's no need to add these in the Configure method. That's it for configuration for now. Now we need to effectively log something. So let's open our PointsOfInterestController. We could request an ILoggerFactory and create a logger, but there's another way. The container can also directly provide us with an Ilogger(T) instance. When this technique is used, the logger will automatically use the type's name as its category name. So for our PointsOfInterestController we'll need an ILogger(PointsOfInterestController). So let's add that field. And let's add the using statement to Microsoft. Extensions. Logging. Now we need to inject an instance of this. And by adding a constructor, we can use constructor injection. So the container will provide an instance through dependency injection. So let's add a constructor. And let's tell the dependency injection system we expect an ILogger(PointsOfInterestController). And there we go. Constructor injection is the preferred way of requesting dependencies. But for those cases where that's not feasible, you can always request one from the container directly. You can do that through HttpContext. RequestServices. On that we can then call GetService and pass in the type of the service we want. What this does is provide access to this HttpRequest's container. But as said, it's advised to use constructor injection instead, so we'll stick to that. And then, let's effectively log something. In the action that returns the points of interest for a given city, we check if the city exists. So let's log if the city isn't found. There are extension methods for each logging level. We'll need the LogInformation method because this is an informational message. And in that, we can then pass in some information--a message preferably including the cityId. This type of syntax is also new to C# 6. If you haven't encountered it before, this is called string interpolation, a less verbose way to construct strings. By surrounding parameter names that are in scope with curly brackets, they will be replaced by their actual values when the string is constructed. Let's give this a try. Let's try getting the points of interest for a city that doesn't exist. The city with ID 5 for example. We get back a 404 Not Found. So far, so good. And because we added that logger statement, this should also result in a message being logged. Let's have a look at the debug output window. And if we look here, we see our message. We also see other messages being logged. This is logging output from the framework services themselves. Logging informational messages can help you find the cause of an error, but what we really want to log, always, are exceptions. So let's surround our code with a try/catch statement, and log that exception. As it's an exception, we'll want this to have the critical level, so we use the logCritical method, passing in a message and the exception itself. There's one more thing we have to think about. We learned in the status code demo that unhandled exceptions will return a 500 Internal Server Error. But now we're handling the exception, be it simply by logging it, because we're not going to throw it again. So we must return that 500 Internal Server Error manually. And for that, we can use the StatusCode method, passing in the StatusCode. Optionally, a message can be provided. Be careful with this though. This information will go to the consumer of the API, and you do not want to expose implementation details to those consumers. So don't write out a stack trace or anything like that. What often happens is that the exception message is returned through ex. Message, but even that potentially exposes implementation details. We have already logged the full exception, so we have all the information we need. There's no need to return that to the consumer. A simple message is sufficient. And to see if this works, we'll throw an exception. And let's give that a try. Let's try that request again. What we get back now is our 500 Internal Server Error including the message we passed into the StatusCode method. And now let's have a look at the debug output window. And there we go. Our critical exception has been logged. Logging to the debug output window is convenient during development, but in a production environment it's not sufficient. We'll want to log to a persistent store, like the event log, or a database, or a file. Let's see what we can do in the next demo.

Demo: Logging to a File
In this demo, we'll look into how we can log to a file. ASP. NET Core does not contain a built-in logger to a database or a file. It does contain one for logging to the event log, but as at the beginning of the course, we chose to develop for cross-platform, that one isn't available as the event log is a Windows concept. So how do we log to, for example, a file? Well the built-in logging system was built in a way that allows third-party providers to easily integrate with it. We're currently at the GitHub page for ASP. NET's logging component. Let's scroll down a bit. If you look down here, at the moment of recording, there are already different providers available. Some of these allow logging to a file like NLog. Others log to online services like Loggr. Configuration of the logger provider depends on the provider itself. But the way to integrate an external provider into ASP. NET Core's logging system is the same across all of these. It would lead us a bit too far to look into all these providers, but as the integration part is the same, it should be sufficient to look into one of them. So let's choose one that's quite populate in the. NET world, NLog. Back to our solution. The first thing to do is add a NuGet package for NLog. So let's open the NuGet dialog, and let's look for nlog. extensions. logging. Now at the moment of recording, the most recent version is 1. 0. 0-rtm-alpha4. And that's a prerelease version, so we have to make sure to check Include prerelease, and the first one here, that's the one we need. So let's install that. And there we go. Then we'll have to configure NLog. By default, it looks for an nlog. config file in the root folder. So let's add that. We choose to add an empty text file, and we name it nlog. config. In this file, we want to configure NLog. So it logs messages to a file. This is not at all ASP. NET Core related. It's NLog related. So let me paste that in. For the demo, we'll keep it simple. What this configuration does is tell NLog to log anything with level info or higher to a file named nlog to shortdate and. log. NLog, just as most third-party loggers, has a lot more options. You can configure it to log to different files or targets. You can tell it exactly what to log, and so on. All that info can be found at nlogproject. org if you want to have a look. But that would lead us to far for this demo. We're looking into the principle of using a third-party logging provider to log to a file using the ASP. NET Core logging system. So this is already it for the provider-dependent configuration. Now onto integration. Let's open the Startup class. We need to tell the loggerFactory to use this provider. And that's the part that's the same across providers. There's an AddProvider method on the loggerFactory we can use for this. We remember that from the previous demo. This expects an ILoggerProvider implementing instance. In the case of NLog, that's an instance of NLog. Extensions. Logging. NLogLoggerProvider. But there's an easier way. Most of these logging providers include a shortcut extension method. So we don't have to directly call that provider. In case of NLog, that's called AddNLog. We'll require a using statement using NLog. Extensions. Logging, and that should be it. The nice part about the fact that these third-party providers integrate with the built-in logging system is that we don't have to change any other parts of our code. We already have the code to log in the PointsOfInterestController. That same code will keep on working, now also using the extra provider to log to. Let's give that a try by requesting the points of interest for a city again, which will throw an exception. Let's send that request. As expected, we get back our 500 Internal Server Error. In the debug output window, we still see the critical exception we logged. And now let's look at the root directory of our application. A new file has been created with today's date. Let's open that. And here we find everything that's been logged with level info or higher including the critical information as we wrote it out. And with that, we know how to use an external logging provider. Let's get rid of the exception we're throwing as that was for demo purposes only. We now know how to use a built-in service, but we can also create our own services. Let's have a look at that.

Demo: Implementing and Using a Custom Service
In this demo, we're going to create a service that sends mail to a certain address whenever a point of interest for a city is deleted. Deleting something has quite an impact on consumers of our API. All of the sudden, they have access to less information. And that sounds like something we as developers or administrators of the API would want to be notified about. A mail service sounds perfect for that. Mind you, I don't have a mail server set up here, so we won't actually send the mail. The point of the sample is to look into how we can create our own custom services and use them. Let's start by creating a folder, Services, and let's add a new class to it, LocalMailService. We'll need at least two fields here--a To and a From address. Let's add those and give them some dummy values. We'll add one method to this class, Send, in which we'll mimic sending the mail. Let's say we want it to accept two parameters, the subject and the message. And as far as implementation goes, we'll mimic sending the mail by writing out the information to the debug window. And that's our mail service. Now we have to register this with the container. So we can inject it using the built-in dependency injection system onto the ConfigureServices method on the Startup class. When we look at the methods we have on services, our container, there're three that are of importance and allow us to register custom services--AddTransient, AddScoped, and AddSingleton. These refer to the lifetime of the services. Transient lifetime services are created each time they are requested. This lifetime works best for lightweight stateless services. Scoped lifetime services are created once per request. And singleton lifetime services are created the first time they are requested or if you specify an instance when ConfigureServices is run. Every subsequent request will use the same instance. Our mail service, well, that's lightweight and stateless, so we'll use a transient lifetime. We pass in our type and from this moment on, an instance can be injected. We want to inject this in our PointsOfInterestController because we want to send the mail when a point of interest is deleted. Let's open that controller and inject it using constructor injection just as we did for the logger service. So we'll add a field to hold the instance, and set that field to the inject instance in the constructor. Then in the DELETE action, we'll call Send on this instance. That's it. Let's set a breakpoint here and give this a try. Let's send the DELETE request to a point of interest. We hit our breakpoint. Let's continue. And let's have a look at the debug output. And there we go. Our mail or at least the way we mimic it has been sent. So now we know how to create a custom service, register it, and inject it. But this doesn't really drive one of the points of using dependency injection home, and that point was that it delegates the function of selecting a concrete implementation type for the controller's dependencies. In this case, that's the mail service to an external component. Let's open ConfigureServices again. We've registered a concrete type, LocalMailService. That will make it hard to work with a testing service in a different environment. Let's improve on this. Back to our LocalMailService class. We're going to create an IMailService interface with that Send method. So let's add the new interface, and let's make sure it contains that Send method. Then we'll make sure our mail service implements that interface. So now we've got a contract, the interface, and an implementation. The idea is that we're going to let the container provide us with different implementations of IMailService. So we'll need another implementation. We've already got LocalMailService. Let's add a CloudMailService. We'll add a new class, name it CloudMailService, and have it implement IMailService. I'm going to copy over the implementation from our LocalMailService and make one change. Change LocalMailService to CloudMailService. We've got two different implementations now. Let's open the ConfigureServices method again. There's an overload we can use for AddTransient. Instead of only providing the concrete implementation, we can provide the interface, IMailService, and then the implementation. We've now essentially told the container that whenever we inject an IMailService in our code, we want it to provide us with an instance of LocalMailService. So let's change the constructor injection in our PointsOfInterestController so we request an IMailService. And that's it. But the thing is if we run this now, our result will be exactly the same as before as we've told our container to provide us with a concrete LocalMailService implementation. How can we differentiate between that one and CloudMailService? Well back to ConfigureServices. One way of doing this is by using compiler directives. A compiler directive tells the compiler to omit or include certain pieces of code on compile depending on the symbol used. If we're running a debug build, the debug symbol is automatically defined. So we can write code that states that if the debug symbol is active, we'll want the container to provide us with LocalMailService instance and, otherwise, we want it to provide us with a CloudMailService instance. We're currently on debug, so the registration for CloudMailService is grayed out. And if we change the build to release, the LocalMailService registration is grayed out. So if we build this, an instance of CloudMailService will be provided, and that makes sense. We've got a LocalMailService probably to be used during development, and that CloudMailService, well, that's probably something we want to do when rolling out, so when we provide a release build. Let's give this a try. I'm afraid we'll have to put this back to debug because, well, we're writing out statements using debug. write line. So we wouldn't see anything with a release build. But the principle should be clear. From the IDE, we could see that another piece of code would be compiled when we change the configuration. Let's try that DELETE request again. And that looks okay. Let's have a look at the debug output window, and we can see that the LocalMailService was used as we're running a debug build. But there's something else--that address we send that mail to. Well we don't want to hard code that. We want it to be in a configuration file, and it also makes sense to make this different depending on the environment we're in. If we're still working on the code, and if we're testing it, we want this address to be different than when we're in staging or production. That's what we'll cover next--working with configuration files.

Demo: Working with Configuration Files
Almost all applications require some way of keeping and reading configuration values. ASP. NET Core can work with a variety of application configuration data sources--from JSON, XML, and ini files over in-memory settings to command line arguments or environment variables. We're going to work with the most common one, a JSON file, to store settings. Let's add that JSON file first. We're going to name it appSettings. json. We'll want to store the To and From addresses used by our mail services in this file. So we'll need to add two key/value pairs. But this is a JSON file so we can work with hierarchies, and this allows us to better structure our settings. So we start with mailSettings and add the mailToAddress and mailFromAddress as children. Then we'll need to tell ASP. NET Core to read these settings from this file. Let's open the Startup class again. To work with settings in an ASP. NET Core app, we should instantiate a Configuration in our application's Startup class. That configuration, which adheres to the IConfigurationRoot interface, should then be stored as a static property so we'll have one instance, application-wide. Let's add that to the Startup class. IConfigurationRoot is part of the Microsoft. Extensions. Configuration namespace. Then we'll have that configuration and store it in our newly-added property. The best place to do that is in the Startup constructor. We haven't got that yet, so let's add one. We'll need to inject the hosting environment as we'll need that to point the framework to the root of our app. Then, let's instantiate a new ConfigurationBuilder used to create the Configuration instance we need. The first thing to do is tell it where it can find our settings file. We can do that by calling SetBasePath. SetBasePath is part of the Microsoft. Extensions. Configuration. FileExtensions package. So we'll add that. In ASP. NET Core 2, this isn't required when you've already referenced the Microsoft. AspNetCore. All metapackage. We pass in the ContentRootPath from the hosting environment. That refers to the ContentRoot of our environment. And then we'll want to add a JsonFile--our appSettings file. For that, we call AddJsonFile, passing in the name of our file. AddJsonFile is defined in the Microsoft. Extensions. Configuration. Json package, so we'll have to add that as well. And by the way, this is really showing the modularity of ASP. NET Core. There are a lot of different assemblies and packages, some containing only small pieces of functionality. And that's a good thing. It avoids having big monolithic assemblies full of functionality you don't need. And that's one of the reasons I prefer to start from an empty web project--all the other templates will already include quite a few dependencies, some of which you probably don't need. So we passed in the name--appSettings. json. But there're two more parameters we'll also look into. Optional signifies whether or not the appSettings file is optional. We'll need this one, so we'll set optional to false. Then there's reloadOnChange, which signifies exactly what the name says--should the settings file be reloaded if we change it? And, yes, this means at runtime. So let's set that to true. And then, we just need to call build, which will create a configuration, and store that in our Configuration variable. From this moment on, the configuration values from our appSettings file are accessible. But how do we access these? Let's open our LocalMailService. Here, we can access the Configuration variable from Startup. We pass in the key, and that will return the value from appSettings. We worked with a hierarchy, so we first have an object, mailSettings, followed by mailtToAddress and mailFromAddress. We separate these by a colon representing the hierarchy. And just to be complete, let's do the same in our CloudMailService. Just in case you are wondering, the casing of the keys isn't important. As you remember from the first module, in ASP. NET Core 2, the CreateDefaultBuilder call on the WebHost in the Program class already sets up these configuration files, so there's no need for this in ASP. NET Core 2. However, we do still want access to our configuration object. Let's switch to the ASP. NET Core 2 flavor for that. To gain access to our configuration files, we can inject an IConfiguration object in the Startup class. After all, it was already created via the call into CreateDefaultBuilder. In ASP. NET Core 1, the default way of handling this is to use an IConfigurationRoot-implementing object. But that's just a specialization of an IConfiguration-implementing object. It represents the root of an IConfiguration hierarchy. In ASP. NET Core 2, the default way is to use the more generic IConfiguration-implementing object. So, we inject it and set the Configuration variable, of type IConfiguration, to it. And that's it. Let's give this a try. Let's delete a point of interest. We get back the 204 No Content. So far, so good. Let's have a look at the debug output window. And here we go, the To and From addresses are visible, but this time they're coming from the appSettings file. But we're not completely there yet. Different environments might require different configuration files. The To address in a production environment likely won't be the same as the To address when developing. Let's check out how we can do that in the next demo.

Demo: Scoping Configuration to Environments
We can scope configuration files to a specific environment by providing different configuration sources, for which part of the name is the name of the environment. First, let's add an appSettings file for the production environment. And let's name it appSettings. Production. json. Production is the value of the environment variable for the hosting environment. We talked about those in the first module. And when we add this file, we see it's nicely added as a child of our already existing file. Now let's work with this. We want to send the mail to a developer address when not in production and to the admin when in production. So we open our old appSettings file and change the To address to developer@mycompany. com. Then, in the production settings file, we add one key-value pair for the To address--admin@mycompany. com. and we respect the existing hierarchy. That's it for these files. Let's open the Startup class again. Here, we're now going to add this new JSON file as a configuration source. As you remember, there's no need to do this in ASP. NET Core 2 as the call into CreateDefaultBuilder on the Webhost in the program class already takes care of this for us. The order in which configuration sources are specified is important as this establishes the precedence with which settings will be applied if they exist in multiple locations. If the same setting exists in both appSettings. json and in the appSettings. Production. json file, the last configuration source specified "wins. " So let's add that. We can specify the environment by calling into env. EnvironmentName. This time, the file can be optional, and we'll also want it to reload on change. So what we're doing here is specifying a default value for the To address, which will be overridden with the production value if we're in the production environment. Let's give that a try. Let's send a DELETE request. We're running in the development environment, so we should see the development address in the debug output window. And indeed the mail was sent to developer@mycompany. com. Now, let's change the environment. As you remember, we can do that in the project properties. Let's set it to production, and let's restart Visual Studio, which will restart the built-in webserver as well and make sure this new value is picked up. And let's send that request again, 204 No Content. Looks good. Let's have a look at the debug output window. And there we go. We now see the mail was sent to the address from our production configuration file. So, by combining working with different environments, environment-scoped configuration files, and compiler directives for registration at container level, we get a really granular approach to all of this and almost all the possible options you might need. And that's it for this module. Entity Framework Core is coming up in the next one, but let's have a look at the summary first.

Summary
In this module, We learned that ASP. NET Core has its own built-in dependency injection system. This is a specialization of the inversion of control pattern. It uses an object, the container, to initialize objects and provide the required dependencies to the object. This principle has looser coupling as an advantage, which leads to less possible required code changes and better testability. We learned how to use this by implementing logging functionality. The logger is a built-in service. And we also created a custom service. These are registered in the ConfigureServices method of the Startup class. We learned that there're three different lifetimes for services. Transient lifetime services are created each time they are requested. This lifetime works best for lightweight stateless services. Scoped lifetime services are created once per request. And singleton lifetime services are created the first time they are requested, or if you specify an instance, when ConfigureServices is run. Every subsequent request will use the same instance. After having registered the service, we can use dependency injection to inject it into the class requiring an instance of it. And that leads us to working with configuration files to store configuration info. We added an appSettings. json file, and another one for the production environment allowing us to use different configuration files for different environments. And with that, our API is finished. Well, not really finished of course. We've been working with an in-memory datastore that works directly on the DTO classes. But that's not best practice, and it's also not realistic. In the next module, we'll introduce Entity Framework Core. And we'll eventually change out the in-memory datastore for a persistent one. On to the next module.

Getting Acquainted with Entity Framework Core
Coming Up
Hi there, and welcome to the Getting Acquainted with Entity Framework Core module from the Building Your First API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. Up until now, we've worked with an in-memory datastore. That's great for demoes, but it's not something you'd use in a real-life application. For those, a persistent datastore is required. In this module, we'll add a database to our project and use Entity Framework Core to access it. We'll start out by introducing EF Core and by adding it to our project. We'll also look into migrations, a way to migrate between different versions of our underlying datastore. We'll learn how we can safely store the information needed to connect to our database, and we'll check out how we can seed the database with data from code. Let's dive in by introducing Entity Framework Core.

Introducing Entity Framework Core
Some of you might have used a version of Entity Framework in the past. Entity Framework is an ORM, or Object-Relational Mapper. Object-Relational Mapping is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. So in our code, we can work on objects rather than directly writing SQL statements in our code. Relational models and object models do not work very well together. A relational database represents data in a tabular format. And OO languages represent it as an interconnected graph of objects. It's the ORM, the library that implements the object-relational mapping technique, like Entity Framework, that takes care of this. So needless to say, that's a big plus. In very simple cases, those objects directly map to your database tables, like in our demo. We've got a city and a point of interest, and that's about it. But in larger, more complicated pieces of software, this OO technique allows us to implement OO functionality like inheritance. A base Person class and an Employee class that inherits person can map to data stored in the same table in the database. Or better granularity without inheritance--a table in a database might translate to a different set of classes in an OO model. The entities the ORM works with are different than a 1 on 1 mapping to tables. On to Entity Framework Core. EF Core is also an ORM, but it's not an upgrade to Entity Framework 6, and it shouldn't be regarded as such. Instead, Entity Framework Core is a lightweight, extensible, and cross-platform version of Entity Framework. So it comes with a bunch of new features and improvements over Entity Framework 6, and it's currently at version 2. For ASP. NET Core 1, version 1 should be used, and for ASP. NET Core 2, version 2 is advised. But not everything you might be used to from Entity Framework 6 will be supported. Entity Framework Core is recommended for new apps that don't require the heavy set of features Entity Framework 6 offers, or for apps that target. NET Core, like our application. It comes with a set of providers so you can use it with a variety of databases. We can use it with Microsoft SQL Server, of course, but also with SQLite, Postgres, SQL Server Compact Edition, MySQL, and IBM DB2. And there's also an in-memory provider for testing purposes. And, by the way, this means you'll be able to use this in universal Windows platform apps as well. In that case, you can use it together with a SQLite provider. You can use EF Core both for a code-first approach, which will create the database from our code, or a database-first approach, which is convenient if you've already got a database. We haven't got a database yet, so in our demoes, we'll take the code-first approach. Let's get to it.

Demo: Creating Entity Classes
Let's start by adding the entities to our application. These entities just like the DTOs we've used before are simple classes. But as we learned, the DTOs, the outer-facing model, are different from the entities, the Entity model we'll work on. Not all fields like computed fields are stored in a database. And the data we want to offer to an API is often shaped differently than how it's stored in the underlying datastore. So it's very important to make this distinction. I like to add these to their own namespace, so let's add a folder, Entities. We've got two entities we want to add--City and PointOfInterest. Let's start with City. Let's add a new class to the Entities folder and name it City. A City has an ID, a name, and a description. And it has a list of children, a list of PointOfInterest. We still have to add this class. Just as before, it's a good idea to initialize this to an empty list to avoid null reference exception problems when you're trying to manipulate that list when the points of interest haven't been loaded yet. We omit the calculated field we had in our CityDto. Let's check that CityDto. This one has a calculated field, NumberOfPointsOfInterest. We don't want to persist that in our database, so that's why we omit it. By giving the class an ID with name Id, this field is automatically regarded as the primary key. CityId, i. e., the class name, followed by ID would be possible as well. If we would name this differently, the convention doesn't apply. But we can apply the key data annotation from System. ComponentModel. DataAnnotations. Personally, I like to apply the key annotation anyway, even if convention would ensure this property would be regarded as primary key, I feel it makes Entity classes so much more understandable at first glance. But, well, I've got the same gripe with a lot of convention-based approaches, so this is totally up to you. Another thing of importance is generation of ID primary keys. By convention, primary keys that are of integer or GUID data type will be set up to have their values generated on add. In other words, our ID will be an identity column. To explicitly state this, we can use another annotation, the database-generated annotation from the System. ComponentModel. DataAnnotations. SchemaNamespace. It has three possible values--null for no generation, identity for generation on add, and computed for generation on add or update. We need the identity option. A new key will be generated when a city is added. How this value is generated depends on the database provider being used. Database providers may automatically set up value generation for some property types, while others will require you to manually set up how the value is generated. In our case, we'll be using SQL Server. So we're good to go. A new integer primary key will be automatically generated without further setup. So now we'll also add the point of interest entity. Let's add a new class for that and name it PointOfInterest. The PointOfInterestDto also has a description, but let's refrain from adding that for now as that allows us to tackle migrations later on in this module. Here as well I like to apply both key and database generated attributes. But there's more. We want to signify the relation between City and PointOfInterest. If we look back at the City entity, we already defined a collection of PointsOfInterest, but we want to navigate through our object graph from a point of interest to the parent city. So we need a property to refer to that parent city. And we need to state what the foreign key property will be. Again, there's a convention-based and an explicit approach possible. By convention, a relationship will be created when there is a navigation property discovered on a type. And a property is considered a navigation property if the type it points to cannot be mapped as a scalar type by the current database provider. So if we add a property City of type City, this is considered the navigation property, and a relationship will be created. Relationships that are discovered by convention will always target the primary key of the principle entity. And in this case, that's the ID of the city. That will be our foreign key. It's not required to explicitly define this foreign key property on the dependent class. And the dependent class, well, that's our PointOfInterest class. But it is recommended, so we'll add one. So that's the convention-based approach. If we do not want the convention-based approach to be followed, which states that a foreign key will be named according to the navigation property's class name followed by id, so CityId in our case, we can again use an annotation for that. The foreign key annotation from the System. ComponentModel. DataAnnotations. SchemaNamespace. Here we state that for the navigation property City, the foreign key on PointOfInterest is named CityId. Again, this isn't strictly necessary as the convention-based approach is sufficient in our case. So the differences between entity classes and our DTOs are becoming clearer and clearer. DTOs typically don't contain these properties and annotations. But there's more. We've applied annotations to our DTOs that we use for Create and Update, like required for name and maxLength. If we leave our entity classes like this, our database columns will allow null for fields that should not be null. And we'll be off max and varchar length instead of a specific maximum size. It's best practice to ensure these field restrictions are applied at the lowest possible level. So in our case, that's the database itself. This ensures the best possible integrity. So let's apply these attributes. For PointOfInterest, the Name was required with a maxLength of 50. And for the City entity, we want these as well. Let's make Name required with a maximum length of 50. And let's give the description a maximum length of 200. We didn't have these attributes on our CityDto, and we don't need them because as you remember, the CityDto is only used to get data, not to store it. So we've got our entities, and we see that they are definitely different from the DTOs we've been working with both on a level of missing properties like the calculated number of points of interest for a city, which we see here in the CityDto, but also as far as additional properties are concerned like the navigation property from point of interest to its parent, City. If we look at the PointOfInterestDto, there's no property like that. But the PointOfInterest, well, that does have this navigation property. Now we need to have some sort of class from which we can access these to get from the database and persist to the database. And that's what we'll tackle in the next demo.

Demo: Creating a DbContext
In this demo, we'll create a context to interact with our database. And that context represents a session with the database, and it can be used to query and save instances of our entities. Our entity classes are just classes. We didn't need any extra dependencies to create those. But the DBContext, that's part of Entity Framework Core. And we'll also need a provider. In our case, we'll use the SQL Server provider so we can connect to a LocalDB instance. Let's open the NuGet dialog. We want to look for the Microsoft. EntityFrameworkCore. SqlServer package. If we install that, the entity framework core dependencies will be added as well, so we'll have all we need for now. And there we go. As you can guess by now, no need to do this when you're on ASP. NET Core 2 and you've referenced the Microsoft. AspNetCore. All package. That includes the necessary references for Entity Framework Core. Now let's add a new class, CityInfoContext, and have it inherit DBContext. DBContext can be found in the Microsoft. EntityFrameworkCore namespace. Bigger applications often use multiple contexts. For example, were we to add some sort of reporting module to our application, that would fit in a separate context. There's no need for all the entities that map to tables in a database to be in the same context. Multiple contexts can work on the same database. In our case, well, we only have two entities, so one context is sufficient. On this context we now want to define DbSets for our entities. Such a DbSet can be used to query and save instances of its entity type. LINQ queries against a DbSet will be translated into queries against the database. Our application will use this context, so we need to register it so it's available for dependency injection. And we already know to do that as we've encountered dependency injection before. Let's open the Startup class. In ConfigureServices, we can now register this DBContext. When we installed the NuGet package, the Microsoft. EntityFrameworkCore assembly was added as a dependency. And that gives us access to the AddDBContext extension method. So let's use this to register the DBContext. And by default, it will be registered with a scoped lifetime. But how do we connect to a database? How do we tell the context where it can find it? Well, that's through a connection string. We need to provide this connection string to our DBContext. In other words, we need to configure this DBContext. And there's essentially two ways of doing this. Let's open our CityInfoContext again. The first way of doing this is through overriding the OnConfigure method on the DBContext. This has an optionsBuilder as parameter. And that optionsBuilder provides us with a method--UseSqlServer. This tells the DBContext it's being used to connect to a SqlServer database, and it's here that we can provide a connection string. So that's one way. But let's look at the other way--via the constructor. So let's comment this out, and have a look at the DBContext definition. The DBContext exposes a constructor that accepts DBContext options. So let's add a constructor that calls this constructor overload. What this allows us to do, and what isn't possible when overriding the OnConfigure method, is that we can provide options at the moment we register our DBContext. And that's a more logical approach. It's more in line with how we work with other services. So let's go back to the startup class. There's a method overload on these options we can use now--UseSqlServer. It's from the Microsoft. EntityFrameworkCore namespace, so let's add that using statement. And in this method, we can pass in the connection string. So let's add a variable to hold this connection string for now. The next logical question is, What would that connection string look like? Well, we're going to be using local DB, as this is installed automatically together with Visual Studio. But if you have a full SQL Server installation in your network, it'll work as well. Just make sure you change the connection string accordingly. And let's have a look; (localdb)\MSSQLLocalDB is the default instance name, but it can be different on your machine depending on whatever you inputted on install. So if you're not sure, have a look at the SQL Server Object Explorer window. If you don't see that on your machine, you can find it underneath the View menu item. Here I can see that on my machine, MSSQLLocalDB is the instance name. Next to the server name, the connection string also contains the database name. I named it CityInfoDB. The last part of the connection string states that this is a trusted connection, i. e. integrated authentication is used, which on a Windows machine translates to our Windows credentials. So that explains that. But we still haven't got a database. This is a code-first approach for a new database, so the database should be generated if it doesn't exist yet. Let's open our CityInfoContext again to make sure that happens. To the constructor that will be used when requesting an instance from the container through dependency injection, we call EnsureCreated() on the database object. This database is object is defined on DBContext. If the database already exists, nothing will happen, but if it doesn't, this call ensures it is effectively created. One more thing left. Just building and running this won't trigger creation of our database, because just registering the context on our container doesn't yet create an instance of it. So this constructor isn't called either. In the next module, we'll replace the in memory store with this database, but we've got a few other topics to tackle in this module before we can get to that. Yet it would be nice if we could already see a database, so let's add a DummyController we can use to test this. And let's aptly name it dummy controller. Let's have it inherit controller, and let's make sure it has access to the CityInfoContext. We know how to do that by now, through constructor injection. At the moment that the DummyController is instantiated, an instance of CityInfoContext is created as well, so that's when the database will be created. Let's just add an action to our controller that we can call to ensure that the controller is instantiated. And that should it. Let's give this a try. Let's send a request to api/testdatabase. We get back a 200 OK. It seems like everything went as expected. Let's open that SQL Object Explorer window again. Let's refresh the database list from our MSSQLLocalDB instance. And it looks like our CityInfoDB data is there. Let's have a look at the tables. Apparently two tables have been created, a Cities and a PointsOfInterest table, the pluralized names of our entities. The Cities table has a primary key, Id, a description with a maximum length of 200, and a name with a maxLength of 50, which cannot be null. If you look at the City entity, we see that the column definition matches the definition of the fields on our City entity. Let's have a look at the PointsOfInterest table. It has a CityId, which is a foreign key, and it has a name field, which is required, thus cannot be null, and a maximum length of 50. So that matches our PointOfInterest entity. The attributes we applied to the properties on our entity classes were, thus, taken into account. So far, so good. But this is only one way of doing this. If we work like this, we work by ensuring the database is created by calling Database. EnsureCreated(). But if we do that, well, we're forgetting something. Just as code evolves, a database evolves as well. Let's look into migrations to see how we can improve on what we've done up until now and how we can handle an evolving database.

Demo: Working with Migrations
Just as our code evolves, so does the database. New tables might be added after a while, existing tables might be dropped or altered. Migrations allow us to provide code to change the database from one version to another. They're an important part of almost all applications, so let's look into it. What we are going to do, we are going to use migrations to create the initial database version, version 1. So we'll replace what we did in the previous demo by this new and better approach. The reason is that by doing that, we'll have code in place to start from no database at all, rather than having to provide an already existing one. Then, we'll add another migration to migrate to a new version, version 2. To allow for something like this, we'll first need to create an initial snapshot of our database. In the entity framework core world, this is achieved with tooling, so we'll have to add these tools first. And these tools are essentially just another set of dependencies that add commands we can execute. Let's add the package Microsoft. EntityFrameworkCore. Tools. And with that, we've got the dependency containing the tools. Now we have to ensure we can actually use the commands contained in this dependency. For that, we open project. json, and we look for the tools section. We see the IIS integration tools have already been added, that's what we're using for our hosting environment, but we have to add something to this, the Microsoft. EntityFrameworkCore. Tools dependency. We pass in the name of the dependency, and we pass in the version. And that's all we have to do to ensure we can use the tooling. Just in case you're wondering, but I'm pretty sure you know this by now, none of this is necessary when you're using ASP. NET Core 2 in Visual Studio 2017 with the Microsoft. AspNetCore. All metapackage. If you're using ASP. NET Core 1 in Visual Studio 2017, you do need to add the references, but there's no need to edit project. json. That doesn't exist anymore in that environment, just adding the references is sufficient. So then we'll have to create that initial snapshot, or migration of our database and schema. For that, we have to be able to execute one of the commands we just enabled. And executing those commands, well, you can do that in the package manager console. If you don't currently see that, you can get it via Tools, NuGet Package Manager, Package Manager Console. The command we're looking for is the Add-Migration command. It expects a name for the migration we're going to add. So let's say we want to name it CityInfoDBInitialMigration. If we look at our solution now, we see there's a new Migrations folder. And it contains two files. One, a snapshot of our current context model. Let's have a look at that. This contains the current model as we defined through our entities, including the annotations we provided. We can find our City entity and our PointOfInterest entity. And at the end of the file, the relation between City and PointOfInterest. The second file we see is the CityInfoDBInitialMigration. That's the name we just gave to our migration. This contains the code needed by the migration builder to build this version of the database, both Up (from current to new version) and Down (from this version to the previous version). If we look at Up, we see two CreateTable statements and a CreateIndex statement. That means it's starting from no database at all, and this migration contains the code to build the initial database. And if we look at Down, we see what should happen to end up with an empty database--two DropTable statements. If new migrations are added, new files like this will be created, and by executing them in order our database can evolve together with our code. By the way, you don't need to run the Add-Migration command to generate these files. We could've written them by hand. And that might still be feasible for one or two or three tables maybe. But it's definitely not something you want to do for a larger database. So these tools are quite helpful. So far, so good. There's one more thing we have to do. We have to ensure that the migration is effectively applied to our database. And there's another command for that. It's called the update-database command. If we execute this, the migrations will be applied to our current database. But we can also do this from code. Let's open the context again. What we can do is replace Database. EnsureCreated() by Database. Migrate(). This will execute migrations, which, if there's no database yet, will create it. And that's really all we have to do. But as said, we're replacing what we did in the previous clip because, well, most applications do require migrations. And for those, it's a good idea to start from no database at all if you have the chance. So what we want to do is remove the current database first. If we don't do that, this call will try and apply the migrations, i. e., create the Cities and PointsOfInterest tables, and that will fail because they already exist. And there we go. The previous database was deleted. If you do want to provide an existing database, you can follow the same flow we just did, but delete the first migration file. Generally speaking, though, that's not a good place to be unless your application must start from an existing database. Let's give this a try. Let's send a request to that Test database endpoint again. The request was successful. Let's have a look at our localDB server. Let's refresh this list. And there we go. Our database was created again, but by working like this instead of how we did it previously, we've ensured our database can migrate from not existing at all to its initial version and upcoming versions after that. A better approach than what we did in the previous demo. Let's have a look at the database itself. It now contains an additional table--_EFMigrationsHistory. Let's have a look at what's in there. Entity Framework Core uses this table in the database to keep track of which migrations have already been applied to the database. This ensures that that Database. Migrate() call, or alternatively the Update-Database call from the command line, doesn't try to execute the same migrations over and over again. And let's continue with adding a new migration. A point of interest doesn't seem to have a description. We may have missed that on purpose because this allows us to look into an additional migration. So let's add that description with a maximum length of 200. That's that. Then, let's execute the Add-Migration command again so the file gets generated for us. Let's name this migration CityInfoDBAddPOIDescription. Our Migrations folder now includes a new file. And if looking at this file, we see that the Up method contains the code to add the description column, and the Down method contains the code to drop the column again. Let's run this again, and execute that same request. We get back a 200 OK. The request was successful. Let's have a quick look at the database. PointOfInterest now indeed contains a description column, maxLength 200, and not required, and let's have a look at that EFMigrationHistory table. And, indeed, it also contains the new migration. And that's how we can work with migrations to migrate our database from one version to another. But if we look at the data that's in these tables, we see there's nothing there yet. No cities, no points of interest. To add data to start with, we should seed the database. And there's another thing. Currently, we hard-code our connection string, and that's not really a best practice. We'll continue with these two topics, starting with safely storing that connection string.

Demo: Safely Storing Sensitive Configuration Data
In this demo, we'll look into how we can safely store sensitive configuration data. As an example, we'll use the connection string. It might be tempting to say, Well, this is configuration data, and we've got an app settings file, so that's where we'll want to store the connection string. But that could be a bad idea. As long as you're working on your local machine in a development environment on your LocalDB instance, you'll be okay. But once you're deploying to production, you'll be providing a connection string to an actual SQL Server instance. We're currently working with integrated security. That's that "trusted connection" part of the connection string. But sometimes a connection string contains a username/password combination, i. e., it uses SQL server authentication. Just imagine accidentally submitting it to a source control system like Git or Team Foundation Server. We do not want to expose that kind of information, not username/password combos, but also not the name of our actual production database server. So here's what we're going to do. First of all, we'll get rid of the hard-coded connection string by adding it to the appSettings file. That's what we'll use in a development environment. But for the production environment, we'll use an environment variable to store this data. Let's start by adding the connection string to the appSettings file used during development. That should take care of that. We can then get the value via the configuration object, as we learned in the previous module. Let's open the ConfigureServices method of the Startup class. And let's replace the hard-coded connection string. And that already takes care of that part. The hard-coded connection string is gone during development. Let's give that a try. I'll put a breakpoint right after the connection string is fetched so we can see this is actually works. And let's make sure we're in the development environment, which we can check in the project properties. That's still in production from a previous demo, so let's change it to development. And as we remember, to make sure these values get picked up, we should quickly restart Visual Studio. We've hit the breakpoint. And if we look at the providers, we see there're two providers. The first one is appsettings. json, a configuration source, and the second one, that's appsettings. development. json. That file doesn't exist, so the values from appsettings. json are used in the development environment. Appsettings. json comes from this line of code, and it's also looking for appsettings. development. json as it looks for JSON files with the name containing the environment name, in this case development. Appsettings. production. json wasn't loaded because we're not in the production environment. Thus, the connection string will come from appsettings. json. And if we look at that connection string, that's indeed the one we entered in appsettings. json. So far, so good. Then onto the production environment. This is where we'll want to use environment variables. And we've even encountered environment variables before. If you look at the project settings, we see there's the ASPNETCORE_ENVIRONMENT variable currently still set to development. Well we can also tell the configuration object to gain access to these variables as an additional configuration course so it can read them out. We can do that by adding the AddEnvironmentVariables extension method to the configuration chain. As we remember from the first module, this is not needed in ASP. NET Core 2 as it's already handled by the WebHost. CreateDefaultBuilder call in the Program class. Then we'll have to define the connection string for the production environment as an environment variable. Let's have a look at those project properties again because here we can add environment variables. First, let's change the current environment to production. And now let's add a new one. We're going to give this the same key as the connection string key in our appSettings file. And as value, we'll give it a fake production connection string-- Server-myproductionserver with SQL authentication, so through user ID and password. And let's save this. Let's restart Visual Studio again. Visual Studio is restarted, so the built-in web server has been restarted as well. Our breakpoint is still there. So now let's give this another try. So we've told our application to look into the environment variables for configuration information. So what will happen is that if the environment variable with the connectionStrings:cityInfoDBConnectionString key is found in a certain environment--production, in our case--it will override the value from other configuration sources as AddEnvironmentVariables was the last configuration source added to the chain. So let's have a look at those providers again. This time there're three. The first one is appsettings. json. It still contains the LocalDB connection string. The second provider, well, that's the appsettings. Production. json file. So, we're definitely in the production environment. That only contains one setting--the mailaddress from the previous module. And, lastly, we have the environment variables. This contains all the environment variables that have been loaded. And if we have a look at position 48 here, well, here's our fake production connection string. Let's have a look at which connection string was actually loaded. And that's indeed our fake production connection string. So there we go. The connection string is now coming from the environment variables, and those are not submitted to source control. But we're not there yet. This works great from Visual Studio, but what happens is that those settings we set in the project properties, the environment variables, are actually added to a launchSettings. json file, and this is not something you deploy to a production server nor check into source control, by the way. So if the environment variables are just set in a launchSettings. json file, how do we access this in a production environment? Well we've just seen that there're a lot of environment variables loaded when we looked at the data in the environment variable provider. And these are not coming from our launchSettings file alone. They've been defined at system level. So let's look at a realistic option. First, let's remove this environment variable from the launchSettings file directly or, alternatively, to the project properties as we've done before. And there we go. No more sensitive information in JSON files. Let's assume for a moment that my machine is the production server. On each machine, a set of environment variables is defined. If you search for "edit the system environment variables, " you'll get to this window. In this window in the lower right corner, we see Environment Variables. The top list is tied to my user account, and the bottom list is tied to the system. This is where that list of variables that we didn't define in our project properties or launchsettings. json is coming from. And this is also where on a production server, you'll want to add an environment variable. So let's add a new one for our connection string. The variable name is the key, and the variable value, well, that's our fake production server connection string. Let's restart Visual Studio again. This is just to make sure that our other environment variables are now gone. And let's run this again. We've hit our breakpoint. And as we can see, connection string contains the fake production server connection server connection string. But this time coming from the list of system environment variables where we just added the connection string. So that's what we wanted to do. Important to know is that as environment variables in our case override all the other configuration sources, this connection string will be used regardless of the environment we choose in Visual Studio. So the general rule is use appSettings for non-sensitive data. Use the environment variables you set on your project properties only on your local machine and never submit them to source control, to the launchSettings file. And once you roll out to the production server, set the environment variables as we just did. I'll remove it again for now so we can still work with our development connection string, and I'll restart Visual Studio again right before the next demo. But, truth be told, there's still a possible risk, and, well, isn't there always? Environment variables are typically unencrypted key/value pairs, so if someone gains access to the machine or process, he or she can read out that data. But the approach we took here is already safer than using plain text files for production secrets, like, by the way, would be the case with the old ASP. NET's default approach with web. config files. And with that, we can continue with seeding the database with some data.

Demo: Seeding the Database with Data
We still haven't got data in our database. It would be nice to have some to test with. That principle, providing your database with data to start with, is called seeding the database. It's often used to provide master data. In this demo, we'll look into that, in our case by providing some sample data. But unlike in Entity Framework 7, there currently isn't a hook to easily provide seed data with Entity Framework Core. At the moment of recording, there's an issue logged at GitHub with an ongoing discussion of how to implement this. But the issue is currently in the backlog, so we don't really have a release date to look forward to. We'll have to look for another solution. In essence in our case, all we want to do is ensure that a set of insert statements are executed in case some tables don't contain data. And we want to ensure that this is done at Startup. There are various ways of doing that, but as it's in the backlog to be implemented as part of the framework, we're going to go for the currently advised solution by the EF Core team judging from the comments on this issue. We'll write an extension method on our context. And we'll ensure that that extension method is called when configuring the HTTP request pipeline in the Startup class. So let's start with that extension method. Let's add a new class, CityInfoContextExtensions. Let's make it static, and let's add one static method to it, EnsureSeedDataForContext. The method has one parameter of type CityInfoContext named context, and it's decorated with this, which tells the compiler it extends CityInfoContext. The first thing we want to do is check if the database already contains our sample data. We want to insert cities and their points of interest. So let's check if the City table is empty. A point of interest can't exist without a city, so that's sufficient. If it's not empty, we already have data in there, and we don't want to insert additional data. And, otherwise, we can start adding data. We first create a list of City, and for each city, provide the points of interest. Let me paste that in. This is much like what we did with the in-memory datastore. But this time, we do not provide IDs as these are now auto-generated by the database. Then we'll want to add these to the context. For that we can use AddRange method on the Cities DBSet on our context. And from this moment on, the entities are tracked by the context. But they aren't inserted yet. For that, we must call SaveChanges on the context. Calling SaveChanges on the context will effectively execute the statements on our database. And that's already it for the extension method. Then we need to execute this extension method. So let's open the Configure method of the Startup class. What we want here is an instance of the context. So we'll accept a new parameter, so the dependency injection container can provide us with that. And on the cityInfoContext, we can then call the EnsureSeedDataForContext extension method. Let's give that a try. All we have to do is start up the application. The application has started, so the EnsureSeedDataForContext method has been executed as it's called in the Configure method. Let's have a look at our database. And the Cities table contains sample data, and so does the PointsOfInterest table. And with that, we now know what Entity Framework Core is, its most important concepts, and how to use those. It looks like it's time for the final module, in which we're going to use what we learned and code it in this module to replace the in-memory datastore with a database through Entity Framework Core. But, first, time for the module summary.

Summary
In this module, we introduced Entity Framework Core. That's an ORM, an object-relational mapper. Object-relational mapping is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. Entity Framework Core is a lightweight, extensible, and cross-platform version of Entity Framework. It's recommended for new applications that don't need the full Entity Framework 6 feature set and for. NET Core applications. We created entity classes first. As we learned, the DTOs, the outer-facing model, are different from the entities, the entity model. Not all fields like computed fields are stored in a database. And the data we want to offer through an API is often shaped differently than how it's stored in the underlying datastore. So it's important to make that distinction. We can use annotations on those to define things like primary and foreign keys, required fields, and so on. Those are then registered as DBSets on the DBContext. That context represents a session with the database. And it can be used to query and save instances of our entities. From that moment on, we could access our entities through LINQ. There was another important concept we looked into--migrations. Just as our code evolves, so does the database. New tables might be added. After a while, existing tables might be dropped or altered. Migrations allow us to provide code to change the database from one version to another. And then we got rid of our hard-coded connection string. We learned how to store it in the appSettings file for the development environment but used a safer storage option for the production environment, an environment variable. And, lastly, we looked into an option to seed the database providing it with data to start with. There's currently no built-in hook to easily provide seed data with Entity Framework Core, but creating an extension method that fills the database with data and calling it from the Configure method on the Startup class was how we implemented our own alternative version. And that's all for this module. Time for the last one in which we'll tie everything we've learned up until now nicely together. We'll switch out the in-memory datastore and replace it with our persistent database.

Using Entity Framework Core in Our Controllers
Coming Up
Hi there and welcome to the Using Entity Framework Core in our Controllers module from the Building Your First API with ASP. NET Core course at Pluralsight. My name is Kevin and just like with the last five modules, I'll also guide you through this one. This is the last module. The one in which the pieces are tied together. We've got all our API actions and we introduced Entity Framework Core in the previous module. In this module we'll tie these together. We'll start by looking into the repository pattern, through which we'll access our database via Entity Framework Core, and then we'll replace the codes that uses our current in-memory data store with code that will talk to a persistence store to database. We'll also introduce something new, AutoMapper. As you remember, DTOs and entity classes serve different purposes and we need a way to map between these. That's where AutoMapper comes into play. After this module we'll have a fully functional API that connects to a database for our Entity Framework Core. Let's start by introducing the repository pattern.

Introducing the Repository Pattern
Let's talk about repository pattern. We could just access the DB context from our controllers directly. But we can easily run into problems like that. Duplication of code is quite common. You might need to access cities, or update points of interest from multiple parts of your application. Then you'd want that code to be written once, rather than in every action or part of the app. And that then leads to another possible issue. Your code will become more error prone if only for the duplication. And then there's testing. If you want to test the controller actions but those controller actions also contain logic related to persistence, it's harder to pinpoint why something might go wrong. Is it the logic in the action, or is it the persistence-related code in the action that fails? If there was a way to mock the persistence-related code and test again, you know that the mistake isn't related to that persistence logic. Well, this is where the repository pattern comes in. It's an abstraction that reduces complexity and aims to make the code safe for the repository implementation, persistence ignorant. So we achieve no duplication, less error-prone code, and better testability of the consuming class. The term persistence ignorant from the definition we just saw might require some additional clarification. Sometimes it's said that through a repository you can switch out the persistence technology when needed. And while that is, strictly speaking, true, that's not really the purpose of the repository pattern. What it is very useful for is allowing us to choose which persistence technology to use for the specific method on the repository. Creating a city might be easier through Entity Framework. Getting a city with some complex logic might be more advisable through ADO. NET. You might even call in to an external service. In short, for the consumers of the repository, it's of no interest what goes on in the implementation. Rather than switching out one persistence technology for another for the complete repository, it allows us to choose the technology that's the best fit for a specific method on the repository. Thus, persistence ignorant. Let's see how we can implement such a pattern.

Demo: Introducing the Repository Pattern
In this demo we'll introduce the repository pattern. Let's start by creating a new interface and add it to the services folder. We'll name it ICityInfoRepository. This is the contract that our repository implementation will have to adhere to. We're going to start by adding methods to get data, as those are the first we'll switch out in the upcoming demos. We'll keep on building from that. It's important to only include the methods we'll actually use in our repository contract. So we'll need a method to get the cities. But what do we want to return here? There's two options I often see. Either return an IEnumerable of City, or IQueryable of City. And this is a decision on which a lot of discussion exists. If we return an IQueryable, the consumer of the repository can keep on building on that IQueryable. For example, he can add an OrderBy clause or Where clause, etcetera, possibly before the query is executed. But in a way, that also means that you're leaking persistence-related logic out of the repository, which seems to violate the purpose of the repository pattern. On the other hand, if you're building an API that allows a huge set of data-shaping possibilities, all requiring different queries writing tens to hundreds of methods in a repository becomes cumbersome, if not almost impossible. Anyway, both approaches exist and typically you'll hear the proponent of the one approach say the other one is bad and the other way around, as is, of course, often the case with pattern implementations. We're not going to get into that discussion here. We've got a fairly straight-forward API, so we'll return an IEnumerable. We'll also need to be able to get one city. So we'll need another method that accepts the cityID and returns a city. Then, we'll need a method to get the points of interest for a city. So it accepts a cityID and returns an IEnumerable of point of interest. And lastly, we'll need one to get one point of interest for a city so that method accepts a cityId and a PointOfInterestId. It returns a point of interest. That's it for our contract for now. Then we'll need to implement it. So let's add a new class, city info repository and have it implement this ICity info repository interface. This is the place where we provide persistence logic. We might need the city info context to do that and only that, but it might as well be a mix. For some methods we could use Entity Framework. For others, we could call a service and so on. The importance lies in the fact that for the consumer the implementation is of no interest what so ever. The consumer is ignorant of how the ICity info repository contract is implemented. In our case, we'll just need the city info context. So, through constructor injection, we ensure that we have an instance of that. Then let's provide the implementation. This is also the place where we can provide additional persistence related logic, like ordering the cities. Let's do that so the cities are returned by name. So we call into the context and the cities list and then we call order by to order the cities list by, let's say, the name. After that we add to list and this is very important because this ensures that the query is executed at that specific time. Calling to list means iteration has to happen, and for that to happen the query must be executed on our database. So that takes care of this method. Let's look into the next one. As you remember, when getting a city we included the points of interest for that city by default all the time. The memory data stored automatically did that. But now we can choose to include them or not. So, what we're going to do is not include them when returning a list of cities, but allow the consumer of the API to choose whether or not these must be included when fetching a single city. So we'll need an additional parameter in the GetCity method. A boolean include points of interest. And we also have to add that to the interface. In this method we can then check the include PointsofInterest boolean, and include PointsofInterest if it's true. For that we use the include extension method. Pass in the collection we want to include. They include extension method is defined in the Microsoft. Entity Framework Core namespace, so let's add the using statement for that. And then we pass in the collection we want to include, PointsOfInterest. To filter on a specific city, we use the where extension methods. We want to check if the ID of the city matches the requested city ID. And lastly, we add first or default, which will effectively execute the query. If the PointsOfInterest shouldn't be included, we return the city with a matching city ID. Likewise we implement the other methods. In GetPointOfInterestForCity, we return a specific point of interest, being that one for which the city ID matches the provided city ID and point of interest ID matches the provided point of interest ID. To get the points of interest for a city, we return all the points of interest where the city ID matches the provided city ID. That should be it for our repository implementation. At least for now, then, we'll need to register the repository, we know how to do that. On to the configure services method of the startup class. When we learned about dependency injection, we learned that there were three lifetimes we could register to service with. Transient, for services that must be created, each time they are requested. Scoped, for services that are created once per request, and Singleton, for services that are created first time they are requested. For a depository, the best fit is a Scoped lifetime, so it's created once per request. So let's AddScoped method, pass in our contract, and pass in the implementation. If we would want to use a mock repository, we'd have to provide another implementation of the ICityRepository interface, and then we could register it here instead of the city info repository. And with that, we created our first repository. Now, what about doing something with this? In the next demo, we'll replace getting data from our in memory data store with getting data from our database through this repository. And what's coming is probably one of the best illustrations of the power of the repository pattern. We'll have to change quite a bit of code to switch out the in memory store for our database. If we had used the repository from the beginning, we wouldn't even have to touch our controller classes. So as far as gradual improvement goes, well, this is quite a big one. Let's continue with the next demo.

Demo: Returning Data from the Repository When Requesting Resources
In this demo, we'll start replacing the calls to the in memory data store with calls to the repository. Let's open the city's controller and inject the repository. As we know by now, we can use constructor injection for that. Then, let's have a look at returning all the cities. We can now call the GetCities method on our repository which will provide us with a list of city entities. These city entities are what our repository and context work on. However, they are not what the actions in our API should return. Those were kind of different set of modules, the DTOs. So, we'll need to map this list to a list of city DTO. But let's have a look at that city DTO again. It contains a list of points of interest and a count of those. We just chose not to include the points of interest this time. If we do use this DTO, we would end up with an empty array of points of interest and a count of zero in our results for the number of points of interest fields. That's not what we want, so let's add a new DTO, city without points of interest DTO. This one won't include those properties. All we need is an ID, a name, and a description. So once again, a good reason to make the distinction between the entity model and the DTOs. Were we to return entity classes, would see an empty array. Back to our controller, we add a result list named results, which is a list of city without points of interest DTO and we run through the city entities. From each of those we create a new city without points of interest DTO object. The GetCities method, does not return points of interest so we don't have to map those. And lastly, we return the result list. Okay, let's already give this one a try. Let's send a get request to API/cities. And there we go, our list of cities without points of interest coming from our database. And they're nicely ordered by name, as that's how we ordered them in the repository. On to the next action, returning a single city. This was a special one because we're going to allow the consumer of the API to choose whether or not to include the points of interest. We haven't encountered something like this before, passing additional parameters to an API call. We do that by accepting a parameter in the actions signature. Let's name it, includePointsOfInterest and give it a default value of false. Then, we call into the repository to get back the city with or without points of interest. The first thing to check, did we actually get something back. If we didn't, we return a Not Found. And the rest follows the same principles as before. We have to map this result, either to a city DTO or a city without points of interest DTO. So we check the value of the IncludePointsOfInterest parameter, and if PointsOfInterest had to be included, we map to a city DTO. We run through the points of interest and map each point of interest to a point of interest DTO. And then, we return the results. That takes care of the case where the points of interest had to be included. If they don't have to be included, which is the default, as false is the default value for the IncludePointsOfInterest parameter, we map to a city without points of interest DTO. After that, we return that DTO. And that's it for this method. Let's give that a try. So, let's send a GetRequest to get the city with ID one. So far, so good, we get back New York City without the points of interest. These points of interest aren't included, as the default value of the IncludePointsOfInterest parameter is false. Just to check, let's try getting a city that doesn't exist, and we get back a 404 Not Found. All is well. Now we want to include the points of interest. Passing a parameter like this to the action is done by using query strings, so we append a question mark to the URI, followed by an equal sign, and the value. Let's give that a try. And that looks good. We got back New York City including the points of interest. By the way, if you want, you can provide additional parameters through the query string by separating them with an ampersand. For example, an order parameter with value name could be passed in like this. Okay, we've got two actions left. Getting the points of interest for the city, and getting one specific point of interest. Back to our code. It's almost the same as what we did but it's different enough to look into them nevertheless. First, let's inject repository. Then, we'll want to get the points of interest for a specific city, so let's call into the repository to get them, and this is where we have to provide something extra. We need a way to check if the city exists, and this code doesn't tell us that. Let's look at the repository again. The GetPointsOfInterestForCity method, is going to return an empty collection if the city doesn't exist. But, it will also an empty collection if the city exists, but there are no points of interest. The former one is a 404 Not Found response, while the latter requires us to return an empty list. Now we do have a method in the repository that returns null if a city doesn't exist, GetCity, and we could definitely use that, but it'll return a city object we don't really need or know. So, I like to take a slightly cleaner approach and add an additional method to the repository that returns true if a city exists, and false otherwise. Let's name if CityExists, accepting a city ID. That any statement will return through if a city with this ID is found and false otherwise. And let's add a signature to the contract as well. And then we can go back to our controller. We can now first call the CityExists method and we can return Not Found if we get a value of false back and as we've been logging these issues before, we'll keep on doing that. So let's comment out that old code, we don't need that any more. The rest of the code follows the same principles we already learned about. We map the returned list of points of interest to DTOs and we return that list of DTOs. Before we test, let's immediately add the implementation for the action to return a single point of interest as well. Just one additional check needed, we have to return Not Found if the point of interest isn't found. So first we check if the city exists, and return Not Found if it doesn't. Then, we try and get the point of interest. If we get null back, we return Not Found, otherwise we map to a DTO and return that DTO. Okay, let's try these last two actions out. If we try and get the points of interest for New York City, we get back the points of interest in our database for New York City, Central Park, and the Empire State Building. If we try a non-existing city, the one with ID four in this case, we should get back a Not Found. That's indeed the case, and that's thanks to our check to see if the city exists. You can also try and get a specific point of interest. This time we get back Central Park. If we try and send the Get Request to get a specific point of interest, for a city with an ID that doesn't exist we should get a Not Found, and that's indeed the case, and if we try and get a point of interest with an ID that doesn't exist, for a city that does exist, we should get a Not Found as well. And that seems to work as well. And with that, we've introduced repository in our controllers, and we're now fetching data from our database through it. So, we could continue with manipulating data instead of just fetching it, but I'm not really happy with what we have now. Let's just have a quick look at our cities controller again. There's an awful lot of mapping code in here, and that mapping code, that's definitely error prone. Just imagine this for an object with 20 properties of which five are collections on types that again have tens of properties, and once we start manipulating objects, we'll have even more mapping code from the DTO that's inputted to the entity, and then from the entity back to the DTO. Luckily, there's a solution for this. A proven and a tested mapping tool, and I guess some of you will have heard of it already, AutoMapper. Let's check that out.

Demo: Using AutoMapper to Map Between Entities and DTOs
Before we continue with manipulating resources, we'll introduce AutoMapper into our solution. In this demo, we're going to use it to get rid of all the manual mapping code we had to write up until now. AutoMapper can be found on NuGet, so let's open the NuGet dialog and look for it. Let's have a look at a part of AutoMapper's description. It states that, generally, AutoMapper is designed for model projection scenarios to flatten complex object models to DTOs and other simple objects. Well, that sounds exactly like what we're looking for. Entities and their relationships, i. e. the object graph of that model, those are typically a lot more complex than the DTOs we're sending over to wire. So let's add AutoMapper. And there we go. The first thing to do is configure the mappings. We have to tell AutoMapper how we trip map between our entities and our DTOs. This configuration should be created once and instantiate that start up. So, on to the configure method in our start up class. We need to create a map from our city and point of interest entities to the DTOs we're returning from the API actions. To create a map we can call AutoMapper. Mapper. Initialize method. That one requires an action on a mapping configuration as a parameter. Let's create a map from the city entity to the city without points of interest DTO, the first one we looked into in the previous demo. We call CreateMap and provide the source as the first type, that's the city entity, and the destination as the second type. That's our city without points of interest DTO. AutoMapper is convention based, it will map property names on the source object to the same property names on the destination object. And by default, it will ignore no reference exceptions from source to target. I. e., if the property doesn't exist, it'll be ignored. From time to time we might end up with having to provide our own property mappings, but for most objects, this is efficient. So now we've told AutoMapper to create a mapping from the city entity to the city without points of interest DTO. That means we can now use it in our controller. Let's open the cities controller. In the GetCities action, we're manually mapping the city entities to a list of city without points of interest DTOs. Let's delete that code, we don't need it anymore. To effectively map, we call Mapper. Map. We pass in the type we want to get back, that's an IEnumerable of city without points of interest DTO. As the parameter, we pass in the city entities we fetched from our repository. And then we return the results. As we told AutoMapper to create a mapping from the entity to the DTO, it will use that mapping to map each item in the sort list to an item in the destination list. Let's give that a try. We've still got that request saved somewhere, Get Cities, let's send it, and there we go. We got back the same result as before, but we didn't have to write all the mapping code ourselves. So let's clean this up a bit more and you can immediately see that this really improves our code base. Let's continue with the next section. In the same controller we've got an action to get a single city. This is then area mapped to a city DTO with points of interest included, or to a city without points of interest DTO. We haven't configured the mapping from the city entity to a city DTO yet. So let's do that first in the configure method on the startup class. So let's add a mapping from Entities. City to Models. City DTO. Back to the controller, we can now replace our mapping code with AutoMapper. If the points of interest should be included, we should map the city entity to a city DTO. So we can delete all this, we call Mapper. Map, pass in the type you want to map to, and as a parameter, the city entity. Afterwards, we return the map results. If the points of interest don't have to be included, we can map to a city without points of interest DTO. And let's delete that code as well, and that should be it. Let's give this a try. First let's try getting a city without points of interest included. And that works as expected. Now let's try including the points of interest for that city, and this fails. Why does this fail? We get an AutoMapper mapping exception that was unhandled in code. Why is this? Well, let's have a look at that start up class again. It looks like we're missing a mapping from the point of interest entity to the point of interest DTO, and that makes sense. We added the mapping from the city entity to the city DTO, but as the city DTO includes a list of points of interest, the mapping fails because AutoMapper doesn't know how to map that. So let's tell AutoMapper it should also create a mapping from a point of interest entity to a point of interest DTO. Okay, then, let's give this another try. Let's send that request again, and this time it works. On to the last two actions we edited in the previous demo, getting the points of interest for a city and getting a specific point of interest. Here as well we can replace our mapping code with AutoMapper. After the points of interest for a city are fetched, we want to map them to an IEnumerable of point of interest DTO. The old mapping code can then be deleted, as can what we still have commented out here. And in the other action, after we got back one specific point of interest entity, we can call AutoMapper. Map to map to a point of interest DTO. We pass in the point of interest entity that was returned from our repository. That's already looking a lot cleaner. Let's give both of these a try. First let's fetch the points of interest for a city and there we go, we get back Central Park and the Empire State Building for New York City. And then, let's fetch one point of interest and this time we only get back Central Park. So here we go, we've now used AutoMapper to replace all the manual mapping code we wrote in the previous module. Our code is a lot cleaner now, and with that we can continue with manipulating resources, which will, again, require some mapping code, of course.

Demo: Creating a Resource
In this demo, we'll make sure we can create a point of interest for a city to the repository. Let's have a look at that post action. It accepts a point of interest for creation DTO. We'll have to map this to a point of interest entity, so let's create a mapping. If we look at the point of interest entity, we see it as more properties than the DTO. An ID and a city, but that's no issue for the mapping because as we learned, AutoMapper will ignore these additional properties. Then, back to the create point of interest action on our points of interest controller. We can leave the additional validation as is, as we still want that, so let's scroll down. We're calling into the memory data store here to check if the city exists. We can now replace that with our repository method. We check if the city exists, and if it doesn't, we return a Not Found. Then we included an ugly and not completely safe count to get an ID. That can be removed now, as the primary column ID is auto-generated. And the mapping, well, we can replace that with a Mapper. Map statement. That takes care of that. Now, we need to add this point of interest, so that's an additional method we'll need on our repository as we want to keep that persistence related code contained there. Let's open the ICity info repository. The method should accept the point of interest entity we want to add, and the city ID as we need to add it for a specific city. Let's implement this on City info repository. In this method, we first have to fetch city. We've already got a method on the repository for that, GetCity, so we can use that. We don't need to load the points of interest, we're simply adding a new one, so include points of interest can be false, and then we add the point of interest to the point of interest collection of this city. That will make sure the foreign key is set to the city ID when persisting. Now, that statement doesn't effectively persist a point of interest yet. We've added it on the object context, i. e. the in memory representation of our objects, but not yet to the database. To do that, we must call safe changes on the context. So that's another method we'll need on the repository contract. We want to know if the Save went well, so we'll want it to return a boolean. And back to the implementation to persist the changes to the underlying data store, save changes on the context should be called. This returns the amount of entities that have been changed, so we want our method to return true when zero or more entities have successfully been saved. And that's it for the repository. Back to the controller, where we can now call these two methods. First, we call AddPointOfInterestForCity, pass in the city ID and the point of interest entity we just mapped from the incoming DTO. After that, we call Save. If this goes wrong, something went wrong at the server side as we already checked our validation rules and possible client mistakes. So in that case, we return a 500 Status Code. If the Save went well, we want to return that Save point of interest with a 201 Created Status code. From the moment on, Save has successfully been called, our entity will have its point of interest ID filled out, auto-generated at database level. So all that's left to do is map that entity back to a DTO and return it. And that should be it. Let's give that a try. Let's use the same post request we sent before for creating point of interests for a city. We'll add Pere Lachaise to Paris's point of interest. And we get back a 201 Created containing the city DTO in its body. In headers we can find location header, let's give that a try just to be sure, and let's get that newly created point of interest, and there we go. On to the next demo, we will learn the required steps to add update functionality to our repository.

Demo: Updating a Resource
In this demo, we'll learn how to update a point of interest for a city using both AutoMapper and our repository. Just as with creating a resource, we'll have to add a new mapping. This time it's a mapping from the point of interest to update DTO to a point of interest entity. Then, on to the updated point of interest action on the points of interest controller. Just as before, the validation can remain as is. So the first thing we need to change is to check if the city exists. That's what the CityExists method on our repository was for. We return a Not Found if the city doesn't exist. Then, there's another check, updating a resource. So that resource must exist as well, but it's that same entity we need to update with values from our past in DTO. So what we can do is fetch it using the existing method on the repository, and if it's not found, we return Not Found. That takes care of the checks which can possibly return a 404 Not Found Status Code. And then, we can use another Mapper. Map overload. If you pass in the source object, i. e. the point of interest passed in as the first parameter and the destination object, i. e. our entity as the second parameter, AutoMapper will override the values in the destination object with those in the source object. That's exactly what we need. After this statement, the point of interest entity will have the values that were passed in through the point of interest parameter. As the destination object is an entity tracked by RDB context, it now has a modified state. So once we call Save, the changes will effectively be persisted to the database. If the Save fails, we return a 500 Status Code. And otherwise, everything worked out as expected so we can return No Content. Let's give that a try. Let's update New York's Central Park point of interest by sending a Put request. We get back a 204 No Content, so everything seems well. Let's get that point of interest, and it indeed tests updated values for name and description, but as you remember, there's another way to update resources, in fact, the preferred way, partial updates. Let's check that out in the next demo.

Demo: Partially Updating a Resource
After having learned how to handle a full update, we'll now look into how we can improve partial updates in this demo. The partially updated point of interest action is a bit different than what we're used to up until now. As you remember, it accepts a JSON Patch document. Important fact here is that the patch operation is on the DTO and not directly on the entity, as we shouldn't expose entity implementation details to the outer facing layer. So, we'll have to find the entity first, and then map it to a DTO before applying the Patch document. So, we check if the city exists, and we return a Not Found if it doesn't. We fetched a point of interest entity and if it doesn't exist, we return a Not Found as well. So far, so good, that's exactly the same as what we did for Put. Then, we'll have to map this entity to a DTO, more specifically we'll have to map it to a point of interest for update DTO, as that is what a Patch document works on as you can see in the parameter list of the partially updated point of interest action. To do that, we call Mapper. Map on the point of interest for update DTO and we pass in the point of interest entity we just fetched from our repository. But we haven't created a mapping for this yet. Let's add one in the configure method of the startup class. So we'll lead the mapping from the point of interest entity to the point of interest for update DTO. Back to our controller. We applied the Patch document to the point of interest to patch, which is a DTO, as we've just mapped the entity to that, so that's the same code as we already had. Afterwards we validate, because the validation has to happen on the DTO after the Patch doc has been applied to see if it's still valid. From this moment on, the DTO we just created has to correctly patch properties and is valid. We now map that back into that entity. For that, we can use the Mapper. Map method, passing in the source object, which is the Patch DTO, and the destination object, which is the entity. As we know from the previous demo, changes are tracked, so once we save, the changes on the entity will be persisted so we call cityInfoRepository. Save. If it fails we return a status code 500, otherwise we return No Content. Let's give that a try. We've got a Patch request here to update the name and description of New York City Central Park, let's send it. We get back a 204 No Content, everything looks good, let's try and get that point of interest, and there we go, our patch has successfully been applied. There's one action left we have to change, and that's the delete action.

Demo: Deleting a Resource
In this demo, we'll make sure our delete action uses the repository instead of the in memory data store. Let's look at the delete point of interest action of the points of interest controller. There's no mapping to be done here, as we're only deleting a point of interest we're not sending JSON to in the request body, nor are we sending a response body back. So, the first thing is, as we're used to by now, to check to see if the city exists. If it doesn't, we return Not Found, and then we need to check if the point of interest we want to delete for that city, exists. Just as in the update actions, we don't really need a point of interest exists method on the repository, as it's that same point of interest that will pass into a delete action on the repository afterwards. So, we look for the point of interest for this city, and with the passed in ID, and if it's not found, we return a Not Found. Then we need to effectively delete it. There's no method like that on our repository yet, so we'll add one to the contract. It should accept the point of interest you want to delete as a parameter. Then we need to implement it on the city info repository. What we want to do is remove that point of interest from the object context through the points of interest db set. For that, we call the remove method. Back to our controller. We can now call this method on our repository passing in the point of interest. And then, all that's left is calling Save to effectively remove that point of interest from the database. If the Save fails, we return a 500 Internal Server error. Otherwise, we send that fake mail to our mail service, and return No Content. We'll still want to send that fake mail when someone deletes a point of interest, so let's make sure it can be sent, by passing the correct name and ID. So that's no longer point of interest from store, but point of interest entity. Let's give that a try. I still have that result from the point of interest we created a few demos ago on screen here. That's Pere Lachaise for Paris to be found at the location you see here on the screen. Let's get that one. It's still there, so now let's send a delete request to ID 3002. We get back a 204 No Content, so far, so good. Let's try and get that point of interest again, and we get a 404 Not Found, it has successfully been deleted. And with that, we now ended up with a fully functional API, built with ASP. NET Core, including logging a custom service and Entity Framework Core, connecting to a SQL Server local DB data store. All of that through the repository pattern. It's time to look at the summary.

Summary
So, it's time for the last summary. Throughout this course, we've built an API from the ground up, using ASP. NET Core and Entity Framework Core. We've built it step by step, gradually improving on our code base. And in this last module we tied it all together. We learned about the repository pattern, an abstraction that reduces complexity and aims to make the code safe for the repository implementation, persistence ignorant. In fact, the best illustration of this was that we would've had a lot less work in switching out the in memory store for our database if we had used the repository in the beginning of the course. We also introduced a new component in our application, another part of code that was quite error prone, was the mapping code between DTOs and entities, so we improved our code base by using AutoMapper for this. And with that, this course is finished. In case you have a question, don't hesitate to ask it in the discussion, or just send me a message on Twitter. I really hope you found this course interesting, and had as much fun watching it as I had making it. Just one more thing before we go, remember, you're ready to be awesome.