# Building a RESTful API with ASP.NET Core - Intermediate

## Course Overview
Building an API is one thing, building a RESTful API is something different. In this course you'll learn how to do that with ASP. NET Core. We'll look into how we can correctly interact with our API by using the correct HTTP methods and status codes, getting, updating, creating, and deleting resources. Learning about method safety and item potency will help us choose the correct approach for these different use cases. We'll also look into some less obvious cases and principals, like creating a list of resources in one go and upserting. We'll cover validation and logging as well, as when we're creating or updating something we want to ensure the input is valid, and if it isn't, we'll want to log that. We'll also cover common functionality RESTful APIs expose these days, like paging, sorting, filtering, data shaping, and so on. But we won't stop there. We're going to ensure our API is HATEOAS-enabled, so hypermedia will drive application state. It's one of those constraints RESTful architecture has that tends to separate truly RESTful APIs from run of the mill web APIs. We'll also learn how to correctly use media types and we'll look into versioning, caching, and handling concurrency. We'll end with a module on protecting and documenting the API. In the end, we'll have built an API with level 3 maturity, which is the highest possible level for APIs.

## Introducing REST
## Coming Up
Hi there, and welcome to the Building a RESTful API with ASP. NET Core course, at Pluralsight. I'm Kevin, and I'll guide you through the rest of this course. This is the first module in which we'll introduce REST. REST is, and has been for a few years, all over the place, or at least so it seems, because a lot of APIs that are called RESTful aren't really RESTful. We'll notice why that's the case very quickly, but we'll start out with the course prerequisites and tooling. After that, we'll introduce REST and we'll look at the constraints a RESTful system must adhere to to be RESTful. And then, we'll learn about the Richardson Maturity Model. That's a model that describes the maturity of an API, and, in a way, to what degree it conforms to REST. It's an interesting model as it helps us think about how APIs used to be built, and still are in some applications, and how that can be improved upon with REST. And after that, we'll look at how we can position ASP. NET Core and the MVC pattern for building RESTful APIs. And we'll immediately dive into the first demo in which we'll inspect the starter solution. So, as you noticed this module is mostly theoretical, but I promise it'll be the longest stretch of slides without code for the rest of the course. Nevertheless, it's a pretty important module. What we'll learn here will come back in all the other modules of this course. Let's start out with the prerequisites.

## Course Prerequisites
This is an intermediate course on REST, so you don't need to know anything about REST yet. In fact, this course has three points of focus, REST, REST, and REST. But, knowledge of C# is required, obviously, and some knowledge of ASP. NET Core will come in handy as well, because that's what we'll use to build our RESTful API. We will run through the basics when we inspect the starter solution, but if you haven't worked with ASP. NET Core before, you might want to have looked at Scott's ASP. NET Core Fundamentals course, or, specifically aimed at building APIs, my course, Building Your First API with ASP. NET Core. That one covers the basics of API building with ASP. NET Core 1. x and 2. 0. Both are beginner courses that don't require any previous knowledge of ASP. NET Core. Let's have a look at the tooling options we have.

## Tooling
As far as tooling is concerned, you've got a few options. The first option is Visual Studio 2015 update 3. This can be used to build ASP. NET Core 1. 0 projects with the xproj-based project system. If you don't have a licensed Visual Studio 2015 version, you can also use the free Visual Studio 2015 Community Edition, which you can find by clicking the link. This second option is Visual Studio 2017. You can use Visual Studio 2017 to create ASP. NET Core 1. x projects using the more familiar csproj/MSBuild-based system. If you want to create an ASP. NET Core 2 project, you'll need version 15. 3 at least. A free version exists as well, the Community Edition. ASP. NET Core applications can also be developed using older IDEs, like Visual Studio Code, which also runs on OS X and Linux, and Visual Studio for Mac. The reason for choosing a specific IDE might depend on the IDE licenses you've got at your disposal, or, on the functionality the IDE and tooling offer. The experience in Visual Studio 2017 is much more streamlined than when working with Visual Studio 2015. In Visual Studio 2015, the tooling never got out of preview stage. But, personal preference is also important. If you prefer to use Visual Studio Code, by all means, you should. As mentioned in the prerequisites, some knowledge of ASP. NET Core would come in handy, so I am assuming you've already got your IDE of choice and. NET Core 1 or 2 installed. If that isn't the case, you'll have to download and install the latest. NET Core SDK, which you can find on the site on screen. At the moment of recording, 2. 0. 2 is the current version. Depending on when you're watching this course, you can expect the versions to be different.. NET Core, by design, has a pretty fast release schedule. But in any case, on this page you'll always be able to find exactly what to install. As far as the code for creating our API demo project is concerned, there's about 95% overlap between ASP. NET Core 1. x and ASP. NET Core 2. That means that almost everything code related you'll learn in this course can be applied to both versions. When there are differences, we'll switch between Visual Studio 2015 with ASP. NET Core 1 and Visual Studio 2017 with ASP. NET Core 2 to show them. The exercise files contain three demo flavors, one that's project JSON-based for use in Visual Studio 2015 with ASP. NET Core 1, another one is based on csproj and MSBuild and can be used in Visual Studio 2017 with ASP. NET Core 1. And lastly, there's the csproj/MSBuild flavor that targets ASP. NET Core 2, which, at the moment of recording, is the newest version, and probably the most advisable one. So, as far as code is concerned, it might seem like a no brainer to go for ASP. NET Core 2. New versions often offer improved performance, better security, and an extended API service. This is the case with ASP. NET Core 2. Often the extended API service is one of the main reasons to choose a newer version. It might contain functionality you need, which wasn't available in an older version. On the other hand, the support lifecycle can be important as well. Not all versions are supported for the same amount of time, which might lead to choosing an older version. There's two different release flavors. Well, three if you count the nightly builds. You can see the support lifecycles on screen.. NET Core 1. 0 and 1. 1 are LTS releases, or long-term support releases.. NET Core 2. 0, on the other hand, is a current release, and there's no guarantee that a current release will become a long-term support release. So, depending on the support lifecycle you require, you might want to go for an LTS release instead of a current release. Or, the other way around if you require a feature that's only available in a current release. In any case, the different exercise file flavors have got you covered. That's it for the IDE and. NET Core version, but we'll use some other tools as well. To create requests to the API we'll build and to see the response messages, we'll use Postman. Postman is a free tool you can use to, amongst other things, create and send HTTP requests and inspect responses. But both together with the exercise files are all HTTP requests we'll send in this course, so you can just import them in Postman and use them. That saves you a bunch of typing. As an alternative, you could use Fiddler or any other tool of choice that allows you to create HTTP requests and inspect the responses. You'll need a browser of choice as well, and that's all that's needed to get started.

## Introducing REST
Let's talk about REST. Most of the time when we hear this term, the first thing that comes to mind is APIs, it's how we build APIs, and JSON, because that's what we get back from these APIs, and that's a pretty common thing to say as all we read and hear about are, well, RESTful APIs that return JSON. But, REST isn't just about building an API which consists of a few HTTP services that return JSON. It's much broader than that. Let's have a look at the definition. Representational state transfer is intended to evoke an image of how a well-designed web application behaves, a network of web pages where the user progresses through an application by selecting links, resulting in the next page being transferred to the user and rendered for their use. Well, that's a lot of text, but we'll clarify this immediately. The definition comes from Roy Fielding. He first coined the term in his PhD dissertation to describe an architectural style of network systems. You can find that at the link on screen. This definition is interesting. It implies that we first had web apps, and then Mr. Fielding described how a well-designed version should behave. This also implies that REST is not a standard, it's an architectural style. When we implement it, we will use standards of course, but in principle REST is completely protocol agnostic. JSON isn't a part of REST, but theoretically not even HTTP is a part of REST, and that's really theoretical. To be honest, I haven't seen a single RESTful system that doesn't use HTTP; it's pretty much the web's golden standard, so that is what is used, and from this moment on we will assume this. But I did want to mention this to drive the point home that it is an architectural style and nothing more than that. It's up to us to fill in the blanks with the protocols and standards we want to use. Now, as the style is intended to evoke how a well-designed web application behaves, we can use a web application to explain that huge definition we just saw. Imagine you want to read your favorite newspaper online. You've opened your browser. That browser, that's an HTTP client. You point it to a URI. That's the unique resource identifier. It identifies where the resource lives. By doing that, the browser actually sends an HTTP request to that URI. The server then does some magic and sends an HTTP response message back to the browser. That HTTP response message contains a representation of the page you've navigated to. In our example, that would probably be some HTML and CSS. The browser then interprets that resource representation and shows it. In other words, the browser, our HTTP client, has changed state. Now let's say we click a link in our browser to access a specific article on the newspaper side. That one is again identified by a URI. A new request message is sent to the server, and the server again sends back a representation of the page, the resource. The browser interprets it and changes state. In other words, the client changes state depending on the representation of the resource we're accessing. And that's representational state transfer, or REST. Now let's map that to building an API. We'll use an example API with authors and books, as that's what we'll use throughout the course. The HTTP client can be a browser, but often it's an application that can be built in a variety of technologies, like Angular or ASP. NET Core MVC. It wants to get an author from our API, so it sends a request to the URI where that author resides, and our API sends back a representation of that resource in the response body. These days that's typically a JSON representation, but it doesn't have to be. So the client gets back the representation and changes state, and then we can continue. The client sends a request to the URI of another resource, say the author's books. The API sends back a representation and the client changes state again. And with that we now know what REST is, but how do we build such a system? An architectural style is described by a set of constraints, and it's these constraints we'll have to adhere to. Let's have a look at those constraints.

## Learning What the REST Constraints Are About
REST is defined by six constraints, one of which is optional. We should see these constraints as design decisions, and each of those can have both positive and negative impacts. So, for these constraints, the benefits are considered to outweigh the disadvantages. Let's start with the client-server constraint, a very basic fundamental constraint. What this does is enforce client-server architecture. A client, or consumer of the API in our lingo, shouldn't be concerned with how data is stored, or how the representation is generated, that's transparent. A server, the API in our lingo, shouldn't be concerned with, for example, the user interface or user state or anything related to how the client is implemented. In other words, client and server can evolve separately. The second constraint is statelessness. You might have guessed that one from the explanation of the acronym we just learned about. This means that the necessary state to handle every request is contained within the request itself. When a client requests a resource, that request contains all the information necessary to service the request. It's one of the constraints that ensures RESTful APIs can scale so easily. We don't have things like server-side session state to keep in mind when scaling up. Then, we have the cacheable constraint. This one states that each response message must explicitly state if it can be cached or not. Like this we can eliminate some client/ server interaction, and at the same time prevent clients from using out-of-date data. We'll look into this in more detail later on. Next up, the layered system constraint. That's a pretty easy one actually. A REST-based solution can be comprised of multiple architectural layers, just as almost all application architectures we use today. These layers can be modified, added, removed, but no one layer can directly access a layer that's beyond the next one. That also means that a client cannot tell whether it's directly connected to the final layer or to another intermediary along the way. REST restricts knowledge to a single layer, which reduces the overall system complexity. And then there's the optional code on demand constraint. This one states that the server can extend or customize client functionality. For example, if your client is a web application, the server can transfer JavaScript code to the client to extend its functionality. I saved the biggest one for last, the uniform interface constraint, as it's divided into four sub-constraints. It states that the API and the consumers of the API share one single technical interface. As we're typically working with the HTTP protocol, this interface should be seen as a combination of resource URIs, where we can find resources, HTTP methods, how we can interact with them, like GET and POST, and HTTP media types, like application/json, application/xml, or more specific versions of that that are reflected in the request and response bodies. All of these are standards, by the way. This makes it is a good fit for cross-platform development. By describing such a contract, this uniform interface constraint decouples the architecture, which in turn enables each part to evolve independently. We're going to start designing it for our API at the start of the next module. But I said there are four sub-constraints for this uniform interface constraint. The first one is identification of resources. It states that individual resources are identified in requests using URIs, and those resources are conceptually separate from the representations that are returned to the client. In our demos, we'll work with authors and books. The server doesn't send an entity from its database or possibly a combination of fields from multiple additional databases and services because our author resource doesn't necessarily map to an author in one database. Instead it sends the data, typically for RESTful APIs, as JSON, but HTML, XML, or custom formats are also possible. So, if the API supports XML and JSON, both representations are different from the server's internal representation, but it is the same resource. There's also a few best practices on how to design these resource URIs. We're covering that in the beginning of the next module as well. Let's continue with the second sub-constraint. That's manipulation of resources through representations. When a client holds a representation of a resource, including any possible metadata, it has enough information to modify or delete a resource on the server, provided it has permission to do so. To continue with our example, the representation of an author and its metadata in the response message should be enough to successfully update or delete that author, if the API allows that. But what does that mean? Well, if the API supports deleting the resource, the response could include, for example, the URI to the author resource, because that's what's required for deleting it. We'll encounter this one and a few ways of adhering to this constraint quite a few times during this course. The third sub-constraint is the self-descriptive message sub-constraint. Each message must include enough information on how to process it. When a consumer requests data from an API, we send a request message, but that message also has headers and a body. If the request body contains a JSON representation of a resource, the message must specify that fact in its headers by including the media type, application/json for example. Like that, the correct parser can be invoked to process the request body, and it can be serialized into the correct class. Same goes for the other way around. Mind you this application/json media type is a simple sample. Media types actually play a very important role in REST. We're covering that in the module on HATEOAS and media types. And HATEOAS, well, that's the fourth sub-constraint. It means Hypermedia as the Engine of Application State. This is the one that a lot of RESTful systems fail to implement. Remember that example we had in the beginning of the module when we explained while looking at a newspaper site how the state of our browser changed when we clicked the link? Well, that link, that's hypertext. Hypermedia is a generalization of this. It adds other types like music, images, etc., and it's that hypermedia that should be the engine of application state. In other words, it drives how to consume and use the API. It tells the consumer what it can do with the API. Can I delete this resource? Can I update it? How can I create it, and where can I find it? This really boils down to a self-documenting API, and that documentation can then be used by the client application to provide functionality to the user. It's not an easy constraint. In fact, there's a full module in this course dedicated to it. So, we'll look into this one in detail and learn how to implement it in the module on HATEOAS. That's quite a lot of constraints to adhere to, but we can't build a RESTful system without knowing what it is. Don't worry if you don't immediately see how to adhere to all of these, we will come back to them when applicable. Not all of these constraints are straightforward to implement, and to be completely correct, an architecture that skips on one of the required constraints or weakens it is considered not conformed to REST, and that immediately means that most APIs that are built today and are called RESTful, well, they aren't really RESTful. But does that mean that they are all bad APIs? In my line of work, most APIs I see when working for clients do not adhere to all these constraints. HATEOAS is a pretty typical one to skip, but that does not mean they're bad APIs. It does mean that you need to know the consequences of deviating from these constraints and understand the potential tradeoffs, which this course should help you with. HTTP APIs come in many forms, and there's a model that describes the maturity of those APIs. You might have encountered it before. It's called the Richardson Maturity Model, and it'll help us position REST for building HTTP APIs better. Let's look into it.

The Richardson Maturity Model
The Richardson Maturity Model is a model developed by Leonard Richardson. It grades, if you will, APIs by their RESTful maturity, so it's interesting to look into it as it shows us how we can go from a simple API that doesn't really care about protocol nor standards, to an API that can be considered RESTful. The first level is level 0, The Swamp of POX, or plain-old XML. This level states that the implementing protocol, HTTP, is used for remote interaction. But, we use it just as that and we don't use anything else from the HTTP protocol correctly. So for example, to get some altered data, you send over a POST request to some basic entry point URI, like host/myapi, and in the body you send some XML that contains info on the data you want. You then get back the data you asked for in the response. To create an author, you send another POST request with some data in the body to that same entry point, and so on. In other words, HTTP is used, but only as a transport protocol. And example of this is SOAP, or other typical remote procedure call implementations. It's not a fully correct statement, but you see a lot of these RPC style implementations when building services with Windows Communication Foundation. Level 1 is resources. From this moment on, multiple URIs are used and each URI is mapped to a specific resource, so it extends on level 0 where there was only 1 URI. For example, we now have a URI, host/api/authors, to get a list of authors, and another one, host/api/authors, followed by an ID, to get the author with that specific ID. However, only one method like POST is still used for all interactions. So, the HTTP methods aren't used as they should be according to the standards. This is already one little part of the uniform interface constraint we see here. From a software design point of view, this means reducing complexity by working with different endpoints instead of one large service endpoint. Then, we have the second level, verbs. To reach a level 2 API, the correct HTTP verbs, like GET, POST, PUT, PATCH, and DELETE, are used as they are intended by the protocol. In the example we see a GET request to get a list of authors, and a POST request containing a resource representation in the body for creating an author. The correct status codes are also included in this level, i. e. use a 200 Ok after a successful GET, a 201 Created after a successful POST, and so on. All of these are covered later on. This again adds to that uniform interface constraint. From a software design point of view, we have just removed unnecessary variation. We're using the same verbs to handle the same types of situations. And level 3, that's hypermedia controls. This means that the API supports HATEOAS, another part of that uniform interface constraint. A sample GET request to the authors resource would then return not only the list of authors, but also links, hypermedia, that drive application state. We're covering this in detail in its own module. From a software design point of view, this means we've introduced discoverability, self-documentation. What is important to know is that according to Roy Fielding, who coined the term REST, a level 3 API is a precondition to be able to talk about a RESTful API. So, this maturity model does not mean there's such a thing as a level 1 RESTful API, a level 2 RESTful API, and so on. It means that there are APIs of different levels, and only when we reach level 3 we can start talking about a RESTful API. And with that, we have covered a lot of theory. Maybe it's time to look into how we can start building something like this. Let's have a look at how we can use ASP. NET Core and MVC to build a RESTful API.

Positioning ASP.NET Core for Building RESTful APIs
RESTful APIs can be built with a variety of technologies. We're going to use ASP. NET Core and the MVC middleware. Now, as we learned in the prerequisites, for a dive into the basics of ASP. NET Core, I'd suggest one of the beginner courses we mentioned in the beginning of this course. This course does assume some knowledge of it. Of course you don't have to, and we will explain some of it in the demo coming up right after this. In a few words, ASP. NET Core is an open-source and cross-platform framework for building modern cloud-based internet connected applications, web apps, but also APIs for those web apps, amongst others. If you've worked with a previous version of ASP. NET, well, this isn't an update. It was rethought from the ground up. Now these ASP. NET Core applications allow injecting middleware into the request pipeline. The ASP. NET Core MVC middleware is one of those. It provides us with a framework to build APIs and web apps using the MVC, or model view controller pattern, so this is what we're going to use to create a truly RESTful API. But it's very important to know that we don't just get a RESTful API out of the box just because we're building an API with ASP. NET Core. That's our responsibility. We get that by adhering to the constraints. So, we'll use ASP. NET Core MVC to wield our API. But what is that MVC pattern then in regards to an API? In essence, MVC is an architectural software pattern for implementing user interfaces. Different interpretations exist, but in all interpretations, it's used to encourage loose coupling and separation of concerns that then allows better testability of the app and the ability to reuse parts of it. But mind you, it isn't a full application architecture, just like REST. Typically, MVC is used in the presentation layer, and that's about it, hence the implementing user interfaces part of the definition. But how do we map this to an API then? Well, an API can also be regarded as a user interface; it's the interface to the consumer of the API. So let's do that. It consists of three parts. The model handles the logic for our application data. A model in this sense can contain code to retrieve or store data at that level. In some implementations, view models are used as a model. These contain code to retrieve or store data, but in quite a few implementations the model doesn't contain any logic at all, it's another component of the application that handles this. For example, the business layer. The model, that's then the DTOs used by an API, those that are serialized into the response bodies, AKA serialized to resource representations. The view represents the parts of the application that handle displaying of data. So I'm building an API, this view should be regarded as the representation of our data or resources. More often than not these days, that is JSON. And the controller then handles the interaction between the view and the model, including handling user input. In the API world, this user that interacts with the API is the consumer, typically another application. If you look at the dependencies between these components, both controller and view depend on the model, and the controller also depends on the view. That's one of the key benefits of this separation. In other words, the controller chooses the view to display to the user and provides it with any model data it requires. So, when a request is made to an API by a consumer, an action on the controller will be triggered. The controller sends any data that's input, like query string parameters, to the part of the application responsible for business or data access logic. It then returns a model to the view, in this case the resource representation in JSON, for example. Let's have a look at the starter solution in the upcoming table.

Demo - Inspecting the Starter Solution
In this first demo of the course, we'll have a look at the starter solution. We'll build upon this throughout all the modules in this course. You can get this from the exercise files, or, alternatively, from my GitHub. Let's have a look at the ASP. NET Core 1 flavor first. I created this solution starting from an empty website project targeting. NET Core 1. That means we end up with a Program. cs class, the starting point of our application. The main method here is responsible for configuring and running the application. In our case, we'll be running a web app, so the host for that web application is set up. All the default options were left as is. It states we're using the Startup class as the startup type to be used by the web host. So, let's have a look at that Startup class. In the constructor our configuration files are set up. Two methods of importance here are ConfigureServices and Configure. ConfigureServices is used to add services to the built-in dependency injection container, and to configure those services. Services should be seen as a broad concept. A service is a component that is intended for common consumption in an application, so the services we add here can later be injected into other pieces of code that live in our application. The second one is the Configure method. This one uses services that are registered and configured in the ConfigureServices method, so it's called after that. The Configure method is used to specify how an ASP. NET Core application will respond to individual HTTP requests. In other words, through this, we can configure the request pipeline. I did make a few changes to these methods, so we can start with the database accessed through Entity Framework Core and a repository that calls into the Entity Framework Core DbContext. In ConfigureServices, the DbContext is configured and registered, and a repository is registered as well, so we can inject it later on in our controllers. Let's have a look at those. The context named LibraryContext consists of two DbSets, a set of Authors and a set of Books. These map to two database tables. An author has an Id, FirstName, LastName, DateOfBirth, and Genre. Next to that, for each author there's a collection of books. Each book has an Id, a Title, and a Description. The other fields are a navigation property, author, and an explicit foreign key back to the author entity. That's the AuthorId property. Let's have a look at the database this results in. We can use the SQL Object Explorer for that. It results in a LibraryDB database with just two tables, Authors and Books. The other one, that's for migrations. The Authors table contains the columns we defined as properties on the Author Entity class, and the Books table contains the same, but for Books. It also contains an AuthorId, which is a foreign key to author. So it's a fairly simple database, just two tables and a very standard parent/child relationship. But with just a simple datastore, we're already able to dive deep into REST. Our LibraryContext is then accessed through a repository, the LibraryRepository. Let's have a look at that. The contract currently contains standard CREATE, READ, UPDATE, and DELETE operations, GetAuthors for an IEnumerable of Author, GetAuthor for a specific author, and so on. We will change this as needed throughout the course. The implementation then, as said, uses the LibraryContext and implements the ILibraryRepository contract. These are all pretty default link statements. But, this database also has to be filled with some dummy data. I created an extension on LibraryContext for that. It's named EnsureSeedDataForContext. In this method, the database is cleared and then filled with a list of dummy authors and books. That makes it very convenient for us to work with throughout the demos. Every time we run a demo, we're starting anew. Let's go back to that Startup class. In the Configure method, EnsureSeedDataForContext is called and the MVC middleware is added to the request pipeline. By doing that, each HTTP request can be handled by the MVC framework. Let's switch to ASP. NET Core 2 for a bit. As mentioned before, about 95% of the demo code between versions is the same. That 5% that is different is mostly situated in the Program. cs and Startup. cs files we just looked through. We're in the Program class; the main method just contains one statement, and that statement calls into CreateDefaultBuilder on an IWebHost instance followed by the startup type to use and. Build. CreateDefaultBuilder will create a default builder to build the web host, so at first sight this code is just a shortcut for what we just ran through in ASP. NET Core 1. But it actually does a little bit more than that. As ASP. NET Core is open source, we can have a look at the actual code that'll be executed. So let's open GitHub. We're looking at the CreateDefaultBuilder method. We see that statements like UseKestrel, UseContentRoot, and UseIISIntegration can be found in this implementation, and those match what we've seen in the ASP. NET Core 1 version. But, there's other statements as well. Default files and variables for configuration are set up, and logging is set up as well. That means that in ASP. NET Core 2 we don't have to manually set this up anymore, it's done for us. Moreover, the fact that logging is set up earlier in the bootstrapping process means we can log issues that happen during bootstrapping as well, and that wasn't that convenient with ASP. NET Core 1. Next up is the Startup class. In the constructor we no longer have to set up the configuration files, the call into CreateDefaultBuilder already took care of that for us. All we need to access our configuration is an instance of IConfiguration, which we can inject when needed. Let's scroll down a bit to the Configure method. The important thing here is what isn't here. In ASP. NET Core 1's default template, a console logger is added here. In ASP. NET Core 2 that's no longer required as such a logger is already added by calling into CreateDefaultBuilder. The last important difference before we can continue has to do with package references. Let's open the NuGet dialog. Next to. NET Core we only see one reference, a reference to Microsoft. AspNetCore. All. That's what's called a metapackage reference. By adding this reference, we're actually adding references to all the dependencies you can see when we scroll down a bit. This is an ASP. NET Core 2-only feature. Now why would we do this? Well, if you've worked with ASP. NET Core before, you know that instead of using one big monolith assembly like System. web in the old ASP. NET, ASP. NET Core instead shows a more modular, granular approach. It's made up of a large set of small packages containing specific pieces of functionality, but that comes with its own set of issues. It's not always easy to find out in which package to the functionality you need can be found. Moreover, keeping an eye on using the correct versions of these assemblies can become quite cumbersome, but that's the way it is for ASP. NET Core 1. x applications. So for ASP. NET Core 2, this was solved with the Microsoft. AspNetCore. All metapackage. That metapackage includes all supported packages by the ASP. NET Core Team, all supported packages by the Entity Framework Core Team, and internal and third-party dependencies used by ASP. NET Core and Entity Framework Core. Let's switch to ASP. NET Core 1 for comparison. We're looking at the NuGet dialog and we can see that we have multiple references to Microsoft. AspNetCore and Microsoft. EntityFrameworkCore related packages. Throughout the course when we need a piece of functionality that's contained in an ASP. NET Core package that isn't reference yet, we'll have to add a reference to that specific package. If you're using ASP. NET Core 2, you won't have to do that. So this metapackage makes life a bit easier. However, it comes with a potential drawback. It requires you to install the. NET Core SDK on the server you're deploying to. That avoids having to deploy them together with the application. That's it. This is completely optional. If you don't like the metapackage reference, you can also manually add references to each specific package you want to use just as in ASP. NET Core 1. Anyway, all of this is just the plumbing to get started. We haven't got any controllers, and we're not returning any data yet. But, starting from a solution like this we can focus on REST instead of on the plumbing. Let's do just that after this module summary.

Summary
REST is an architectural style evoking an image of a well-designed web application, and that's also what the acronym describes, a system in which we progress through an application by selecting links, state transitions, resulting in the next state of the application being transferred to the user and rendered for their use. We looked into that with an example of how that can be linked to a web application and to an API. So it's an architectural style, but not a standard. It does use standards to implement the architectural style. And what's needed to implement it is described by six constraints. The client-server constraint enforces client-server architecture, which in turn enforces separation of concerns. The statelessness constraint means that the necessary state to handle every request is contained within the request itself. It's one of the constraints that ensures that RESTful APIs can scale so easily. And then we have the cacheable constraint. This states that each response message must explicitly state if it can be cached or not. Like this, we can eliminate some client-server interaction and at the same time prevent clients from reusing out-of-date data. Up next was the layered system constraint, which restricts knowledge to a single layer. That then reduces the overall system complexity. It doesn't need to know about the other layers. And then there's the optional code on demand constraint. This one states that a server can extend or customize client functionality. A typical example is a server serving up a JavaScript file to a client. Lastly we had the uniform interface constraint divided into four sub-constraints. It states that the API and API consumers share one single technical interface. And by describing such a contract, this constraint decouples the architecture, which in turn enables each part to evolve independently. Then we covered the Richardson Maturity Model. It grades APIs by their RESTful maturity. We learned that there were four levels, from level 0, the Swamp of POX, right up to level 3, hypermedia. To be able to talk about a RESTful API, level 3 is a prerequisite. Then we looked into how we can go about building such an API. We're using ASP. NET Core, which is an open-source, cross-platform framework for building modern internet connected applications. It allows injecting middleware into the request pipeline. And the ASP. NET Core MVC middleware provides a framework for building APIs and web applications using the model view controller pattern. Finally, we looked at the starter solution and we learned about the basics of our ASP. NET Core MVC setup. So we've got everything to build upon and we know what REST is. Time to start implementing this in the next module, Getting Resources.

Getting Resources
Coming Up
Hi there and welcome to the Getting Resources module from the Building RESTful API with ASP. NET Core course at Pluralsight. My name is Kevin and I'll guide you through this module. First we'll learn about the outer facing contract, i. e. how are we going to design our resource URIs, allow the HTTP methods and payloads for our API? Once we know that, we'll start implementing it by getting resources, both single and collection resources. We'll also learn how to work with parent/child relationships. While doing this, we will encounter concepts that are essential to how we build RESTful APIs with ASP. NET Core. We'll cover HTTP methods and routing. We'll learn what status codes are why they are so important. And we'll learn about faults and errors and what the difference between these is. Lastly, we'll learn about content negotiation, which allows a consumer of the API to choose the format of the resource for presentation, if the API supports it. A lot of interesting stuff coming up, so let's dive in with structuring our outward facing contract.

Structuring Our Outer Facing Contract
The outer facing contract consists of three big concepts a consumer of an API uses to interact with that API. First the resource identifiers, i. e. the URIs where the resources can be found. That's then combined with HTTP methods like GET, to get resources, and third is an optional payload. When creating a resource, the HTTP response will contain a resource representation in its body. The format of those representations is what media types are used for, like application JSON. The uniform interface constraint does cover the fact that resources are identified by URIs. Each resource has its own URI, but as far as naming of resources is concerned, there isn't a standard that describes that, or at least not unless you want to dive into OData. There are, however, best practices for this. A resource name in a URI should always be a noun. In other words, a RESTful URI should refer to a resource that is a thing, instead of referring to an action. So we shouldn't create a getauthors resource, that's an action. We should create an authors resource, that's a thing conveyed by a noun, and use the GET method to get it. To get one specific author then, we'd append it with a forward slash and the authorId. Using these nouns conveys meaning, it describes the resource, so we shouldn't call a resource orders, when it's in fact about authors. That principle should be followed throughout the API for predictability. It helps a consumer understand the API structure. If we have another non-hierarchical resource, say employees, we shouldn't name it api/something/something/employees, we should name it api/employees. A single employee then shouldn't be named id/employees, it should be named employees, forward slash, and the employeeId. This helps keep the API contract predictable and consistent. There's quite a bit of a debate going on on whether or not we should pluralize these nouns. I prefer to pluralize them as it helps to convey meaning. When I see an authors resource, that tells us it's a collection of authors and not one author, but good APIs that don't pluralize nouns exist as well. If you prefer that you can, but do make sure to stay consistent. Either all resources should be pluralized nouns, or singular nouns, and not a mix. Another important thing you'd want to represent in an API contract is the hierarchy. Our data or models have structure. For example, an author has books that should be represented in the API contract. So if you want to define an author's books, where the books in the model hierarchy are children of an author, we should represent them as api/authors/authorId/books. A single book should then be followed by the bookId. APIs often expose additional capabilities. Later on we'll learn about filtering and ordering resources, those parameters should be passed through the credit string, they aren't resources in their own right. So we shouldn't write something along the lines of api/authors/orderby/name. There's a few contract smells in that URI. A plural noun should be followed by an Id, and not by another word, and orderby isn't a noun, and a URI like this would mean we'd have defined three different resources - authors, authors/orderby, and authors/orderby/name. So api/authors followed by orderby=name in the credit string is a better fit. And with that we've already covered a lot, but there is an exception. There always has to be an exception, life would be too easy without them. Sometimes there's these remote procedure calls, style-like calls, like calculate total, that don't easily map to resources. Most RPC-style like calls do map to resources, as we've just proven, but what if we need to calculate, say, the total amount of pages an author wrote? It's not that easy to create a resource from that using pluralized nouns. You'd end up with something like api/authors/authorId/pagetotals, and then what? Because we'd expect this to return a collection and not a number. You could go for something else, like api/authorpagetotals, followed by an Id where the backend would then have to map that Id to an authorId, or it can even be the same Id, and that would work, and it does fit the URI design guidelines, but it does feel a bit out of place so this is one of these examples, or one of these exceptional cases, where I'd suggest to take a bit of a pragmatic approach, api/authors/authorId/totalamountofpages. It isn't according to these best practices, but as long as it's an exceptional case, it doesn't mean you've suddenly got a bad API. Remember there isn't standards for following for these naming guidelines, these are just guidelines. So by following these simple rules we'll end up with a good resource URI design. But there's one more thing we need to talk about, Ids. Should these be integers? Typically auto-numbered from the database, or should they be GUIDs? In fact, REST stops at that outer facing contract. The layers underneath, including the data store, are of no importance to REST, so getting an author might mean that you're actually fetching data from three different data stores, including some fields from Active Directory, to compose that author resource representation. So it's of no importance, our resource isn't the same as what is in the backend store. These are two different concepts. But from that follows the question, what should we use as identifiers? REST is unrelated to the backend data store, yet often you'll see APIs that actually use the auto-numbered primary key Ids from the database. If the backend doesn't matter, what happens to the resource URIs if you change the backend? The resource URI should remain the same, but if resources are identified by their database auto-numbered fields, and we switch out our current SQL Server to a backend that uses another type of auto-number sequence like MongoDB, all of a sudden all our resource Ids can change. We can, of course, work around that on migration, but still, it's a good idea to keep this in mind when designing resource URIs. And there's a solution for this. GUIDs, unique and unguessable values you can use as primary key in every database. From that we can then switch out datastore technologies and our resource URIs will stay the same. We're also no longer potentially exposing implementation details, as those GUIDs don't give anything away about the underlying technology. They work with all of them, so that's why we'll use GUIDs in this course. This advantage, in my book, is readability. As a developer it's not that convenient to type over a GUID to test an API call, but that's what testing tools are for, and for the end product it shouldn't matter. It's not users like you and me who tend to talk to an API, say for during development, its other pieces of code. And for those it really doesn't matter if the URI contains a hard-to-type GUID. And with that it's time to start implementing this contract.

Demo - Implementing the Outer Facing Contract (Part 1)
What we're going to do is ensure we can return a resource collection, authors from one API. In other words we want to implement the beginning of our outer facing contract. The first thing to do to enable that is add a controller. So let's create a Controllers folder, and let's add a new class to that. We're going to name it AuthorsController. That's in line with the naming guidelines for the outer facing contract we just talked about. Then let's have it inherit controller, that's from the Microsoft. AspNetCore. Mvc namespace. All controllers in ASP. NET Core must inherit controller. To return data we need to add an action on our controller. We want it to return an IActionResult. IActionResult defines a contract that represents the result of an action method, so that's exactly what we need. As we're getting the authors, we'll name the action GetAuthors. So in this action we should get the authors from the repository and return them, but for that we need an instance of our repository, so let's inject it through constructor injection. We get the authors, and we do that by calling into the GetAuthors method. And then we need to return these. For now let's serialize them to JSON and return that. For that we can use JsonResult. JsonResult is an action result, which formats the given object as JSON. So let's pass in our authors from the repository. Let's give this a try by building, running and sending an HTTP request with Postman. If you've downloaded the exercise files you'll notice they contain collections of requests for each module, you can import them in Postman, which will save you a bit of typing. To import them just click the Import button, drop the files in the window you see on screen, and you're good to go. Let's try out our first request. We want to get resources, so we use the GET HTTP method. Our authors should reside at api/authors. Let's send this, and that fails with a 404 Not Found status code, what happened? Well, we might want this authors resource to be at api/authors, but we didn't tell ASP. NET Core that it should be there. To fix that we must look into routing.

Working with Routing
Routing matches a request URI to an action on a controller. So once we send an HTTP request, the MVC framework parses the URI, and tries to map it to an action on a controller, and there's two ways it can do this, convention-based or attribute-based. For convention-based, of which you see an example on screen, we have to configure these conventions. We can do that by passing in these routing conventions to the UseMvc extension method. This example would map the URI values index to an index method on a controller named values controller. As you can guess from that example, this is a typical sample of something that's used when building a web application with views that return HTML. The MVC Middleware can be used for that, but for APIs the ASP. NET Core team recommends not using convention-based routing, but attribute- based routing instead. Attribute-based routing, as the name implies, allows us to use attributes at controller and action level. We provide these with a URI template, and through that template and the attribute itself, a request is matched to a specific action on a controller. For this we use a variety of attributes, depending on the HTTP method we want to match. And now we know about routing, we can think back at that slide on our outer facing contract. We learned about how to design the resource URIs, and now we can look into the correct HTTP methods, and potential payloads for our actions. Let's have a look at the ones defined in the HTTP standard, which we'll implement throughout this course.

Interacting with Resources Through HTTP Methods
Different actions can happen to resources at the same URI. For example, getting an author and deleting an author are interactions with the same resource URI. It's the method that defines the action that will happen, and depending on the method, we'll potentially need to send or get a payload. It's important to follow this standard so other components of our application can rely on this being implemented correctly. These are the methods we'll cover. First up, reading resources. The correct method for this is GET. A sample URI would be api/authors for a list of authors, and api/authors followed by the Id for the one with that Id. There is no request payload, but the response payload contains either a list of author representations, or a single author. Then there's creating a resource, POST is used for this. A sample URI is api/authors to create an author. Payload we pass in is a representation of the resource we're going to create, an author in our example, and the response payload then contains the newly created author resource. For updating resources, two options are available. The first one is PUT, which should be used for full updates. A PUT request to api/authors/authorId would update the author with that Id. The request payload is a representation of the resource we want to update, including all fields, and if a field is missing, it should be put to its default value. The response payload can be that updated author, or it can be empty, but you don't always want to fully update your resource, in fact, more often than not, you'll need partial updates to update only one or two fields instead of all of them. And that's what the PATCH method is for. The URI is the same as for PUT. The request payload is somewhat special here, it's a JsonPatchDocument, essentially a set of changes that should be executed on that resource. We will look into that in more detail once we're talking about partial updates in the module on updating resources. And just as with PUT, the response payload can be that updated author, or it can be empty. The last of the common HTTP methods is the DELETE method to delete a resource. A sample URI is, again, api/authors followed by the authorId, to delete the author with that Id. This time, both requests and response payloads are empty. But there's two more HTTP methods we'll cover. HEAD is identical to GET with the notable difference that the API shouldn't return a response body, so no response payload. It can be used to obtain information on the resource like testing it for validity, for example, to test if a resource exists. OPTIONS represents a request for information about the communication options available on that URI. So in other words, OPTIONS will tell us whether or not we can GET the resource, POST it, DELETE it and so on. These OPTIONS are typically in the response headers and not in the body, so no response payload. With that, we know about routing, and about using the correct HTTP methods and URIs. Let's continue with implementing the outer facing contract.

Demo - Implementing the Outer Facing Contract (Part 2)
In this demo we'll implement routing to complete our first action, improving on what we did in the previous demo. We're back in our AuthorsController, and we've got the GetAuthors action on screen. We know we want to respond to a GET request, so let's use the HttpGet attribute. We want this controller action to be executed when we send the request to api/authors, so that's what we pass in as the routing template. Let's build and run, fire up Postman, and let's send that same request we sent in the previous demo. The request is still on screen, so let's click Send. This time we do get back the list of authors in our database, so that's nice. We've just created our first API action that actually works. Let's go back to our controller. A controller typically contains other actions, depending on the functionality we want to expose via our outer facing contract. An example would be getting one author, creating an author or updating it. Consistency is important. For example, we want to consume all our resources by starting with api, and all the resources in this controller will start with api/authors. We can use the Route attribute at controller level for that. Like that, we only have to define this once, and not for each action, and we can now get rid of the template for the HttpGet attribute. Now the template we pass in to the Route attribute contains the plural noun, authors. That's actually the prefix of the controller, AuthorsController. We can refer to that by putting controller in straight brackets in the template. That will then match the route api/authors to the AuthorsController. But, to be honest, I'm not a big fan of that approach. If we were to have refactoring of our codes, and rename the Controller class to URI, to our authors resource, would automatically change. For APIs this isn't an advantage. Resource URIs should remain the same, and were we to refactor this controller so it has another name, all our resource URIs would change. The name of the underlying class is of no importance to the consumer, so that's something we want to avoid. Let's change it back to api/authors, and let's give this a try. Our request is still on screen, so let's click Send again, and there we go. But we're missing something here, we're returning entities, so we're actually working with an outer facing contract that works on the model used by Entity Framework, and that's not a good practice. Let's learn why that's the case.

Outer Facing Model vs. Entity Model
When we designed the outer facing contract, we learned that REST stops at that level. What lies underneath that outer facing contract is of no importance to REST. From that we already know that the entity model, in our case used by Entity Framework Core, as a means to represent database roles as objects, should be different from the outer facing model. In some application architectures there's a business layer in-between, which in turn is different from the outer facing model and the entity model. The outer facing model does only represents the resources that are sent over the wire in a specific format, but it also leads to possibilities. Take an author, for example. On screen we can see some pseudocode for that. An author is stored in our database with a DateOfBirth, but that DateOfBirth, well that might not be what we want to offer up to the consumers of the API. They might be better off with the age. Another example might be concatenation. Concatenating the FirstName and LastName from an entity into one name field in the resource representation, and sometimes data might come from different places. An author could have a field, say, Royalties, that comes from another API our API must interact with. That alone leads to issues when using entity classes for the outer facing contract, as they don't contain that field. Keeping these models separate leads to more robust, reliably evolvable code. Imagine having to change a database table, that would lead to a change of the Entity class. If we're using that same Entity class to directly expose data via the API, our clients might run into problems because they're not expecting an additional renamed or removed field. So when developing, it's fairly important to keep these separate. Let's start by creating a model class for our author that'll be used by the outer facing contract in the next demo.

Demo - Getting a Resource Collection
Now we can actually start getting our resources. Let's add model classes to be used by the outer facing contract. Let's add a new class to the Models folder. We'll name it AuthorDto. Let's open the Author entity next to this so we can compare. So on the right there's the Author entity, and on the left, our new AuthorDto class. An Author entity has an Id, FirstName, LastName, DateOfBirth, Genre and a list of Books, but as we just learned, these types don't have to have matching fields. Let's say we don't want to return a FirstName and a LastName as separate values, but instead return a concatenated value, name. so that means we'll need an Id, a Name property, which is a string, and let's say we don't need the DateOfBirth either, we just want to return the age. So we'll need an Age property, an integer, and we will copy over the Genre. All right, the entity also, possibly, contains a collection of books. We are not going to add that to the AuthorDto for now. Later on in the course we'll learn how we can work with including child collections, but for now this will do. If we compare the AuthorDto and the Author entity again, we notice something else. The entity class contains data annotations for required fields, maximum length, keys and so on. These attributes will ensure the columns in the underlying database will not be nullable in case of the Required attribute, will have a fixed length for the MaxLength attribute and so on. Attributes like these can also be used in other classes, like our DTOs, and they're very useful for validation scenarios, but the point here is that we won't implement these on the AuthorDto class, because this one is only used for one purpose, returning data. So using validation attributes or data annotations used to validate input doesn't make sense on this AuthorDto class, which is only used for returning data to the consumer. Okay, let's open the GetAuthors section on the AuthorsController again. After calling into the repository, we want to map the entities to DTOs. So let's create a variable to hold these, a List of AuthorDto, then we run through the list of entities from our repository. For each of them we create a new AuthorDto and we add it to that collection. So it's a matter of copying over the fields. We'll have to concatenate FirstName and LastName into the Name field. Genre is a simple one on my mapping, just like Id, and Age is a calculated property from DateOfBirth. If you have a look at the Helpers folder, we see there's a DateTimeOffsetExtensions class. In this, I added the helper extension method GetCurrentAge, which will calculate the current age starting from a provided DateTime offset value. So let's import this Library. API. Helpers namespace in our controller so we can use this helper method. Let's use the extension method to calculate the Age, and lastly we return the authors list. So instead of returning the authorsFromRepo variable, we return the authors variable. Let's build and run and give this a try. We're still using the same first request to get authors, so we send a GET request to api/authors, and this time we get back our AuthorDtos instead of the author classes used by Entity Framework Core. But this as well can be improved upon. Let's have a look at the code in our action again. This mapping code here, well, it's not very great. Sure, it's okay for a few fields, but just imagine having to write code like this for classes with 20 properties, of which 5 are collection properties on different classes with, again, 20 properties. That's going to become cumbersome quite fast, and more importantly, code like that is typically very error-prone. But luckily there's another option, using an object mapper. One of those, the most popular one in the. NET world, is AutoMapper. Let's look into that in the next demo.

Demo - Introducing AutoMapper
We've got some manual mapping codes etween entities and DTOs. In this demo we'll get rid of that by using AutoMapper. AutoMapper can be found on NuGet, so let's open the NuGet dialog and look for it. Let me type in automapper, and if you look at the description, we see that AutoMapper is an object to object mapper. Object to object mapping works by transforming an input object of one type into an output object of a different type. Well that sounds exactly like what we're looking for. Let's install this. (Installing) And there we go. The first thing to do is configure the mappings. We have to tell AutoMapper how it should map between our entities and our DTOs. This configuration should be created once, and instantiated at startup. We can do that in the Configure method on the Startup class. For the time being we only need to create one mapping, from the Author entity to the AuthorDto. We will revisit this throughout the course when we work with additional DTOs and entities. To create a map, we can call the AutoMapper. Mapper. Initialize method. It accepts an action on a mapping configuration as a parameter, so let's create a map from the Author entity to the AuthorDto. For that we call CreateMap on that mapping configuration parameter. We provide the source as the first type, and the destination as the second type. AutoMapper is convention-based, it will map property names on the source object to the same property names on the destination object, and by default it will ignore null reference exceptions from source to target. So if the property doesn't exist, it'll be ignored. But we have some additional work. We need to ensure that FirstName and LastName are concatenated into the Name property, and we also need to calculate the age. This means we need projection. Projection transforms a source to a destination beyond flattening the object model. So we must specify this through a custom member mapping. To do that we call into ForMember, we pass it a function to get the destination, in other words the name, and then we tell AutoMapper how to create this Name property value. We want it to be mapped from the source's FirstName and LastName. So we call into MapFrom, and we pass in FirstName and LastName. We need an additional ForMember statement for the age, but let's import the helpers namespace first so we can use that GetCurrentAgeExtension method on DateTime offset, and let's add the projection. So we call into ForMember again and pass in a function for the destination object, and one that states that we have to get the age by calling into GetCurrentAge on the source's DateOfBirth property, aand with this we've told AutoMapper to create a mapping from the Author entity to the DTO. Let's open the AuthorsController again. We're back in the GetAuthors section, and we can now remove this foreach loop we wrote in the previous demo. To effectively map we call Mapper. Map, we pass in the type we want to get back, in our case that's an IEnumerable of AuthorDto, and then we pass in the authors from the repository. AutoMapper will now use the mapping we just created to map each item in the source list to an item in the destination list. Let's give this a try. We've still got that request saved, so let's send it again and we got back the exact same result, but we didn't have to write the mapping code ourselves. Let's continue with learning how to get a specific resource rather than a collection resource.

Demo - Getting a Single Resource
In this demo we'll learn how to get a single resource, so let's add a new action to our controller, GetAuthor. It also returns an IActionResult. As we learned, a good practice when designing the URI for such a resource is using the plural noun followed by a forward slash, followed by the Id. The Route attribute at controller level already takes care of that plural noun, authors. Our HttpGet attribute should thus only contain the id. The forward slash between authors and Id is added automatically. That Id is dependent on the resource you want to access. It's a parameter that changes, so to reflect that we surround it with curly brackets. We need access to this parameter in our action, and we can get that by adding it as a parameter to the action signature with the same name we gave it in the Route template. In other words, we accept the parameter Id of type Guid. Then onto the implementation of this action. First we call into the repository to get our author. That's the GetAuthor method, and we pass in the Id from the URI. Then we map this authorFromRepoto an AuthorDto using AutoMapper, and lastly we return a JsonResult, passing in the author, which is of type AuthorDto. All right, let's give that a try. Let's build, run and fire up Postman again. I can close this one from the previous demo and we've got a new request here, GET Author. It's still a GET request, now the last part of the URI is actually a GUID of an existing author. Let's Send this and we get back George RR Martin. What's also important is that we get this response back with a status code. That's a part of each response message. In this case that's a 200 OK status code. But something is missing. Let's try getting an author that doesn't exist. I've got another request here for that, so that contains some random GUID at the end, and we also get back a 200 OK status code, with a null value as its response body. That doesn't seem right. The API consumer tried to get a resource that doesn't exist, so he made a mistake, and if a consumer of an API makes a mistake, he should know he made a mistake, as that allows him to correct it. In this specific case we'd expect a 404 Not Found because the author wasn't found and that brings us to status codes. Let's have a look at that.

The Importance of Status Codes
Status codes and when to use them are part of the HTTP standard, and it's really important to get these right because these status codes are the only thing a consumer of the API can inspect to know if a request worked out as expected, and if something went wrong, whether it's the fault of the consumer or of the API itself. We're currently returning JSON result in our code, which sends back a 200 status code by default, but if we keep doing that for each and every request, the consumer of the API would assume this request worked out as expected, even if it went wrong. Remember that consumers of the API are typically non-human, all they can inspect is the status code. They can't read out and interpret error messages. In the example code we just wrote, trying to get an author that doesn't exist is a client mistake. That also means he can correct it and try the request again, and other times it's the server's responsibility, for example when the database is unavailable. The consumer of the API cannot correct this because the mistake isn't his. There's a lot of status codes, and an API doesn't necessarily have to support all of them, but let's look into a few common ones. There's five levels of status codes. Level 100 status codes are informational and weren't part of the HTTP 1 standard. These are currently not used by APIs. The level 200 status codes mean the request went well. The most common ones are 200 for a successful GET request, 201 Created for a successful request that resulted in the creation of a new resource, and 204 for the successful request that shouldn't return anything. Level 300 status codes are used for redirection, for example, to tell a search engine a page has permanently moved. Most APIs don't have a need for these. Then there's the status codes that tell the consumer he did something wrong, level 400, client mistakes. Four hundred means a Bad request, the request you, as a consumer of the API, sent to the server is wrong. For example, the JSON the consumer provided can't be parsed. 401 Unauthorized means that no, or invalid authentication details, were provided, and 403 Forbidden means that authentication succeeded, but the authenticated user doesn't have access to the requested resource. 404 Not found means that the requested resource doesn't exist. Then we have 405, Method not allowed. This happens when we try to send a request to a resource with an HTTP method that isn't allowed. For example we try to send a POST request to api/authors, when only GET is implemented on that resource. 406 means Not acceptable, and now we're diving into our presentation media types. For example a consumer might request the application/xml media type, while the API only supports application/json and it doesn't provide that as a default representation. This is one we will be covering in this module. 409 Conflict then means that there's a conflicted request versus the current state of the resource the request is sent to. This is often used when we're editing a version of a resource that has been renewed since we started editing it, but it's also used when creating a resource, or trying to create a resource that already exists. Two more of importance here, 415 means Unsupported media type, and that's the other way around from the 406 status codes. Sometimes we have to provide data to our API in the request body, when creating a resource for example, and that data also has a specific media type. If the API doesn't support this, a 415 status code is returned. And lastly, 422 stands for Unprocessable entity. It's part of the WebDAV HTTP Extension Standard. This one is typically used for semantic mistakes, and semantic mistakes, well, that's what we get when working with validation. If a validation rule fails, 422 is what's returned. And lastly there's level 500, server mistakes. Often only 500 Internal server error is supported. This means the server made a mistake and the client can't do anything about it, other than trying again later. That's a lot of status codes, no reason to learn them by heart, we'll encounter all of them during this course. Some mistakes do happen, and these mistakes are then categorized into two categories, faults and errors. Errors are defined as a consumer of the API, like a web app, passing invalid data to the API, and the API correctly rejecting that data. Examples include invalid credentials or incorrect parameters, in other words, these are level 400 status codes and are the result of a client passing incorrect or invalid data. Errors do not contribute to overall API availability. Faults are defined as the API failing to correctly return a response to a valid request by a consumer. In other words, the API made a mistake, so these are level 500 status codes, and these faults, they do contribute to the overall API availability. Let's see how we can implement these in our API in the upcoming demos.

Demo - Returning Correct Status Codes
In this demo we'll make sure the two actions we've got return the correct status codes. First let's look at the GetAuthors section, the one that returns a collection of authors. By default JsonResult returns a 200 OK status code. That can be changed, but there's built-in methods at the controller level that return an IActionResult with a specific status code. For 200 that's the Ok method. So let's replace JsonResult with Ok. Another advantage is that we no longer necessarily return JSON now, more on that when we cover content negotiation. When the authors collection is empty, we don't return a 404 Not found, because the authors resource exists, it's just an empty collection. Let's immediately continue with getting a specific author. In this case we do need to return a Not found if the author isn't found, because that means the passed in id, which is part of the resource URI, reflects a resource that doesn't exist. So we need to check for this. There's different approaches we can take here. There's an AuthorExists method on the repository. That'll execute an additional database call. If the author doesn't exist, we can return NotFound, and after that we can then get the author from the repository, which is the code we already have, and return an Ok instead of JsonResult, after the author has been mapped, of course. But this does mean an additional call. As a general rule what I like to do for these Not found checks is get the author and do a null check if we need the entity afterwards, and if we don't need the entity afterwards, I tend to use an exists check, like AuthorExists. Checks like these may mean additional code, even additional round trips to an underlying database, but even then these checks are necessary when building a RESTful API, as we just learned. Okay let's build our own and let's give this a try. First let's GET the Authors resource again, that's that first request we already executed a few times. We click Send and we get the list of authors with a 200 OK status code, just as before. Then let's try that request to GET an unexisting author. Let's click Send, and this time we get a 404 Not found. Our additional check works, and if we get an existing author we, of course, get that existing author with a 200 OK Status Code. With that we've covered client mistakes, errors. We'll encounter more of these in the upcoming modules. In the next demo we'll look into how we can handle faults.

Demo - Handling Faults
Let's talk about server errors, faults. Let's start by throwing an exception to see what happens. We're in the AuthorsController and when we GetAuthors, we'll just throw a random exception. And let's give this a try. Let's Send the request to GET the Authors, we hit the breakpoint as we're throwing an exception, let's continue, and we get back a 500 Internal Server Error, that's what we need. ASP. NET Core will automatically return this when an unhandled exception happens. If we have a look at the Preview tab, we see a developer-friendly exception page. So do we still need anything else to handle faults? Well yes, we might want to execute some additional logic when an exception happens, like logging a custom message or some information for the system admins. Let's open the AuthorsController again. There's a few ways we can handle these exceptions. Let's start with one we're all undoubtedly familiar with, try-catch. We want to make sure a 500 Internal server error is returned when something happens that results in our API not being able to fulfill the request, like our random exception we're throwing. So when we catch this exception, we return a status code 500. For that we can use the status code helper method. We pass in the status code and an optional message. That message, an object actually, will be serialized into the response body. What's important here is that we don't want to pass in the exception itself, let alone the stack trace. Passing in the stack trace and sending it to the consumer of the API means we would be exposing implementation details to that consumer, and that's a security risk, but it's also useless. The consumer of the API has no use for that stack trace, as this is a fault an exception is not responsible for, and can't do anything about. So we return a generic message or no message at all, as consumers of the API are often machines rather than humans. Let's get rid of this throw statement. Let's send that statement again. We get back a 500 Internal Server Error, and our custom generic error message. But is this a good approach then? This means we'd essentially have to write try-catch statements for each action, and maybe logging statements. Moreover, even though we're in the development environment, we no longer see the developer-friendly error page. That's because we handled the exception when we called it by returning that 500 Internal Server Error. When running in production, you don't want to return a stack trace, but when you're developing it can be very useful. There's another option, global exception handling. In fact, what we saw before we wrote a try-catch statement, well that is global exception handling at work. Let's open the Configure method in the Startup class. There's two pieces of middleware here that handle these exceptions, the exception handler middleware is used when we're in a production environment. An approach I like to take to handle faults when building a RESTful API is to let the developer exception page as-is during development. That said, when not running in a development environment, I like to add a bit of configuration to the exception handler middleware to have it return a generic error message. So this is the exception handler middleware, and we can configure that by passing in a lambda that returns an action on IApplicationBuilder. We can then call Run on that appBuilder. What that will do is add a piece of code, which we're going to write, to the request response pipeline. So what we want to do is make sure that the status code is 500, and that we write out a generic error message as the response body. So let's write a piece of code. We need an action on the context here, and on that context we use context. Response. StatusCode to set the status code, and then we write out our error message by calling into context. Response. WriteAsync. That one is from the Microsoft. AspNetCore. Http namespace. We pass in that same generic error message and we're done. We learn how to get the actual error and log it later on. Now let's remove the try-catch we added to our GetAuthors method, as that's just been replaced by the code we wrote. Let me format this so it looks a bit better, there we go. So now when the exception gets thrown it's the custom code we wrote in our middleware configuration that will take care of sending the correct response. There's just one other thing we have to take care of, we've got to make sure we're not running in the development environment anymore. As we learned, when in the development environment, the developer- friendly error page will be shown. Let's open the API's project properties, and in the Debug tab we can set the ASP. NET Core environment variable to Production. Now to make sure this has an effect, the web server must be restarted. We're using IIS Express built-in to Visual Studio and a quick way to restart that is by restarting Visual Studio, so let's do that. That should be it, let's build, run, and switch to Postman. Let's send that request to api/authors again. Our exception is being thrown so we break into debug. So far so good, let's continue, and there we go. We get back a 500 status code with our custom message, but this time handled on a global level. Let me just remove this exception again so we can continue building our API. As mentioned, logging these errors is coming up in another module, so for now we're done. And with that, we can now continue with building the outer facing contract in the next demo, implementing parent/child relationships.

Demo - Working with Parent/Child Relationships
Up until now we've been working with authors in our API, but for each author there might be books. A book always belongs to an author, so we want to expose it through a URI that reflects this. Let's add a new controller first, BooksController. So we add a new class, and give it a name, BooksController. Let's have it inherit controller and let's make sure the repository is injected, for this we again use constructor injection, and there we go. Then let's add an action, GetBooksForAuthor, returning an IActionResult. We're still getting data so we should choose the HttpGet attribute, but what should a template for the route look like? We want to reflect that books are children of an author, and we do want to reflect that in the URI, so what we want is a template like api/authors/authorId and books. We can then accept the authorId as a parameter for the action so we have access to it. As before we're going to add other actions to this controller throughout the course, but one thing they'll all have in common is that they'll all start with api/authors, an authorId parameter and books. So we can use the Route attribute at controller level to avoid having to re-type this for every action. Using the authorId parameter in the template for the Route attribute will still make it available for our actions, as long as we accept it as a parameter. The first thing we want to do when we enter our GetBooksForAuthor action, is check if the author exists. We can't get books from an unexisting author, so a request to search URI means the resource can't be found. So that warrants a NotFound status code. Then let's fetch the books for this author. For that we can use the GetBooksForAuthor method from the repository, it accepts the authorId as a parameter. Then we'll have to map these to a collection of DTOs. We don't currently have a book DTO yet, so let's add one. Just as the AuthorDto, we want to add this to the Models folder. We'll name it BookDto and let's open the Book entity and put it next to this one. Let's add the Id, title and description to our DTO already. Then there's that author entity. We could add an AuthorDto here, letting AutoMapper take care of the mapping for us, but in this case that would result in the same AuthorDto being returned for each book. That's redundant information, and it will hurt performance sending it over the wire for each book. Moreover, later on we'll learn how to include children when requesting a parent, i. e. returning the books when getting an author. If you had that same author again, and again, for each book, while the author possibly includes a collection of books, we will run into circular reference errors, so we do not include the author here. We can, however, safely include the AuthorId. And let me close that Book entity, and let's spin the Solution Explorer again, back to the BooksController. Now we want to map our list of books from the repository to a list of BookDto. We can use AutoMapper for this. We call into Mapper. Map, passing in the destination type, which is an IEnumerable of BookDto, and as a parameter we pass in the booksForAuthorFromRepo variable, in other words, our entities. Let's just add a using AutoMapper statement. Now this is the first time we're mapping a book entity to a BookDto, so we will have to configure this mapping. We've done that before, so we know how to do that. Let's open the Configure method in the Startup class. So we have an additional CreateMap statement from a Book entity to a BookDto. There's no projections we have to configure for a book, so this this CreateMap statement is sufficient. One thing that's left is actually returning our mapped enumerable of BookDto from the BooksController's GetBooksForAuthor action. We do that with an Ok status code. All right, let's give this a try. We're back in Postman and I've got a request lined up here, GET Books for Author. The URI starts with api/authors, followed by an authorId, followed by books. Let's send this request, and that indeed returns the books that are in our database by, in this case, George R. R. Martin. Let's input an unexisting authorId. This GUID here is just a random GUID, so this should result in a 404 Not Found, and that's indeed the case. While we're at it, let's ensure we can also get a specific book, one book. Back to the BooksController. Let's add a new action here, GetBookForAuthor, and it accepts an authorId just as before, but this time that's not sufficient. We also need the bookId, so that's what we input as a template for our HttpGet attribute, and we add it to the list of parameters. Combined with the template for the Route attribute, this will result in a URI that adds the Id of a specific book to the URI of the collection of books for an author. The first thing to check is if the author exists. We can just copy/paste that from our other action. Then we will also have to check if the requested book exists for this author. If it exists we're going to use it to map to a BookDto, so this time we don't call into a method like BookExists, instead we fetch the book. For that we call into the GetBookForAuthor method on our repository, passing in the authorId, and the Id for the book. If it's null we return a Not Found, if it isn't, we can continue. We map the book entity to a BookDto and we return it by passing it in to the Ok method. All right, let's build and run and give this a try. We're in Postman and I've got a request to GET a Book for a specific Author here. As we see on screen, the URI now additionally contains an Id for a book. Let's click Send, and we get back "The Winds of Winter", which I sincerely hope will be released by the time you're watching this course. If you try an unexisting author we should get a 404 Not Found, so this is a URI where the first Id is just a random GUID, and that's indeed the case. Now let's try one more request, getting a book for an author that exists with an invalid Id, so the book itself does not exist. We click send, and we get back a 404 Not Found. Now let's get a book again, and look at what we actually get back. We've been getting resources throughout this module, and all of this looks good, right? But let's set this off against one of our constraints, manipulation of resources through representations. That's the constraint that's stated that when a client altered a presentation of a resource, including any possible metadata, it must have enough information to modify or delete a resource on the server, provided it has permission to do so, and in the next module we'll allow creating and deleting an author. So if the consumer of the API gets the response we see now, does he have enough information to modify or delete the author? Well, not really, what should be in the response to allow for that, at a minimum, is the resource URI. We already include an Id, and often that's considered enough. From the Id a consumer can create a URI, but if you think about this a bit further, it isn't completely correct. An Id alone isn't what identifies the resource, it's the URI that identifies the resource, and the resource URI is part of the request, but it's not part of the response. So to adhere to this constraint we should include the URI in each representation if update or delete is allowed. We could do that now already, it's just a matter of adding an extra field and filling it up with the URI, but it's also not completely correct, and we're just getting started. There's a much better way of handling this than including the URI, and that's through HATEOAS. So we're going to leave this as is currently, and we'll solve this issue once we get to HATEOAS. I just wanted to already mention this at this point, if you do not want to implement HATEOAS, make sure to at least include the resource URI in the response if updating or deleting of the resource is allowed. Okay, on to the last part of this module. We're getting data, but we're only getting it in one format, JSON, even though most APIs we see in the wild these days return their representations in JSON, that's not always the case. So let's see how we can support different media types.

Formatters and Content Negotiation
When we think about a RESTful API these days, we often think of JSON, but as we learned in the previous module, JSON has nothing to do with REST per se, it just happens to be the format of the resource representation, and that implies that a RESTful API can also work with other representation formats, like XML, or a proprietary format. So that also means that we have to be able to let our API know what format we want to get back, and that brings us to one of the key concepts when working with HTTP requests and responses, and that's content negotiation. That's the process of selecting the best representation for a given response when there are multiple representations available, and it's important. If you're only building an API for a specific client, it might be okay to always get back representations in JSON format, but if we're building an API for consumption by multiple clients, some of which we have no control over, chances are that not all of these clients can easily consume JSON representations as a response. Some might be better off with XML or another format, so how is this done? Well that's what the Accept header of the HTTP request message is for. An API consumer should pass in the requested media type like application/JSON or application/xml through this header. For example, if an Accept header has a value of application/json, the consumer states that if our API supports the requested format, it should return that format. If it has a value of application/xml, it should return an XML representation, and so on. We'll learn later on in the module on HATEOAS and media types that media types like these have a more important role in REST than what we're covering now, but let's not get ahead of ourselves. If no Accept header is available or if it doesn't support the requested format, a lot of APIs default to a default format, like JSON. But I'd suggest to avoid that, especially in the latter case. We could argue that we'd allow our API to serve up responses for requests without an Accept header with the default representation, but if the client requests a specific format, that means he'd also expect to get something back that he can parse. If the API returns its default format, if the requested format is missing, we'd likely end up with a client that cannot parse the response anyway, so in those cases a 406 Not acceptable status code is warranted. After all, that's what this code was made for. Needless to say that it's also a best practice to always include an Accept header. Okay, so now how do we do that in ASP. NET Core? Well this is about output, the format of the resource representation. ASP. NET Core supports this with output formatters. The consumer of the API can request a specific type of output by setting the Accept header to the requested media type, but if there's output there's also input. We're running a bit ahead of ourselves with that, we don't provide input in a request body yet, that's what we'll do once we start creating resources in the next module, but this does seem like a good place to mention this already, because next to output formatters, ASP. NET Core also supports input formatters. An input formatter deals with input, for example, the body of a POST request for creating a new resource. For inputs the media type of the request body is identified through the content-type header. These types of headers can be seen as part of the self-descriptive message constraint, which states that each message must contain enough information to process it. In case of the Accept header, it tells the API what format to return, so how to process the message and return a response. While the content-type header will tell the API the format it'll have to parse the body to to be able to process the message. Let's check that out with a demo.

Demo - Working with Content Negotiation and Output Formatters
In this demo we'll implement an output formatter. By default only the JSON input and output formatters are used. On screen is that GET request for an Author we still have from a previous demo. If you look at the headers, we see there is none, so we never passed in an Accept header, yet if we send the request, we do get back JSON. That's because our API defaults to that. If we add application/json as value for the Accept header, which we always should, we also should get back JSON. So let's send this again, and that is indeed the case. Now let's change this, let's try and send a request with an Accept header of application/xml. We still get back a JSON representation. That's already a bit worse, so we actually have two things we want to fix here. First, if the requested media type isn't supported by the API, we shouldn't default to the default type, as the consumer probably won't be able to parse it anyway. So sending a request with an Accept header of application/xml and returning JSON, that's not okay. And the second thing we do want to fix is that we want to ensure we do support XML. So let's start by handling unsupported media types, and for that we'll have to open the ConfigureServices method of our Startup class. We've got this services. AddMvc statement here, which adds MVC services to the container so they can be used for dependency injection. But here we can also configure these services. The AddMvc method accepts a setupAction, used to set it up. So let's add this, and if we look at what we can configure, we see there's a ReturnHttpNotAcceptable property. It's a Boolean, if it's false the API will return responses in the default supported format, if an unsupported media type is requested, and by default it is false, which explains why we get back JSON even though we asked for XML. So we're going to change that to true. Let's give this a try. Let's Send the request without an Accept header again. We can just unmark the Accept header, Postman will no longer send it if that's the case. So let's click Send and we get back JSON, so far so good. Same for sending it with an Accept header of application/json, let's click Send, and indeed, the same result. Now let's change it to application/xml. Let's click Send, and this time we get back a 406 Not Acceptable status code, as we should. So far, so good. On to the next part, supporting an additional formatter. We do want to support XML so let's open the ConfigureServices method on the Startup class again. On the setupAction we can find a list of OutputFormatters. To this list we can add a new one. By the way, if you're wondering how ASP. NET Core chooses its default formatter if no Accept header is added to the request, it's always the first one in this list. So by manipulating this list, for example by adding a formatter at the end, inserting one in the beginning, or removing a formatter, we can define what types of output formatter our API supports, and what the default output format is. We want to support XML output, not as default, but as a possibility. So we'll have to add an XML formatter to the list. The Xml formatters are part of the Microsoft. AspNetCore. Mvc. Formatters. Xml assembly. We're looking for the XmlDataContractSerializerOutputFormatter. Let's add that dependency. What's important when adding references to ASP. NET Core-related packages is that we select a version that matches the version of the ASP. NET Core SDK we're using. There we go. And let's add the using statement. As you know from the first module, thanks to the microsoft. aspnetcore. all metapackage reference, adding this reference isn't required for ASP. NET Core 2. 0. Now let's build and run again. Let's send that request with application/XML as the value for the Accept header again, and this time we do get back XML. If we send it without an Accept header, we should get back JSON, and if we send it with an accept header, but with a media type we don't support, like application/gzip, we get back a 406 Not Acceptable, just as expected. And this leads us nicely into the next module, where we'll learn how to create and delete resources. There we will use the content-type header for providing the media type of a request body. But first let's check out the module summary.

Summary
We started this module by looking into the outer facing contract, a combination of resource identifiers, HTTP methods, and optionally, payloads. As far as resource identifiers are concerned, there's no standard for naming resources, but pluralized nouns are most often used. We should convey meaning when choosing them, don't name an authors resource, orders. On top of that, a model often has hierarchy, and that should be represented in the identifiers, but most of all it's important to stay consistent, yet sometimes RPC-style calls don't easily map to pluralized resource nouns as names. It's okay to divert from those pluralized nouns for those exceptional cases. HTTP methods are the second part of our contract. We learned GET is for getting resources, POST for creating them, PUT for full, and PATCH for partial updates, and DELETE for deleting resources, but there's more. HEAD is identical to GET, but it doesn't contain a payload in the response body, it's used to obtain information on a resource. And OPTIONS tells us what methods are allowed on a specific resource. On a technical level routing helps with this, it matches a request URI to an action on a controller. And the last part was the payload. Resource representations are sent to the API and returned from it as a specific media type. The Accept header tells the API what representation to return in the response body, and the content-type header tells the API the format of the representation in the request body. Requests can work out as expected, but they can also go wrong, that's where the status codes come in. This is what a consumer of the API can inspect to know if a request worked out as expected, or if something went wrong, and if something went wrong, whether it's the fault of the consumer of the API, or the API itself. Level 200 status codes mean the request went well. Level 400 status codes mean the consumer made a mistake, and that translates to errors. Level 500 status codes mean the mistake is the server's responsibility, which translates to faults. What's very important is that the model used to represent our resources, the outer facing model, is conceptually different from the business model or the entity model. Keeping these models separate leads to more robust, reliable and evolvable code. During the demos we essentially implemented part of that contract, according to RESTful principles. So everything we learned in this module was held together by that outer facing contract, as it's so important, but we've only just started. We only learned how to get resources. Time to learn how to create and delete them in the next module.

Creating and Deleting Resources
Coming Up
Hello, and welcome to the Creating and Deleting Resources module from the Building RESTful API with ASP. NET Core course at Pluralsight. My name is Kevin, and I'll guide you through this module. We now know how to get data from our API in a RESTful manner. Up next, we continue with implementing the outer-facing contract. We'll create a single resource, a child resource, and we'll learn what we can do if we want to create a resource and its children in one go, but we'll also cover a not so obvious case that often raises a few questions on how to do it, creating a list of resources in one go. Creating resources means using the content-type header to signify the media type of the request body, as we learned at the end of the previous module. We learn how to use additional content types for input together with input formatters. After that, we learn about deleting resources, we'll delete a single resource, a resource and related resources, and we'll look into whether or not deleting a collection resource is a good idea. But before that, we must talk about two principles that will help us understand why specific methods should be used for certain actions, method safety and method idempotency. Let's look into that.

Method Safety and Method Idempotency
So in this module, we'll create and delete resources. It's the first time we'll use HTTP methods other than get, that makes this the ideal time to look into two important principles in the HTTP standard, method safety and method idempotency. A method is considered safe when it doesn't change to resource representation. For example, get and head are considered safe. Mind you that doesn't mean that other types of manipulation can't happen as a result of performing a get request. Behind the outer-facing contracts, the API might update a fields in related tables, but the resource representation itself shouldn't change, and if those side effects happen, these weren't requested by the consumer of the API. A method is considered idempotent when it can be called multiple times with the same result. In other words, the side effects of calling it once are the same side effects that happen when calling it multiple times. Let's have a look at how this maps to the HTTP methods we looked into when talking about routing. Get is both safe and idempotent. It doesn't change the resource representation, and when you call it multiple times, the same results are returned. Options and head follow the same logic, but post is none of these, it causes changes in resource representations because it creates them, and if we call post multiple times, multiple resources should be created. Delete isn't safe, the resource representation changes as the resource is deleted, but it is idempotent. Deleting the same resource multiple times has the same result as deleting it once. Put for full updates isn't safe either. If you update the resource, its representation might change, but it is idempotent. Updating the same resource multiple times results in the same representation as updating it once. And lastly, patch for partial updates is neither safe nor idempotent. We learned that through patch it's easy to, for example, add items to an array, which results in different representations if you do that multiple times, and this is not just some theory, it helps us decide what we are allowed to do for each method. We're building a RESTful API that uses these HTTP methods. It's a standard, so we should correctly implement it so other components we might use can rely on this being correctly implemented. We already learned which method to use for what type of action when we learned about routing. From what we learned here, we now know the reasoning behind why we should, for example, not use get to create a resource, that would make it both unsafe and non-idempotent, which is a direct violation of the standard. This table here will help us decide what we should use to implement the functionality we'll want from our API. We'll notice, for example, that post is indeed used to create resources, but put can in some specific cases also be used for that. It's allowed to be unsafe as long as it stays idempotent. So if you ever wonder what method to use for which use case, this is a table you'll want to go back to and check, but let's start with that non-idempotent unsafe method, post. In the next demo, we'll create our first resource.

Demo - Creating a Resource
In this demo, we're going to create a new author with post. In the next one, we're going to add a new book for an existing author. These two cases allow us to look into additional checks and status codes to return when creating child resources, but first let's open the AuthorsController. And let's add a new action, createAuthor, which returns an action result. The correct method to use for this is post, so we'll decorate the action with the HttpPost attribute. To route template, as we learned, should be api/authors, and if you scroll up, we see that's already the case. Each section in this controller has a route template that starts with api/authors, as we pass that in as template for the route attribute at controller level. Then we need to get the inputted author, that'll be provided in the request body. That means we can use the FromBody attribute for that. If you use this in our parameter list, we signify that the parameter should be de-serialized from the request body, but what should it be de-serialized to? So we have this AuthorDto for output, but for post, we need a Dto for input, and that should be a different type than this AuthorDto. This AuthorDto contains an ID, and we're creating an author in a system for which the responsibility of generating the Id is at level of the API. So the Dto shouldn't contain that Id field. And there's more, we return the age of the author, but in the backend, we store the date of birth, so we do need that date of birth as input. And name, well, we store first name and last name fields, and not a concatenation of those, as is the case in our AuthorDto class. So let's create an author for CreationDto. We'll give it a FirstName field, LastName, a DateOfBirth, and the Genre. Mind you there are system where the properties on the class used for output are exactly the same as those on the class used for input, but even in those cases, I'd suggest to keep these separate. It leads to a model that's more in line with the API functionality, make change already factoring afterwards much easier, and when validation comes into play, you typically want validation on input, but not necessarily on output. So I suggest to use a separate Dto for creating, updating, and returning resources. Back to the create author action on our controller. We can now accept the parameter of type AuthorForCreationDto in the action signature. The first thing we need to check is if the input provided in the request body was correctly serialized to AuthorForCreationDto. If that isn't the case, the author value will be null, that means the client made a mistake, so we return a 400 BadRequest. If that checks out, we can map the AuthorForCreationDto to an author entity, and add it to the database finder repository. So we'll have to create a new mapping first. We do that in the configure method of the startup class. We've already got a few maps created here, so let's add another one, this one from Models. AuthorForCreationDto to Entities. Author. Alright, back to the action on our controller. We call Mapper. Map, passing in Author as the destination type, and our Author variable as the source object. And after this, we can call into the repository. Our repository contains a method, AddAuthor, which accepts the authorEntity. This is the first time we are adding an item to the database, so this might need some additional explanation. At this moment, the entity hasn't been added to the database yet, it's been added to the DbContext, which represents a session with the database. To persist our changes, we must call save on the repository, and if that's save fails, we should return a 500 internal server error. This save methods returns a Boolean, true or false, and we should see the repository has a bit of a black box. In essence, the controller doesn't know about the implementation, so it might contain exception handling code, it might not, it all depends on the provided implementation. So, I do like to treat it as a black box, but we do know what we want to return, a 500 internal server error with a generic error message. We only return a generic error message, because the consumer of the API really doesn't need to know what exactly went wrong, it just needs to know that it's not its responsibility. Our generic exception handler will not catch this, as save doesn't throw an exception, but we can use the StatusCode method for that, passing in the StatusCode. The StatusCode is 500, and we provide a generic message. But we already configured the exception handler middleware in the previous module to return a 500 internal server error with a generic message if an unhandled exception occurs. So another option is to throw an exception from the controller, and let the middleware handle it. Now is this a good approach or a bad approach? Well it's, it kind of depends. Throwing exceptions is expensive, it's a performance hit, so that would lead us to returning the StatusCode from the controller as a best practice, but on the other hand, that also means that we'll have code to return 500 internal server errors in different places, on the global level and in the controller itself. At this moment, that's not too much of a problem, but once we start implementing logging, that would also mean we'd want to provide logging code on each StatusCode 500 we return. I've seen both approaches, and there's something to be said for both. In this case, we're going to have the middleware handle all our responses that warrant a 500 internal server error. That'll come in nicely when we need to implement logging for these types of StatusCodes later on. We'll only have to write that code in one place, being at the configuration of the exception handler middleware. So let's leave it at this, after this, we can map the results. After the author has been saved, the authorEntity will contain the ID. That leaves us with one question to answer, what should we return. In case of a successful post, we should return a 201 created response. For that, we can use the CreatedAtRoute method. This method allows us to return a response with the location header, and that location header, that will then contain the URI where the newly created author can be found. So the first thing we need to pass into this method is the route name that's going to be used for generating the URI. And if we scroll up a bit, we see that it should refer to the action to get a single author, that's our GetAuthor action. So, let's give this a name we can refer to, say GetAuthor, there we go. And we pass in GetAuthor as the first parameter of our CreatedAtRoute method. Now to get an author, we need the author ID. We need to pass that in as a route value, so the correct URI can be generated containing that ID. To do that, we pass in an anonymous type, and we give that anonymous type one field, ID, which is the name used in our route template, and we give it a value of authorToReturn. Id. And lastly, we want to pass in the actual authorToReturn Dto. This one will get serialized into the response body. Alright, let's give that a try. We're going to create a post request to API/authors. The request body should contain the payload of the request, the author for creation representation. So we pass in a firstName, a lastName, a dateofBirth, and the Genre. By the way, if you happen to look for a good thriller to read, have a look at what James Ellroy has been writing. The payload is in a specific format, in our case JSON, so we need to let our API know this, so it knows how to de-serialize it. For that, we need to provide a content-type header. Through that content-type header, we pass in the media type of the request body. In our case, that's application/json at the moment. We also pass in an accept header as we learned in a previous module. Alright, let's send this request, and we get back a 201 created with a created resource representation in the body. Notice that this now contains an ID, and also the age inside of the date of birth, and let's have a look at the headers. Here we see that location header we talked about, so let's send the get request to that. And we indeed get back James Ellroy, our newly-created author. As we just learned, post is an unsafe method, it changes a resource representation from nonexisting to existing. Let's try and send this exact same request again. Let's send it, we get back a 201 created, so that means that our author has been created again. We've now got two James Ellroy's in our database. Let's get that list of authors, and here we see that James Ellroy has been added to our database twice. So post is not idempotent, we cannot send that same request twice, and have the same result as only sending it once, because we now have two authors instead of just one. So now you've got a bit of unnecessary data in our database, but don't worry about that, as we learned in the first module when we looked at the starter solution, I created the seed procedure in a way that ensures the database is cleaned and refilled whenever the application restarts. Now, what would happen if we provide invalid input? We've got a post request to API authors that contains an invalid value for the dateofBirth. Then we set a breakpoint, and let's send that request. We hit the breakpoint, let's have a look at this parameter. We see that our author variable is now null, and that makes sense because the payload couldn't be de-serialized into an author for CreationDto. Let's continue, and we get back a 400 Bad Request, that's our null check at work. And with that, we created our first resource. Onto the next demo, creating a child resource, a book for an author.

Demo - Creating a Child Resource
In this demo, we'll create a book for an author. Much of this is the same as in the previous demo, but there's a notable difference. We're implementing a RESTful system that wants to adhere to the HTTP standard, which means we should correctly return HTTP StatusCodes. In this case, there's an extra possibility, and moreover, posts like these introduce the possibility of unexpected mistakes. Let's see what that's about. We're in the BooksController, and let's add a new action here, CreateBookForAuthor. It returns an action result, just as in the previous demo, we use the HttpPost attribute. The route template should signify that we're creating a book for an author, so it should be something along the lines of API/authors/ the author ID/books, and that's already the case, as we've used that template for the route attribute at controller level. In the parameter list, we should then accept the authorId, and that should then be followed by an object of a type you can de-serialize the payload to. So let's add that class and name it BookForCreationDto. Let's have a quick look at the BookDto we use when getting books. This one contains an Id, a Title, a Description, and the AuthorId. We already know from the previous demo that we don't want the Id in our new class, as the server is responsible for choosing the URI, but what about that AuthorId, there's already an AuthorId in the URI, so if we allow the AuthorId in the payloads, well, we might end up with an issue we want to avoid, and that issue is that a post to the book's resource for author A would create a book for author B. Let's go to our BookForCreationDto. There's two ways to tackle this issue. One is adding the AuthorId in the BookForCreationDto. But that would mean we have to check if that AuthorId matches the AuthorId from the URI. The second approach is not sending over the AuthorId in the request body, and thus not adding it as a property on our BookForCreationDto. In my opinion, that's the cleaner approach, so we only add a title and a description to the BookForCreationDto. Back to our BooksController. We can now accept a BookForCreationDto. We again use the FromBody attribute to signify that it should be de-serialized from the request body. First we check if the input provided in the request body was correctly de-serialized into a BookForCreationDto. If it wasn't, we return a BadRequest. It's a level 400 error, meaning the consumer of the API made a mistake. Then, and this is something you don't have when not creating a child resource, we need an additional check. We need to check if the author with the AuthorId from the URI exists, and if it doesn't, we should return a not found. So we call into the AuthorExists method on our repository. We already ensured that the payload doesn't contain an AuthorId, so we avoided that possible error. The rest of the action is much like what we did for creating an author. First, we mapped the BookForCreationDto to a BookEntity, and add it to the database via the repository. We haven't got a mapping for that yet, so let's add it. We're back in the configure methods on the startup class, so let's create a map, this time from BookForCreationDto to bookEntity, and now we can go back to our action, we call into author Mapper, that's the Mapper. Map statement to a destination type of Entities, book, and we pass in our BookForCreationDto. After this, we call into the repository, our repository contains a method, AddBookForAuthor, it accepts the AuthorId, which we have from our parameter list, and the bookEntity. Then save by calling into libraryRepository. save, and if the save fails, we throw an exception, so the exception handler middleware will return a 500 internal server error. Then we map the bookEntity to a BookDto, and we return this book to return variable in the response body. We should return a 201 Created. And remember from the previous demo that we can pass in the URI to the newly-created book, but to do that, we need to be able to refer to that. So let's scroll up a bit to the GetBookForAuthor action. And let's name this action GetBookForAuthor, so we can refer to it. That's our first parameter, the second parameter is an anonymous object that contains the values needed by the route template, in our case we need an AuthorId and the Id of the book. The last parameter is the object that will be serialized into the response body, in our case our bookToReturn variable. Let's give that a try. We have a post request here to the books collection of Douglas Adams. Our body contains a title and a description. We're going to create The Restaurant at the End of the Universe, which is the sequel to The Hitchhiker's Guide to the Galaxy. The content-type header contains the media type of the request body, application/json. And as usual, we pass in an accept header as well. Let's send this, and we get back a 201 created with our newly-created book in the response body. Let's just make sure that this one now actually exists. By trying to get the book, we can get the book from the location header of the response, let's send this get request, and indeed there's our book, and just to make sure, let's try sending a request that has an nonexisting author reference in the URI. This is a GUID that doesn't match an author in our database. Let's send it, and we get back a 404 Not Found, so we're all good, we now know how to create a resource, and how to create a resource that's a child of another one. But what about creating this in one go, i. e. creating child resources together with a parent resource, let's check that out in the next demo.

Demo - Creating Child Resources Together with a Parent Resource
There's a pretty common use case when working with RESTful APIs, creating a list of children together with a parent resource in one go. So rather than having to send subsequent requests to the book's resource for an author, we want to create the author and its list of books all together, let's check that out. We are in the CreateAuthor action on the AuthorsController. We're going to extend this, so it can create both an author without books and one with books. The first thing we need to do is extend the AuthorForCreationDto, so let's open that. Here we'll want to add a collection of books, and we already have a Dto for creating books, our BookForCreationDto. We must use that class and not the BookDto to avoid running into issues like having a book with the wrong AuthorId. It's a good idea to always initialize these types of collections as to avoid null reference exceptions. There we go. Next, well, there is no next, this is it. Let's look at that CreateAuthor action again. What's going to happen when we pass in an author with a list of books is that the author will be de-serialized into an AuthorForCreationDto, which contains a list of BookForCreationDto objects, so all the books in our request payload will be de-serialized into a list of that type. The checks here can remain as is, and the actual persistence logic is in the repository. With this little change, we've now got an action that we can use to create one author with or without books. This again drives home the point of differentiating between input and output. Input, in this case, is an author with optionally a list of books, an output is just the author, but with different fields. Now of course the repository must support this as well. We're treating it as a bit of a black box, because we're focusing our rests, and that stops at the outer-facing layer, but nevertheless, let's have a quick look. The AddAuthor method on the repository will look at the books, and it will generate an Id for each of them if there are books in the collection. From that moment on, the DbContext contains a new author and a new list of books for that author, and once we call save, these are persisted, and that's handled by Entity Framework Core. Were we to make the Ids for each entity an identity column, EF Core will automatically generate GUIDs for us, so we don't even have to write code to create them. There is of course a specific reason we're doing it like this, and that's because we're going to have a use case later on in the course where the consumer is responsible for generating these Ids instead of the API, but more on that later on. For now let's try this out. Let's put a breakpoint in the CreateAuthor action, and let's our own. We have a new post request line up here, it's post request to api/authors to create an author, but this time we're passing through an author and a list of books for that author. Let's send this. Let's have a look at the AuthorForCreationDto. Our request payload was de-serialized into an author with two books. Let's continue. After mapping, our authorEntity also has these two books, as the authorEntity also has a collection of books, the AuthorForCreationDto has been mapped to an authorEntity, and the BookForCreationDto has been mapped to bookEntities, but the entities don't have their Ids filled out yet, and the author is also null at the moment. After they go to the repository, the Ids will be filled out. Let's continue. Save has been called, and we get back a 201 Created with de-serialized authorDto in the response body. Let's have a look at the location header, and let's get this author. That's James Ellroy, and if you now have a look at his books, we see that this indeed contains the two books we sent together with the author in the request. So this is pretty nice, right? Well, there's one more very interesting use case, and it's probably one of the most often asked questions when building RESTful APIs, how do we go about creating a list of resources in one go, i. e. how do we have multiple authors parent resources at once, rather than posting them one by one? Let's have a look.

Demo - Creating a Collection of Resources
So in this demo, we'll see how we can tackle an interesting problem, creating a list of authors in one go. We can't just post a list of authors to api/authors, as how we named URI implies that one author will be posted, so what can we do? This is one of the reasons it's so important to realize there doesn't have to be a direct mapping between the entities in the backend store and our outer-facing contract. It's not because the author's resource maps to the authors in our database, that other resources cannot have an effect on the same database table, so what does all of this mean, and what should we do then? Well, we design a new resource, an AuthorCollections resource. This requires a request body that's an array of all representations, and it will result in a list of authors being added to the database. Let's open Visual Studio. As we're working on a new resource, I prefer to separate it out into its own controller. It's not a requirement, but it does keep our code a bit cleaner. So let's add a new class, AuthorCollectionsController. It should inherit Controller, and we inject the repository through constructor injection. We'll again use the route attribute, this time with api/AuthorCollections as the template. Then let's add an action CreateAuthorCollection. We decorate this with the HttpPost attribute as we're going to create such a collection. This time though, the parameter coming from the request body is an IEnumerable of AuthorForCreationDto instead of a single AuthorForCreationDto. Let's not forget the from body attribute, and let's name it AuthorCollection. If the collection is null, we return a Bad Request. If it's not, we can continue. We map our AuthorCollection to an IEnumerable of AuthorEntities. Then we add each of them to our data store. For that, we run through our AuthorEntities, and in the for each loop, we call at author on the repository for each author. Then we save, and if the save fails, we throw an exception. That will result in a 500 internal server error. As we're creating a resource, this warrants a 201 Created status code. So we should return CreatedAtRoute, and this is the tricky part. We should include the URI where we can get this collection in the location header, so we need an action to get this AuthorCollection, but how do we do that then? If we're adding to the authors to the author table via our data store, so we don't have a direct key for an AuthorCollection. The key is a combination of a set of AuthorIds, the separation between the outer-facing contract and the data store is becoming even more apparent. For now, let's just return an okay so we can check out if creating a list of authors already works. Let's build our own, and let's give this a try. We're back in Post now. The current request will be an HttpPost to AuthorCollections. In the body, we have an array of two authors. Let's click Send, and we get back a 200 OK, so far so good. Now let's quickly get the authors, so we can check out if our two authors have indeed been added. Here's James Ellroy, and if we scroll down a bit, there's Jonathan Franzen, but we actually want a 201 Created. So we're not there yet. This leaves us with tackling the location header issue, or in other words working with array keys, and while we're at it, we'll cover composite keys as well. Let's check that out in the next demo.

Demo - Working with Array Keys and Composite Keys
In this demo, we learn how we can tackle array keys and composite keys. We're back in our AuthorCollectionsController. Let's first separate these two. With an array key, I mean that the key is a comma-separated list, something along the lines of 1, 2, 3, if we were working with integer keys. With a composite key, I mean the key is a combination of key-value pairs, something along the lines of key 1=1, key 2=2, and so on. I've combined these in one demo, because the question is more related to URI design than implementation. We need an array key for our location header when creating an AuthorCollection, so let's start with that. There isn't a standard that states how to work with this, but what's often done is using round brackets containing the comma-separated key. So, let's add a method to get one AuthorCollection that accepts a key that is actually a list of author GUIDs. Let's name it GetAuthorCollection, and it should accept an IEnumerable of GUID. Let's name that ids. We'll decorate it with the HttpGet attribute, and we pass in ids as the last part of the route template. That id's parameter is an array of GUID, but there's no implicit binding to such an array that's part of the route. But if there's no implicit binding, well, we'll just have to provide it ourselves, and we can do that with the help of a custom-model binder. So let's add one to the Helpers folder, ArrayModelBinder sounds like a good name. Let me paste in the code so we can run through it. A custom ModelBinder is what we use to clarify how a specific piece of input should be mapped to a model. So any ModelBinder should implement IModelBinder. And that interface exposes exactly one method, BindModelAsync. We get a model bindingContext passed in, and through that model bindingContext, we can get information of what we're binding to, and we can get the input values. So the first thing we do is make sure that the metadata about the model tells us that it's an enumerable type. If not, we set the result on the bindingContext to ModelBindingResult. failed, and we return a completed task signifying that this part is done, otherwise we can continue. The bindingContext also exposes a value provider. Through that, we can get the inputted value by passing in the model name on the getValue method. That value will, in our case, then contain a string that's a list of GUIDs. If the value IsNullOrWhiteSpace, we want to return null, that's what we use to check for a bad request. If the checks we just did check out, we can continue. We look for the generic type argument on our model type, that should return GUID. And then we create a converter. These converters are built in, and they help us with converting, in our case, string values to GUIDs. To get that converter, we call into GetConverter on the type descriptor class, passing in the elementType, GUID in our case. After that, we run through our list of values. So we split it up with a comma as a separator, and each string is converted to a GUID by calling into Converter. ConvertFromString. Lastly, we call to array, so values now contains an array of GUID. The last thing to do is create an actual result for our bindingContext. This should contain an IEnumerable of GUID. So we create an instance of that by calling into Array. CreateInstance. This creates an array of elementType for a specific length. ElementType is GUID, and values. Length, well, that's the amount of GUIDs we passed in. Then we copy over the values array into our typedValues array, and we set the typedValues as the model on our bindingContext. At this moment, our binding was successful. So, we set the result to success, and we pass in the bindingContext. Model. Lastly, we return Task. CompletedTask. That's it for our custom model binder. Let's make sure we can use our custom model binder to bind the Ids from the URI to our enumerable of GUID. We can do that with the ModelBinder attribute, passing in the BinderType. That's our ArrayModelBinder. If the Ids value is null, that means the consumer provided invalid input. We should return a bad request. Then we get the authors from the repository. For that we call into the GetAuthors method, passing in our IEnumerable of GUID. One thing that's important to check is if all authors have been found. If they haven't, that means that the key is invalid, and that means we should return a not found. To do that, we check if the count of the inputted Ids matches the count of entities. If it doesn't, we return not found. Then we map the entities, so we have a value to return, we map to an IEnumerable of AuthorDto, and we return them with an OK StatusCode. That takes care of this action. We need to be able to refer to the route from our method that creates an author collection, so we can create URI for the location header. So let's give it a name, say getAuthorCollection. Alright, let's scroll to the CreateAuthorCollection action, we can now change this return OK to return CreatedAtRoute. To correctly create a response, we'll need the content for the response body, an IEnumerable of AuthorDto. So let's map the AuthorEntities to that. But also we're going to need something extra, a list of Ids, as that's our key. For that, we can use a string. join statement, passing in a comma as a separator. From each author in our collection to return, we then select the ID. Then let's return CreatedAtRoute, we pass in GetAuthorCollection as the route name, and anonymous object with IdsAsString as value for Ids, and AuthorCollectionToReturn as the value for the response body. And that should be it, let's build, run, and fire up Postman. Let's send that request to create an AuthorCollection again. The body still contains James and Jonathan. This time we get back a 201 Created, that's looking good. Let's have a look at the location header. This indeed looks like a comma-separated list of GUIDs, let's send the get request to that, and here are our two newly-created authors, in other words, here's our AuthorCollection. That takes care of that, but what about composite keys? A composite key consists of multiple key value pairs, instead of a list of one field. So it could be something along the lines of key 1 equals value 1, key 2 equals value 2. This is a requirement that often comes up in systems where there is no simple one-on-one mapping between the outer-facing contract and the backing data store. We haven't got a good example of such a resource, but we already know how to implement something like this. We should use a route template with two keys, in this case, that map to two parameters in the action signature. So, there's actually nothing new about that, say for designing the resource URI. There's one more thing we have to handle, or other protect against as far as working with post is concerned. Posting to a single resource, let's check that out.

Demo - Handling POST to a Single Resource
We've always posted to collection resources up until now, one author to the author's resource, but what about posting to a single resource instead of to a collection resource? Posting to a URI like this can never result in a successful request. It should either return a 404 Not Found if the author doesn't exist, or a 409 conflict if the author already exists. Let's imagine we'd allow posting to a single resource URI that contains an Id of an nonexisting author. We've got such a request on screen here. While post is used for creating resources, the Id part of the URI signifies a mistake. It's the server that's responsible for creating the resource URI and not the consumer of the API. If we allow the consumer to generate the URI, a post like this would have to be idempotent, and posts isn't idempotent, we cannot rely on the fact that multiple post requests will result in the same outcome. So if treating post as idempotent can be avoided, it should. In other words, a 404 is warranted. Let's send this request, and we indeed get back a 404 Not Found. It's a URI that's not linked to a route, so ASP. NET Core MVC handles this for us. Let's try another one. Say we send a request like this to an existing URI, we'd expect a 409 conflict as we're trying to create your resource that already exists. Let's try this, and we also get back a 404 Not Found. It's still the same type of URI not linked to a route template, so this makes sense, but it's not correct. It's a matter adhering to the HTTP standard. Most APIs would not bother with adding an additional action just to return a correct HTTP StatusCode, but we're trying to adhere to it as good as possible. In this case, that's actually quite easy. So, let's make sure we do. Let's open the AuthorsController again. Here we're going to add a new action, BlockAuthorCreation. We'll again use the HttpPost attribute, this time adding Id to the route template. We then accept that Id in the parameter list. We don't have to bother with accepting a specific type of parameter to serialize the request body to. We're not going to use it to effectively add an author as that's not allowed, we're only going to return the correct StatusCodes. We use this Id to check if an author with that Id exists. If it does, we should return a 409 conflict. There isn't a convenient helper method for that like OK and not found, but we can use the StatusCodeResult class for that. StatusCodeResult is an action result that results in a response without a body, but with a specific StatusCode. So we new that up, and pass in Satus409Conflict from the StatusCode enumeration. If the author doesn't exist, we return not found. That's it, let's give that a try. First let's send a request to an nonexisting author, we get back a 404 Not Found, but it's now our code, and not the framework code that handles this. Let's try sending a post request to an existing author. This GUID is actually Douglas Adams' GUID, and this time we do get back the 409 conflict. So we've effectively solved our issue. We've been working with post in the last few demos, and we've used a content-type header to signify the media type of the request body. Let's open a post request again, and let's look at those headers, here's that content-type header. What would happen if we didn't provide a content-type header. Let's give that a try, let me uncheck that, and let's send. We get back 415 Unsupported Media Type. We didn't pass in the media type, so our API doesn't know how to handle this. In other words, we always need to provide a content-type header when providing a request body that's in line with the self-descriptive message constraint. But what if you want to support other types of input like XML, especially when you're working on an API that integrates between different systems, this can be a requirement, so let's have a look at that in the next demo.

Demo - Supporting Additional Content-type Values and Input Formatters
In this demo, we're going to add an additional input formatter, so we can handle XML input. Let's have a look at the configured services method on the startup class. We added an additional output formatter in the previous module by manipulating the output formatters collection. There's another collection InputFormatters, which as the name implies, allows us to add a new input formatter. Let's add one for XML. What we're looking for is the XmlDataContractSerializerInputFormatter. In case you're wondering why we're using the XMLDataContractSerializer versions of the formatters, well, that's because this formatter supports types like the dateTime offset value we have for dateOfBirth. The XmlSerializer requires that the type be designed in a specific way in order to serialize completely. Most types in. NET were not designed with the XmlSerializer in mind, so that's why it's preferable to use the dataContractSerializer versions of it. Alright, let's build our own and fire up Postman. I've got a few requests laid out here, which we can use to have a look at the options we have. Let's have a look at the first one. What we see on screen is a post request to ApiAuthors, in other words, to create a new author, but this time the request body is in XML format. Note that dateTime offset is split up in two values, the dateTime and the offset. To signify that the body is in XML format, we set the content-type value to application/xml. The accept header has a value of application/json, which means we should get JSON back. Let's send it. We get a 201 Created, so that looks good, and we indeed get James Ellroy serialized to JSON. We can continue now by sending the same request again, but this time asking for XML output. So that's exactly the same request with one small change, the accept header has now got a value of application/xml. Let's send this, and we get back James Ellroy, but now serialized to XML. We've got a pretty descent API already, and it can handle different types of input and output. We've been adding books and authors, but what about deleting them. Let's check that out in the next demo.

Demo - Deleting a Resource
In this demo, we'll look into how we can delete a resource, a book for an author. We'll also look into the StatusCodes that can be returned from such an operation. We're currently in our BooksController, let's add an action DeleteBookForAuthor. We want to decorate it with the HttpDelete attribute. A single-book resource can be found at api/authors/authorid/books/ the Id of the book. It's that URI we'll need as a route template for this action. Api/authors/authoridbooks is covered by the route attribute at controller level. That means that as a template for the HTTP attribute, we should only pass in the Id. The action signature then accepts that authorId and the Id of the book, both GUIDs. First thing to check, does the author exist? If it doesn't exist, we should return a 404 Not Found. Then we try and fetch the book for this author. For that, we call into the getBookForAuthor method on our repository, passing in the Id of the author and the Id of the book. If we can't find the book, we return not found. If both check out, we can delete this book. For that, we call into the delete book method on the repository, passing in the bookforAuthorFromRepo variable. To effectively execute the delete, we call Save on the repository, and we throw an exception if the save fails. Lastly, we'll have to return something. There's no response body after the successful delete to resources gone, so we return a 204 No Content. This signifies that the request was successful, but doesn't have a response body. Let's give this a try. Let's send the delete request to Stephen King's It, significantly lowering the chance of nightmares about killer clowns. We get back a 204 No Content, let's try and get that book, and we see it's now nowhere to be found. Delete is obviously unsafe, as it changes the resource, in this case it deletes it. It's also idempotent, sending the same delete request over and over again will not change the resources any more than it did with the first request the first time it was deleted. Let's send it again. We get back a 404 Not Found, and again, and again, all 404 Not Found, because the resource doesn't exist anymore, so there cannot be an additional change. We also included two checks. Let's send the delete request to an nonexisting book, so the last part of this URI is a GUID of a book that doesn't exist. And we get back a 404 Not Found. Now let's try sending one to an author that doesn't exist, so the GUID in the URI after authors doesn't map to an author, and we also get back a 404 Not Found, so this all checks out. In rest, deleting a resource can have effects on other resources like the children of a parent resource you're deleting. Let's check that out in the next demo.

Demo - Deleting a Resource with Child Resources
In this demo, we'll see how we can handling deleting an author. When we do that, the books for that author should be deleted as well, as an author is the parent resource of a book's resource. Let's open our AuthorsController. Here we'll want to add a new action, DeleteAuthor. We'll annotate it with the HttpDelete attribute. An author resource can be found at api/authors, followed by the AuthorId, api/author discovered by the route attribute, so as a template for the HttpDelete attribute, we only pass in the Id, the action signature then accepts that Id as a parameter. First thing to check, does the author exist? We don't use the AuthorExists method, because we need that author entity to pass into the repository to delete it. So we call GetAuthor on the repository, passing in the Id. If it's not found, we return Not Found. If that checks out, we call into the DeleteAuthor method on the repository. We pass in authorFromRepo. CascadeOnDelete is on by default, so when we delete an author with Entity Framework Core, the books for that author are deleted as well. To effectively execute this statement, we call Save on the repository, and throw an exception if the save fails. Lastly, we return NoContent. Let's give that a try. We'll send a delete request to the author Stephen King, and we get back a 204 No Content. So the resource is gone and so are all the books, there's no way to get them anymore.

Deleting Collection Resources
There's one case we didn't cover, sending a delete request to a collection resource. There's nothing that would stop us from doing so, so it's perfectly allowed. A resource is identified by a URI, and that URI can refer to a collection resource or a single resource, but it's still, well, just a resource, but what would that do? Say we send a delete request to api/authors. That would mean we'd have to delete all our authors, and as books are children of authors, the books for all those authors are well. In other words, we'd end up with not a single resource left to get. So while supporting this is allowed, it's advised against, because delete is a pretty destructive action. Unless you really need it, you don't want to allow this on a collection resource, as that might have the effect of thousands of resources being deleted in one go. Note that that's different from what we just did in the previous demo, where we deleted the books for an author when the author was deleted. In that case, we still send the request to a single resource, which happened to result in other resources getting deleted, because they don't make sense without that parent resource. And with that, we've reached the end of this module, time to check out the summary.

Summary
In the beginning of this module, we learned about method safety and method idempotency. A method is considered safe when it doesn't change the resource or presentation. It's considered idempotent when it can be called multiple times with the same result. Method safety and idempotency help us decide what we are allowed to do for each method, which makes them two very important principles when deciding on which method to use for which use case. Then we started creating resources with post. Post is unsafe and not idempotent. A successful creation should result in a response with a 201 StatusCode and the location of the newly-created resourced in location header. A special case was creating a list of resources in one go, we can't post a list of authors to the authors resource, as that expects one author. But as we learned, the outer-facing contract is different from the business or entity model. They don't have to map one-on-one. That means we can solve this by creating a new resource of the collections, and post a list of authors to that. In other words, one collection of authors that will result in the creation of a new author collection, i. e. additional authors. A RESTful API doesn't have to work with JSON. In the previous module, we learned that we can support different media types for the response body. We can also do that when submitting data in the request body. The content-type header signifies the media type of the request body. We can support different media types for input in ASP. NET Core with input formatters, and as input formatters and output formatters are separate concepts, this allows us to build APIs that serve as an integration layer between different systems. One system might consume the API by passing an XML and getting back JSON, and the other way around. To delete a resource, we use the delete methods. Delete is not safe, as it changes the resource or presentation to resources deleted. It is idempotent though. Sending the same delete request multiple times will have the same result as sending it just once. It results in a 204 No Content StatusCode when it's successful. Deleting a collection resource is allowed, but it's rarely implemented, as it can be quite a destructive action. And with that, this module is done. We can now get, create, and delete resources. One important part is still missing though, updating them. We'll cover that next, including a special case, upserting, which means creating a resource with methods like put and patch that are normally used for update. Let's dive in.

Updating Resources
Coming Up
Hi, and welcome to the Updating Resources module of the Building a RESTful API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. In this module, we'll dive into updating resources, again continuing to implement our outer-facing contract. We know from the HTTP methods clip in the second module that there's two ways to update, put for full updates and patch for partial updates. We'll cover both in this module. But there's more, there's an in between case that's sometimes seen in RESTful APIs, upserting. That essentially means that we'd be creating a resource with put or patch. In some cases that's perfectly valid. We'll look into this as well. After this module, we'll have covered all the common HTTP methods for interacting with an API. So we'll end this with an overview of which method to use on which resource for what use case. Let's dive in immediately with put for full updates.

Demo - Updating a Resource (Part 1)
In this demo, we'll learn how we can update a book for an author. Let's open the BooksController for that. Let's add an action UpdateBookForAuthor that returns an IActionResult. We know from when we learned about routing that put should be used for full updates. That means we should choose the HttpPut attribute to annotate this action. The template we need to pass in should point to the URI. The URI for updating a resource is the same for deleting one, and the first part of that URI, just as before, is covered by the route attribute at controller level. So we only need to accept the Id for the HttpPut attribute route template. Our action signature then accepts an AuthorId and the Id of the book, both are GUIDs. But we should also be able to get the payload data, so just as with create, we can use the fromBody attribute. But we'll need something to de-serialize this request body into, so let's add a new class, BookForUpdateDto. It should contain the title and description. It doesn't contain an Id, because the Id is already part of our URI. We also learned that as put is for full updates, when a value for a specific field isn't passed in, it should be put to its default value. The approach we're taking here makes sure of that. If a put request is sent without a description, the description string will have its default value. But what about an Id? We're updating an existing resource, so we know the Id. Shouldn't we be able to pass it in then through the request body/ Well, we could, but let's look back at the URI of our resource. That already contains the Id of the book. Adding the Id in the request body is thus redundant information, and it will also mean we'd have to add an additional check. The Id in the URI should be the same as the Id in the request body to avoid allowing requests to one resource that actually updates another one. If they are not the same, the Id in the URI wins, and the exact same goes for the AuthorId, that as well is in the URI. So what I prefer to do is avoid these mistakes as soon as we can by not allowing the Ids that are already in the URI in the request body. That then brings us to another issue, if we want to call it that. Let's open the Dto for Creation next to the one for Update. Our BookForUpdateDto and our BookForCreationDto now contain the exact same properties. Can't we just reuse that then? Well, currently we could, but there's a conceptual difference between these Dtos reflected by the name. The one is for updating and the other one for creating, and even though the fields will stay the same in a lot of cases, what is allowed at an update isn't necessarily the same as what is allowed at creation. We'll encounter a good example of such a situation in the next module when we'll look into validation. For now, back to the controller. We can now finish our action signature with the additional BookForUpdateDtoParameter. The first thing we want to check is if there was input provided in the request body that could be de-serialized into a BookForUpdateDto. If that isn't the case, the book parameter will be null, and it means the consumer of the API made a mistake, so we return a 400 Bad Request. We don't have to check for matching Ids, as we don't allow inputting this Id in the request body. We do have to check if the author and corresponding book exists. That's the same code we used in the delete action, so let me copy that over. If one of these doesn't exist, we return a Not Found, and then we need to apply the update. Important here is that in REST we are updating the resource and not the entity. So while it might look tempting to just take the input and copy the field values over to this BookForAuthorFromRepo entity, we must keep in mind that projections might have to happen. So that actually means we should first map the entity to BookForUpdateDto, then apply the updated field values to that Dto, and then map the BookForUpdateDto back to an entity. The nice thing is that we're using AutoMapper, and it allows us to combine these steps. By executing, the Mapper. Map statement first passing in the source object, our Dto, and then the destination object, the entity, these steps can be mimicked. Let's open the configure method on the startup class to see how that works. Here we define our mappings. We need a mapping from the BookForUpdateDto to the Book Entity. And here if there are projections that must happen, we could write them just as we did with the mapping configuration for the AuthorDto mappings. So all of this to just keep in mind that it's always the resource we're updating, and now whatever we're getting back from the repository. AutoMapper simply makes this a bit easier for us. Back to the controller. From this moment on, the entity contains the updated field values. Then let's call into the repository to update. We call the UpdateBookForAuthor method, passing in the BookForAuthorFromRepo variable. And let's look into how that method on the repository looks. It's empty, so why are we calling into this? And why does this update even work? Up until now, we've just used this repository, but now we're actually executing a method that doesn't even do anything. So maybe that's a bit weird. This looks like a good place to side step for just a little moment, and look into the purpose of this repository pattern implementation, so we can also learn why we add this method to that repository.

The Repository Pattern
We've already used a repository pattern in our demos, but what is it exactly? Well, the repository pattern is an abstraction that reduces complexity and aims to make the code safe for the repository implementation, persistence ignorant. So, from that we know there's a few good reasons to use the repository pattern. Let's have a look. Writing code to access a backing data stored directly in a controller action easily leads to code duplication. We might need to access authors or books from multiple parts in our application, so it's preferable to write that code just once instead of in every action or part of the application. The repository pattern helps with that. And from that, code will also become less error prone, if only for avoiding that application. And then there's testing, if you want to test the controller action, but the action also contains logic related to persistence, it's harder to pinpoint why something might go wrong. Is it logic in the action, or is it a persistence-related code in the action that fails? If there's a way to mock the persistence-related code and test again, you know that the mistake isn't related to that persistence logic. So using the repository pattern allows for this mocking of persistence logic, which means better testability of the consuming class. Sometimes it's said that through a repository, you can switch out the persistence technology when needed, and while that is strictly speaking true, it's not really the purpose of the repository pattern. What it is very useful for, however, is allowing us to choose which persistence technology to use for a specific method on the repository. Getting a book might be easier through Entity Framework, getting an author with some complex logic might be more advisable through ado. net, or you might even call into an external service. For consumers of the repository like our controllers, it's of no interest what goes on in the implementation, rather than switching out one persistence technology for another for the complete repository, it allows us to choose the technology that's the best fit for a specific method on the repository, thus persistence ignorant. Okay, so now we know what the repository pattern is and why it's used. Let's get back to why we're calling into that empty update method on the repository. We're working on a repository contract, not an implementation. Different implementations of that repository contract could exist. There's often a mocked repository implementation of a contract used for testing. For that, an update might actually have to do something. So that's why I'd suggest to always have this update method on your contract if an update is allowed. Even if in the specific implementation we're using, it doesn't do anything. By calling into it from the controller, we ensure that older implementations will still execute as expected. If we wouldn't call into this method, because we don't need it for our repository implementation, we might run into issues with other implementations like a mock implementation for testing. Let's dive into the second part of our demo.

Demo - Updating a Resource (Part 2)
So we're back in our BooksController in the action to update a book for an author. We just learned about repository patterns, so we know why we have that update method on our repository, but we still have one thing to clarify, why is this method empty in our implementation? Well in Entity Framework Core, these entities are tracked by the context. So by executing that Mapper. Map statement, the entity has changed to a modified state, and executing a save will write the changes to the database. So let's call into the save method on the repository. If the save fails, we throw an exception. Then we'll need to return something. For a successful update, a 200 OK could be returned, containing the updated resource representation in the response body. Alternatively, we could return a 204 No Content, both are valid. These days I tend to return a 204 No Content to avoid sending over data that isn't required by the consumer of the API. This approach leaves it up to the consumer to decide on this, but it's not a good case for all APIs. Sometimes fields can be updated that weren't part of the request. Think about a modified at timestamp that's returned when getting a resource. As learned when talking about method safety, put can have side effects like this. If that's the case, a 200 OK response containing the updated resource representation in the body seems more appropriate, but in the end, both are valid. And that's it. Let's fire this up, and open Postman. The request we need to send is a put request to the URI of the resource. In this case, that's the URI to George R. R. Martin's A Game of Thrones. In the request body, we input the fields we want to change. We need to provide all of them, unless we want to override values with our default values. The URI contains both the AuthorId and the Id of the book, so we don't include that in the request body. Next to that, we have two headers, the Accept header and the Content-Type header, as we're already used to from the previous module. Let's send this. We get back a 204 No Content, looks like it worked out. Let's get this book again to see if that's indeed the case, and indeed our fields have been updated. Now let's try a few other things. What would happen if we did include the AuthorIds and BookIds, but both set to different values than what's in the URI. These are actually the Ids to Douglas Adams' A Hitchhiker's Guide to the Galaxy. Let's send this. We get back a 204 No Content again. Let's try and get this book again at the correct URI. We see that it was updated, that's because our BookForUpdateDto doesn't contain these Id and AuthorId fields, they are ignored, and the ones from the URI are used. Let's try another request. What would happen if we leave out a field? Say we won't include the description this time. Let's send this, 204 No Content, that's good. Let's get the book. We see that the value of description has bee put to its default value, so that's correct as well. Put should fully update the resource, so what's sent in the request body should be considered the modified version of the resource. If we leave out a field, there is no value for that field. By the way, this does lead to the fact that put is used less and less these days. Imagine a resource with 30 fields, it's not that good for performance that a consumer of an API should have to send over all these field values when he just wants to change one. That's why patch for partial updates is often the preferred option. It's coming up later in this module. But in any case, a request was handled correctly. We had some additional checks. The author and the book had to exist. Let's send a request to an author that doesn't exist. We get back a 404 Not Found, so that's good. Then let's send a request to an author that does exist, but a book that doesn't. And we also get back a 404 Not Found, everything seems to work. One last thing, we just learned that put is unsafe, but idempotent. Unsafe is obvious, we're actually changing the resource. Idempotent should mean that if we send the put request multiple times, it will not change the resource any more than it did with the first request. So let's send our first request two times. We send it once, and we get back a 204 No Content. We send it again, we again get back a 204 No Content. And if we get it, we see the title and description were updated to the same values as the first time. That's it for this demo. Let's look into updating collection resources, or rather whether or not that's a good idea for most RESTful APIs.

Updating Collection Resources
As we remember from when we talked about deleting a collection resource in the previous module, a resource is just a resource. Single or collection, there's nothing that forbids an update. Say we send such a request to api/authors/ an AuthorId/ and books. That means we'd be updating the books resource to a new value, in this case a set of books. We are replacing the current set of books with a new set of books. Put is for full updates after all. The request body replaces what's at the URI the put request is sent to, which means that the correct way to handle this is to delete all the previous books for that author, and then create the list of books that's inputted for that author, so we end up with a completely-new books resource. Very important here is the distinction between what we're actually doing and what the side effects could be. We are still updating a resource, books, so put is warranted. The fact that some book resources are deleted, and others are created when sending that put request is just a side effect. So the reasoning is a bit the same as for deleting collection resources. It's allowed, but in general it's advised against, because it can be quite destructive. The full list of books must be replaced. We won't implement this in our API, but if you need to, you can. Just be aware of the potential consequences.

Upserting
In the beginning of the module, we learned about the possibility to create a new resource with put or patch instead of post. That principle is called upserting, but it requires a bit of explanation, because it's often misinterpreted and can cause quite a bit of confusion. We've got a consumer of our API on the left, and the API server on the right. In a lot of systems, it's the server that's responsible for creating the identifier of a resource. Part of it is often the underlying key in the data store, an integer value or a GUID, for example. A consumer might post a new author to the authors resource, and the server response with that newly-created author in the response body, and the location of the author resource, which the server generated in the location header. In fact, in most systems, the server decides on the resource URI, but REST does not have this as a requirement. It's perfectly valid to have a system where the consumer can do this, or where it's allowed for both the consumer and the server. Now if the key in the database is part of the resource URI, this doesn't just work with AutoNum fields. It does, however, when working with GUIDs. So yeah, you got me, this is one of the reasons I chose GUIDs instead of ints to store data in the backend store. Now let's think about what could happen. Say the server is responsible for creating the resource URI, and we want to send an update request. We need to get the URI to a resource from the server to be able to update it. The resource must already exist. And if it doesn't, we must return a 404 Not Found. Now let's imagine the consumer is also allowed to create resource URIs. In that case, we no longer need to get the URI from the server. The hard requirement for the resource having to have been created before vanishes. We can now send the put request to a previously nonexisting resource identifier that is valid, because the consumer is allowed to create it. In that case, that resource must be created when sending the put request, or in a way it's updated from being empty. So if the server is responsible for the resource identifiers, we must use post to create resources. We can't not know the URI of the resource in advance, but if the consumer of the API is allowed to create the resource identifier, well we can use put as well, and this is called upserting. Now let's think back at method idempotency to see if this still fits. We learned that post isn't idempotent, sending the same request more than once will result in different outcomes. Put, however, is, and that fits. If the consumer of the API chooses the URI, sending the request once will create a resource. Sending it again after that will have the exact same result. The same resource with the same Id is created. Let's see how we can implement that.

Demo - Upserting with PUT
In this demo, we'll extend the action we wrote for updating a resource, so it also supports upserting. We are in our UpdateBookForAuthor action on the BooksController. We've got code here that returns a 404 Not Found if the book we're trying to update doesn't exist yet. This is what we'll need to change. If the book doesn't exist, we want to create it. So let's remove that Return Not Found statement. The first thing to do is map the book from the request body to book entity, as it's an entity we want to add. We also want to set the Id of that bookToAdd to the Id from the URI. Then we call the AddBookForAuthor method on the repository, passing in the AuthorId and the bookToAdd. We are now using this method in both post and put, a good example of one of the advantages of using the repository pattern. Let's have a look at that method. If there's a book Id filled out like in this case, we're upserting, and we don't fill it out again. If there isn't, we generate a book Id, and that's what happens when we post. Back to our controller. Up next is saving, for which we call Save on the repository. After that, we must return a response. If we're upserting, we're creating a resource, so we should return a 201 Created StatusCode. That's the CreatedAtRoute helper method we can use for that. The response body should contain the created book and location header. This is exactly the same as for post, so let's map the book we just added to a Dto. We can use that as the last parameter for our response body. First parameter is the route name, that's the name we gave to the route leading to the get action for one book, in other words, GetBookForAuthor. As far as the route values are concerned, we'll need to pass in an object with an AuthorId and the Id of the book to return. And that should be it, let's give this a try. We've actually got a request we can use for upsert. Remember that we checked that we would get a 404 Not Found when trying a put request to an nonexisting book? Well, let's try sending that one again. We should get a 201 Created, and that's indeed the case. Let's try and get this one just to be sure it has now been created, and indeed it has, and if we send that same request again, we should get a 204 No Content. So let's put this one again, and this time we indeed get a 204 No Content. Sending that same request again this time resulted in an update, which in fact didn't update anything, because we sent over the exact same fields. So this means that we're still adhering to the fact that put is idempotent, even when using it to create a resource. So if you consider doing something with a specific action, keep that idempotency principle in mind. Up until now, we have been using put to fully update the resource, or upsert one in this last case, but we don't always want to fully update the resource, and that's where patch comes in, used to partially update resources. Let's look into that.

Partially Updating a Resource
Full updates with put aren't always advised, if only for the overhead it creates. In fact, if we would look at the data, which is a standard that's essentially a set of best practices for creating RESTful APIs, we'd notice that standard state, patch should be preferred over put, and patch, that's partial updates. But that's not sufficient, we need a way to pass a change set to a resource using this HTTP patch method. In other words, what should the body of a patch request look like? Luckily there's a standard for this, the JSON patch standard. This defines a JSON document structure for expressing a sequence of operations to apply to a JSON document. You can look at that structure as a change set, a set of operations that'll be applied to the resource at patch request with that JSON patch document in the body is sent to. The application/json-patch+json media type is used to identify such patch documents. Let's have a look at such a request body. Imagine this is a patch request to a specific book for an author. It starts with straight brackets signifying an array, that array, that's a list of operations that have to be applied to the resource. We're seeing two operations in the example on screen. The first one is a replace operation signified by op. Path signifies the path to the property. These are the property names of the resource, the Dto, and not on whatever lies beneath that layer. Value signifies a new value, so the title property gets the value new title. The second operation removes the description of a book. The operation is set to remove, the path is thus /description. There's no value, the properties value will be removed. In some dynamic systems, the property itself will be removed from the resource, but when working with Dto classes, it should be set to its default value. Once this request is received, the API will apply it to the resource. Each operation is applied after the other one, and a request is only successful when all operations can be applied. There's six different operations possible. The add operation will add a property at a path location with a specific value, passed through via value. If it's used on a path that exists, that property will be replaced. If it's used on an nonexisting path, the property should be added to the resource, but something like that is only possible when working with dynamic resources often in CRM-like systems. It's not applicable in our case, as we're currently working with statically-typed classes. The remove operation will remove a property, or in non-dynamic cases, set it to its default value. Next to the operation value, it only has one property that has to be set, path. Replace replaces the value at a specified path with the provider value. It's functionally the same as a remove operation, followed by an add operation. Copy will take the value from the from property, and copy it over to the path property. It starts an add operation at the path location, with the value specified in the from member. Move then will copy over the value at the from property to the path property, and remove the value at the from property. So this operation is functionally identical to a remove operation on the from location, followed by an add operation at the path location with the removed value. And lastly test tests that a value at the target location is equal to a specified value. These cases aren't limited to simple properties on a resource. We can manipulate array properties, we can access nested properties, and we can even add a list of items to an array. So path doesn't have to be a simple property, and value doesn't have to be one string value. It might as well be an array, so it's a really powerful standard. Let's have a look at a demo.

Demo - Partially Updating a Resource
In this demo, we'll look into how we can partially update a resource with patch. We're back in our BooksController. Patch is signified by the HttpPatch attribute with as a route template, the location of the book. Let's name it PartiallyUpdateBookForAuthor, and decorate it with the HttpPatch attribute with that Id for the book as the route template. We then accept the AuthorId and the Id of the book. Then we need the input from the request body. This is a JSON patch document. It's from the Microsoft. AspNetCore. JsonPatch namespace, ad it works on a specific type. So what is that type that we want to patch? There's two ways we can go about this, the BookDto or the BookForUpdateDto. You can already feel this coming, right? The BookDto has an Id, and we do not want to allow an update to change the Id of what we're updating. That potentially could no longer match the Id we pass in the URI. If we go for BookForUpdateDto, we're covered on that front. So let's use BookForUpdateDto. We'll name the parameter patchDoc. The first thing to check is if the input isn't null. If it is, we return a BadRequest. Up next, and you know what's coming, we need to make sure the author and book for that author exist; otherwise, we return Not Found. That's exactly the same as we've done a few times before, so let me paste that in. Then it's time to apply the patch document. It's a patch document that works on a BookForUpdateDto. So we need to get the matching book and map it to a BookForUpdateDto before applying the patch document, and that principle is not new. Remember that with put we talked about the fact that the update is on the resource or the Dto, and not the entity, so we map to BookForUpdateDto. But we haven't got a mapping defined for a book entity to a BookForUpdateDto yet, so let's add that in the configure method of the startup class. That's a mapping from a book entity to a BookForUpdateDto. Back to our controller where we can now map, to this bookToPatch, we can then apply the patch document, and to do that, we use the applyTo method on the patch document. We pass in the bookToPatch, our Dto. Important to know is that problems might happen when we do this, if the patch document is invalid, for example, it contains operations on fields that don't allow those operations, like adding a field to a static type that doesn't exist, or replacing a read-only value. Well, if you do that, this will fail, so we do need to add some validation here. The module after this one goes in depth on validation, so for now let's just write a reminder. At this moment the Dto has been changed, so what's next is familiar. It's just the same as what we did with put. First we map the patched values back to the entity by calling the Mapper. Map statement, passing the bookToPatch and the entity. We then call UpdateBookForAuthor on our repository. We pass in our BookForAuthor from repo variable, then we call save on the repository to persist the changes. And lastly, we return No Content. And that's it for now. Let's give this a try. Let's open up our first batch request. Let's have a look at the headers. We learned that the correct media type for patch requests to use isn't application/json, but rather application/jsonpatch/json. Now ASP. NET Core will accept application/json for patch requests, and in fact most RESTful APIs will as well, but we do want to adhere to the standard, so we sent our requests with the correct media type. Let's have a look at the request body. This will update the title, for that we can use the replace operation. It replaces the value at path/title with the new value, which we pass in as value. Let's send this. We get back a 204 No Content, so that looks okay. Let's get that book again. Let's get that book again. And indeed, the title was updated. But patch request can also contain multiple operations. Let's replace the title again and additionally the description. Let's send this, we get back a 204 No Content, so let's get the resource, and indeed the title has been updated again, and the description has been updated. Let's continue with another example. Patch can also be used to remove values, which in our case will result in the field getting its default value. So let's send a patch request to remove the description of a book. For that, we use remove as value for the operation, and /description as value for path. Let's send this, we get a 204 No Content, let's get this book again, and the description has indeed been set to its default value, null. Let's have a look at one more example. We can also copy values with patch, and a patch document is processed in order. So if we first add new description as value to the path description, and then copy over the description to the title, we should end up with new description as value for both the description and the title. Let's send this, we get back a 204 No Content, let's get this resource again, and we see that title and description have indeed new description as their value. So patch is really quite powerful. There's two more things we need to check, let's send the request to a nonexisting author. This should give us a 404 Not Found. As is the case, and let's try sending one to an nonexisting book, that also gives us a 404 Not Found as expected. And that automatically brings us to the last demo of this module. Can we upsert with patch? In other words, could we implement functionality, so the request we just sent would not result in a 404 Not Found, but instead in the creation of this nonexisting book. Let's have a look in the next demo.

Demo - Upserting with PATCH
We learned about upserting a few clips ago, and implemented it for the put HTTP methods. In this demo, we'll implement it for patch. We're back in our BooksController in the PartiallyUpdateBookForAuthor action. If the book for this author doesn't exist, we want to create it, and we want to do that with the Id from the URI. With put this was quite easy, because in the request body for those types of requests, the full resource representation should be passed in. For patch, it's a change set to JSON patch document that's passed in. We need something to apply that change set to, a BookForUpdateDto. So let's create one and apply the patch document to it. We do that when the BookForAuthorFromRepo wasn't found. We create a new BookForUpdateDto, and we call apply on patchDoc to apply the change set to it. Once this is done, we can map this Dto to a book entity. We should also set the Id of this book to at to the Id that was passed in via the URI. One word of caution though, those fields for which no operations are passed in will keep their default values, so keep that in mind when using upsert together with patch. The rest of the code will look familiar. First we add the book via the repository, we do that by calling AddBookForAuthor, passing in the AuthorId and the book to add. Then we call Save on the repository, and we throw an exception if the save fails. Our exception handler middleware will take care of handling this, and what we want to return is a 201 created response with the created resource in the response body. So we map the book we added to a BookDto, which we can return, and then we call return CreatedAtRoute. Here we pass in the name of the route where we can get the book. As the route value object, we need one that contains the AuthorId and the Id of our newly-created book. And lastly, the book to return to serialize to the response body. Alright, that should be it, let's give it a try. From the previous demo, we've still got that patch request to a nonexisting book, so let's just send this again, we get back a 201 Created. The response body contains our newly-created resource. And let's have a look at the headers. Here we can find the location, and that's actually the same as the location we sent our patch request to, so let's try a get request now. We get back a 200 OK with The Winds of Winter in the body. And now we go, we've just implemented upsert for patch, and with that, we're done with the demos in this module. We covered a lot of different options in the last few modules all in a RESTful manner, so it's time for an overview by use case of what these different methods can result in.

HTTP Method Overview by Use Case
Let's start with reading resources. If you want to read a collection resource like authors, we send a get request to authors, that then results in a 200 OK with an author's collection in the response body. A 404 Not found is also possible if the URI doesn't exist. In ASP. NET Core MVC, we didn't have to code anything for that, the framework handles this for us. If we want to read one specific resource, we add the Id, and we should get back a 200 OK with the author in the response body. If the resource doesn't exist, a 404 is warranted. The leading a single resource is done by sending a delete request to that specific resource URI. If the delete is successful, we send back a 204 no content. If the resource doesn't exist, we should return a 404 Not Found. Deleting a collection resource follows the same principles, but it's rarely implemented, because it can be very destructive. For creating resources, we have to make a separation between those requests where the responsibility of creating the URI is at the server versus when it's at the consumer of the API. Let's start with the most obvious case, the server. To create a new author, we should send the post request to the author's resource with the author representation as the request body. If creation is successful, we send back a 201 Created response, with the author representation in the response body and location in location header. If there is no author's resource, a 404 is warranted. Posting to a single resource URI can never result in a successful request. It either returns a 404 Not Found if the author doesn't exist, or a 409 Conflict if the author already exists. And to add a collection in one go, we should create a new resource for that collection. For example, author collections, a post to that resource then contains an author collection, in other words an array of authors, and it can result in a 201 Created, or a 404 Not Found. When the consumer of the API can create resource URIs, for example, when we're working with GUIDs, we can upsert to create resources. A put request to a previously nonexisting URI for an author then results in that author being created. It warrants a 201 Created response with the author in the body. There's no 404 for this URI, because if it's not found, it will be created. And the same is possible with patch with that difference that we're passing in a JSON patch document for that author. Lastly, updating resources. We're starting with full updates with put. A put request to a specific author resource updates that author. We should pass in the full author representation. Fields that are omitted are set to their default values. That can result in a 200 OK with the author representation in the body or a 204 No Content. If the author isn't found, a 404 Not Found is warranted. Of course, if the consumer is allowed to choose the URI, you could upsert instead of returning a 404. Put requests to collection resources are allowed, but rarely implemented. A partial update, well that's what patch is for. To update a specific author, we'd send the patch request to that author's URI, a patch request body can be defined by the JSON patch standard, a list of operations on that specific author. If the patch request is successful, a 200 OK with the updated author representation in the body or a 204 No Content is warranted. A 404 Not Found is returned in the author isn't found, unless a set with put we're upserting. Patch requests to collection resources are just as put requests allowed, but rarely implemented. And that should cover all our use cases. When in doubt, have a look at that method safety and method idempotency table from the previous module. Now there are also other StatusCodes that can be returned, 500 is always possible, and once we add functionality like validation, validation-specific codes can be returned. But this overview is the gist of it, and it should go a long way in mapping required functionality to an outer-facing contract. Let's have a look at the module summary.

Summary
In this module, we implemented the last big part of our outer-facing contract, updating resources. We started out with put for full updates. It's important to remember that all fields should be updated as the result of a put request. Fields that aren't passed through in the request body should be set to their default values. A successful put request warrants a 200 OK or a 204 No Content StatusCode. And put is not safe as the resource changes, but it is idempotent. Executing the request multiple times should result in the same result. These days, put is being replaced by patch, that allows partial updates, removing the overhead put requests can have. The body of a patch request is defined by the JSON patch standard. It describes how to pass through a list of operations that have to be applied to the resource. A successful patch request also warrants 200 OK or a 204 No Content StatusCode just like put. Patch is very powerful and neither safe nor idempotent. We also learned about a special case, upserting, or creating a resource with an update request, be it put or patch. This can be done when the consumer of the API is allowed to create the resource URI. When we're creating or updating resources, we might run into validation issues. And if we run in to those, we'll want to log them, that's what's coming up next.

Working with Validation and Logging
Coming Up
Hi there, and welcome to the Working with Validation and Logging module from the Building a RESTful API with ASP. NET Core course at Pluralsight. My name is Kevin, and I'll guide you through this module. When creating or updating resources, the inputted data often must be checked against a set of rules, for example, a title might have to be smaller than a specific number of characters. In this module, we'll learn what parts validation in a RESTful world consists of. From that, we'll then implement validation when creating and updating resources. When a validation error happens or something else goes wrong, an error, a fault, additional information we want to keep, we might want to log this fact. We'll cover how we can do that in ASP. NET Core. It includes a built into logger for this. We'll include logging to a file as well.

Working with Validation in a RESTful World
When talking about validation, there's essentially three things we need. We need a way to define our validation rules, we need a way to check them, and we need a way to report these validation errors to the consumer of the API, if there are any. Mind you we don't return those errors to let the consumer know whether he or the server made a mistake, that's what the StatusCodes are for. However, often the response body contains a set of validation error messages that the client application could use to show to the user of the application that consumes the API. Most applications require some rules on their objects, be it on a Dto, a business, or a domain object, or an entity. If those rules don't check out, the request should be halted. To create these validation rules, we can use ASP. NET Core's built-in approach or a third-party component. In ASP. NET Core, the default approach to this is using data annotations on our properties. For all common property validation rules, annotations exist, like required or max length. Next to that, we can also define custom rules, rules we can't just easily define with annotations. In the demos, we'll learn how to do both, but that's all just implementation. From a conceptual point of view, what's of importance is what we want to validate. The rule is that we typically validate rules on input, and not on output, so for post, put, and patch, but for not for get. After all, only the input needs to be validated, that's where something can go wrong. This again drives home the point that we should use separate Dtos for separate operations. And that brings us to step two, checking validation rules. For checking validation rules in ASP. NET Core, the building concept of ModelState is used. ModelState is a dictionary containing both the state of the model and model binding validation. It represents a collection of name and value pairs that were submitted to the API, one for each property. It also contains a collection of error messages for each value submitted. Whenever a request comes in, the rules we define in step one are checked. If one of them doesn't check out, the ModelState's valid property will be false, and this property will also be false if an invalid value for a property type is passed in. So that's what we'll use for checking our rules. And that leaves the final step, reporting validation errors. When a validation error happens, the consumer of the API needs to be notified. It's a mistake of the client, so that warrants a 400 level StatusCode. There's one that's specifically used for this, 422 Unprocessable Entity. As we learned before, the 422 StatusCode means that the server understood the content type of the request entity, and the syntax of the request entity is correct, but it wasn't able to process the contained instructions. For example, the syntax is correct, but the semantics are off, and that's a perfect fit for what we need. But when we say reporting validation errors, we're not only talking about the StatusCode, we're also talking about response body. What we write out in the response body isn't regulated by REST, nor by any of the standards we're using, but for validation issues, this will often be a list of properties and their related error messages. You can see an example of that on screen. If the client application needs to be able to do something more than simply binding these errors to a control, often an error code is included as well, so the client application can act on that error code, also quite useful for multi-language applications. In APS. NET CORE MVC, typically the ModelState's validation errors are serialized, so we end up with a list of property names and related validation errors. But this can vary depending on what we're building the API for. Let's dive in with the first demo, working with validation on post.

Demo - Working with Validation on POST
In this demo, we'll get our first look at how to handle validation issues, in this case when creating a resource. Our project is up and running, and we're in Postman. Let's try sending a post request with an empty name and description. We hit an exception, a DbUpdateException, let's continue, and we get back a 500 Internal Server Error. The response body contains our generic error message, stating that a fault happened. Why would that be? Let's have a look at the code. And let's open the BookEntity. We were sending a post request to a book. At entity level, title is required and has a maximum length. Description also has a maximum length. Okay, this ensures no invalid data, at least at this level, is persisted to the database, and that's a good thing, but a 500 Internal Server Error doesn't sound right. That would mean the server made a mistake, but obviously we as consumers of the API made the mistake, and that's an error, not a fault, and it warrants a StatusCode in the 400 range. Moreover, these are data annotations at the level of the entity model, and what we should report back are validation errors at the level of the Dtos, our outer-facing contract, so let's fix this. First we need to define the rules, and we can do that with data annotations. So let's open our BookForCreationDto model class. This is the one the body of the post request is de-serialized into. Let's apply Required to the title. The annotations can be found in the System. ComponentModel. DataAnnotations namespace, so let's add that using statement, and let's have a look at what else we have as far as data annotations are concerned. We can check for valid credit card numbers for data types, we can check for maximum length, we can check for minimum length, phone, range, regular expressions, and so on. So there's a lot of attributes there, and we can already get pretty far as validation is concerned with those. Let's add a maximum length for title and description. There's still some duplication of rules between the Dto and the entity, the MaxLength for example, but these are objects that live on different layers, and the rules might not always be the same across layers, so it is warranted. Okay, so we've got our data annotations on our BookForCreationDto, but as we know, that's just part of the story. We also need a way to check if these rules are adhered to, and this is where the ModelState comes into play. On to the CreateBookForAuthor action on our BooksController. ModelState is a dictionary containing both the state of the model and model binding validation, and it also contains a collection of error messages for each value submitted. And whenever a request comes in, the rules we just applied to our model, the Dto are checked. If one of these doesn't check out, the isValid property will be false. It will also be false if an invalid value for a property type is passed in, so it's a great value to check to see if we can continue with what's in our action. So let's do that. We do need to keep the null check, as the ModelState will be valid if the body can't be de-serialized to the excepted type. That still warrants a 400 Bad Request, 422 Unprocessable Entity has a requirement that the syntax of the request body must already be correct. When we don't provide input, i. e. an empty body, that's not the case. So if ModelState isn't valid, we should return Unprocessable Entity. In the response body, we can then write out the validation errors, but there is no Unprocessable Entity helper method available on the controller. No worries though, there's something else we can use, an object result used to return responses that contain an object. We can create our own class that inherits from it, so let's create a helper class. Let's name it UnprocessableEntityObjectResult, and let's have this class inherit ObjectResult, that can be found in the Microsoft. AspNetCore. Mvc namespace. We need a constructor, and in that constructor, we want to accept an object, the content that will be written out to the response body. So we'll name it error. This constructor should override the base constructor of ObjectResult, and we pass in our error object. And this will already work, it will return a response with StatusCode 422, but the problem is that if we pass in our ModelState as error object, which is what we're going to do, what will be written out is a huge list of all the properties of the ModelState object and all their values. And that's not what we want. We do not need the full list of everything that's in the ModelState, we want key value pairs, the property that triggered the error, and the error messages. So let's change this constructor a bit. We don't want to accept just any object, we only want to accept the ModelState dictionary. That can be found in the Microsoft. AspNetCore. Mvc. ModelBinding namespace. Let's name it ModelState, and instead of passing this ModelState directly to the base constructor, we wrap it in a serializableError object. SerializableError defines a serializable container for storing ModelState information as key value pairs, in other words it was made for this. Just one additional check, if the ModelState is null, we throw an ArgumentNullException. Okay, that's it for our unprocessable entity result, back to the controller. If the ModelState isn't valid, we return a new UnprocessableEntityObject result. We pass in the ModelState as a constructor parameter. Okay, let's give this a try. Let's send the post request to books with null for title and description again, and we get back the 422 Unprocessable Entity StatusCode, which is great. If we scroll down a bit, we see that the response body contains an error message because title is required. Let's try another request. We still pass through null as value for the title, but we also pass in a description that's way too long. I got this one from the Pirate Ipsum generator by the way. Let's send this, we get back a 422 Unprocessable Entity, and both messages in the response body. This is looking good, but while we're on the subject of these error messages, what we see here are the default messages that come with the annotations. We can change those, let's open the BookForCreationDto class again. The data annotations we've used allow us to pass in custom error messages. To do that, we can set the error message property to the message we want to return if validation fails. So let's do that. Okay, let's build and run, and let's send this request again. This time, we see our custom error messages in the response body. Okay, so now we know how to handle validation and how to correctly respond to failed validation, as always related to the resource, and not to a lower layer in the application architecture, but this is just simple validation. Some business or validation rules can be covered with data annotations, so let's make one more change to our code. We're back in the BooksController in our CreateBookForAuthor action. Let's say we have a rule that states that the description should be different from the title of a book. We can't easily cover that with the annotations we're using, so we'll have to write a custom rule. But we do want any validation errors that happen to result in an invalid ModelState, like that, we can keep on using it like we learned in this demo. Well we can do that. First of all we'll have to write that rule, and that's custom manual code. Right after we check our input for null, but before we check the ModelState, we check if the book's description equals the book's title. If that's the case, we add a ModelError, we can do that by calling into AddModelError on the ModelState. This method requires a key and an error message. The key can be a property name, but it doesn't have to be. Often for cross-property problems, the class name is used. In our case, that's BookForCreationDto. Then let's add an error message, and the rest of the code should be okay, so let's fire this up. Let's send the new request with the same title as description, and we get back a 422 Unprocessable Entity, this time with our custom error message. And there's one more thing, custom validation rules are nicely combined with our annotations. So let's send one more request, this one has a title that's way too long, and the same description also way too long. Let's send it, and we get back a 422 Unprocessable Entity with the error message for title and description from the MaxLength annotation and our custom error message from our custom code. That's it for validating input on post, but what about validating input on put? Isn't this exactly the same? Well, let's check that out in the next demo.

Demo - Working with Validation on PUT
In this demo, we'll cover validation on put. So isn't this exactly the same as on post? Well, for some resources, if not most, that's probably the case, but it doesn't have to be. In fact, these days we often see use cases for exactly that. Think about those registration forms on fancy startup company sites. They want to onboard you as soon as possible and want to make the process frictionless, for example, they're not going to ask you to fill out tens of pieces of information on yourself at registration, but once you start using the service more, they might ask for more information. And then that information is required, like credit card information if you want to order something via their service. In our case, we could say we want to make it easy for admins to create a book. A title should be sufficient, but if the admin wants to book to a peer on a site, well he must fill out the description as well. So updating that book resource adheres to different rules than creating it. And we can do that. Once more a good reason to separate the Dtos for update and create. On screen we see the BookForUpdateDto. This should have the same rules as the BookForCreationDto and an additional one. Description is now required. First, let's copy over the annotations from the BookForCreationDto. And then let's add a new one, Required for description. But this doesn't feel good, let's open the BookForCreationDto again next to this one. What we see here is a lot of duplicate validation annotations, and to be honest, properties as well. Where possible I prefer to minimize the amount of duplicate code, and we can do that. We can create an abstract class that will serve as the base class for both these Dtos, so let's add one, and let's add the common properties and rules to it. We'll name it BookForManipulationDto. Let me paste that in, it's an abstract class, because we don't want that class to be used on its own, it must be derived from. Using the abstract modifier in a class declaration indicates that a class is intended only to be a base class of other classes. And then let's have our other two classes derive from it. Let's remove whatever's in this class, and let's do the same for the BookForCreationDto. But we are not there yet. We still need to be able to apply that required attribute to the description for the BookForUpdateDto. And preferably we want to do that without having to copy the MaxLength for description to both classes. Let's open the BookForManipulationDto again. We can use the virtual modifier. Virtual properties are great when you have an implementation in the base class, which we have, but we do want to allow overriding. Now we no longer have to implement this property in the BookForCreationDto. Let's open our BookForUpdateDto. Here we can override the description property. Once it's overwritten, we can add our additional validation annotation. The validation annotations on the properties on the base class will still apply, but for the BookForUpdateDto, the additional required annotation will apply as well. So this is already looking a lot better. No more duplication at this level. Let's open the UpdateBookForAuthor action on our BooksController. The rest of the code is in fact almost exactly the same as for post, one small difference, the class name is now the name of BookForUpdateDto when we add a model error. You might've noticed there's still a bit of duplication as far as validation rules are concerned, our custom rule. We'll get back to that, but now let's try out what we've done up until now. We're back in Postman, let's try sending an update to an existing book. This is a put request to A Game of Thrones, let's update it with null for the title and a description that's too long. This should thus fire the annotations on the base class. We get back a 422 with our 2 error messages, so that's looking good. We still see the same errors, even through we override the description property. So the validation annotation on the description property on the base class is still applied. Now let's try sending one without the description filled out. This payload would work for a post, but it shouldn't work for put. And indeed, here's our additional validation rule, the required annotation we applied to the overridden description property. And if we try one with null for both title and description, we see that we not only get a rule from the base class on title that fails, but also two rules on description that fail, it's null, and it's the same as title. So that's our custom rule, the one we couldn't apply with these annotations. And like that, we know how to work with different rules for post and put, why this is valid use case, and how to minimize code duplication. I do say minimize. Let's open that BooksController again. Our custom rule here in the UpdateBookForAuthor action, well, that's almost the same as the custom rule in the CreateBookForAuthor action. We could factor that out into a separate method, or create some sort of validation service that would handle all the validation for us, but the main issue is in my personal opinion with how ASP. NET and ASP. NET Core by default handle this validation. Annotations mix in rules with models, and that's not really a good separation of concerns. And having to write validation rules in two different places, the model and the controller, for the same model, doesn't feel right either. It's good enough for our purposes, because after all, we are talking about RESTful architectures, so that's what we're focusing on. However, I do want to give a tip for when you're building more complex applications, then it might be a good idea to keep an eye on something like fluent validation, which offers a fluent interface to build validation rules for your objects. From version 6. 4 and onwards,. NET Core is supported. And now let's look into that other HTTP method we covered, patch.

Demo - Working with Validation on PATCH
In this demo, we'll look into what we have to keep in account when validating during a patch operation. The input for a patch operation is a JSON patch document. So if something is wrong with that input, we must return that validation problem to the consumer of the API with a 422 Unprocessable Entity StatusCode. The application is currently running, and we've got a patch request on screen that's invalid. It's a request that contains a remove operation on an nonexisting property. Let's send this. An exception happens, and we get back a 500 Internal Server Error, and that's not right. How can we fix this? Well, let's open the partially UpdateBookForAuthor action on our BooksController. A patch document can be malformed in many ways. We learned how it has to look in the previous module, so if a consumer passing in a patch document that, for example, tries to apply an operation on an existing property, we must notice, and we can by using that same ModelState we've used before. We applied a patch document to a Dto, but there's an overload for that applyTo method that accepts the ModelState. If we pass in the ModelState, any errors in the patch document will make the ModelState invalid. So we do that, and afterwards we check the isValid property, and if it's not valid, we return a new Unprocessable Entity object result, passing in the ModelState. Let's give this a try. Let's send this invalid request again. This time, we nicely get back our 422 error response. In the response body, we see the actual problem, but that's not all, let's try another patch request. This patch request will remove the description, effectively setting it to null. Let's send it, and we get back a 204 No Content, so this just works. But that's not what we want, because a book without a description isn't valid on update. We just added the required annotation to a BookForUpdateDto in the previous demo. Let's see how we can fix that. The issue is related to the fact that the input is a JSON patch document and not a BookForUpdateDto. That means that we must manually validate that BookForUpdateDto after the patch document has been applied to see if it's still valid. We shouldn't map or send it to the repository before that. So let's scroll down a bit. Currently after the patch document has been applied, we check the ModelState. This ModelState possibly contains errors on the inputted model. The inputted model isn't the BookForUpdateDto, it's the JsonPatch document. So as long as that document is valid, the ModelState will be valid. First let's add that extra validation we couldn't achieve with validation annotations. Title and description must be different. Then let's validate. We can use the TryValidateModel method, passing in the now patched BookToPatch instance. This triggers validation of BookToPatch, and any errors will end up in the ModelState. So after that, we check the ModelState, and return a new unprocessable entity object result containing the errors, that's code we already have. Let's try that again. Let's send this request again, and this time we get back a 422 Unprocessable Entity, stating that the description should be filled out. That leaves us with upserting, and that's quite easy, it's exactly the same principle we have to follow. So this is our code for upsert. If a book for an author hasn't been found, we create it. First we pass in the ModelState into the applyTo method. Then we check the now PatchDto, so we define our extra rule, then we call TryValidateModel on the Dto, and if it isn't valid, we return a new UnprocessableEntityObject result containing the ModelState. Let's give this a try. The first request is an upsert. We're patching a book that doesn't exist, but the patch document tries to remove a property that doesn't exist for a book, so it's a invalid patch document. Let's send this. We get back an Unprocessable Entity result containing all possible errors in one go. The title hasn't been filled out, the description hasn't been filled out, both description and title are null, so they are the same, and there's a problem with the patch document itself, the property at path does just not exist, could not be removed, because that property doesn't exist. The second request is a valid patch document for upsert. We're upserting a book with a description, but a book with just a description isn't valid, it needs a title. So this should result in an invalid Dto, and we indeed get back a 422 Unprocessable Entity result containing an error message on the title. Now with that, we've covered validation for post, put, and patch. Forget and delete, none is required. But while we're on the subject of validation, and errors, and faults, there's another thing we have to cover. If a fault happens, we want to log that, because the admin needs to know that something went wrong, and depending on your requirements, you might also want to log other things like informational messages. So let's continue with logging. We're going to start with logging faults. Faults, as we remember, are level 500 problems, the server made a mistake.

Demo - Logging Faults
We've used the exception handler middleware to ensure we return a 500 Internal Server Error when a fault happens, but when a fault happens, we need to know about this. We want to log this. In this demo, we'll see how we can do that. First we need to know how to log in ASP. NET Core. Well, ASP. NET Core has a bunch of built-in services we can inject and use throughout our application. We learned that in the first module. And the logger service, that's one of those, but we do need to configure this. So let's scroll to the configure method in the startup class. There's already some code here, and that's part of the default template. In the configure method, a loggerFactory is injected. This logger factory can be used to configure the logging system, and that's exactly what happens right below. There's this line of code, loggerFactory. AddConsole, what that statement does is add a console logger to the logging system. This means that any logging statement made through this system, typically via an iLogger implementing instance, is logged to the console. What we want to do is add another provider. To do that, there's an addProvider method on the logger factory. But there are shortcuts to this, we already see one on screen. AddConsole will log to the console window, and that's already there. What I like to do is log statements to the debug window, which is quite convenient when developing. We can do that by calling AddDebug on the loggerFactory. AddDebug is part of the Micorosft. extensions. logging. debug package, so let's add that. We're in the Manage NuGet Packages dialog. Here's the package we need. And there we go. When calling into addDebug, we can choose the minimum log level, and there's a few of those. Trace is used for the most detailed log messages, which are typically only valuable to a developer debugging an issue. Debug is used for messages that have short-term usefulness during development. They contain information that may be useful for debugging, but it doesn't have any longterm value. Information-level messages are used to track the general flow of an application. These logs do have some longterm value. The warning level should be used for abnormal or unexpected events in the application flow. So this may include errors or other conditions that do not cause the application to stop, but which do need to be investigated further in the future. Errors should be logged when the current flow of the application must stop due to some failure, such as an exception that cannot be handled or recovered from. And lastly, critical, this log level should be reserved for unrecoverable application or system crashes, or catastrophic failure that requires immediate attention. Let's keep it at the default, which is information or higher. When you're targeting ASP. NET Core 2, it's a bit different. Adding the debug output logger or console output logger isn't required when you're targeting ASP. NET Core 2. To create web host builder, go to in the program class, it already adds those as we learned in the first module. And that's it already for configuration for now. Now we need to effectively log something. In this demo, we want to ensure that all exceptions or faults are logged, and the code to handle that is a bit below this. This is from the second module where we configure the exception handler middleware. We stated that if an exception happens, we want to return a 500 Internal Server Error response with a generic error message. But by doing that we lose our exception, it shouldn't be sent to the consumer of the API, as we learned, but we do want to know about this, so we need to log it. So first let's get that actual exception. Through context. Features, we can get collection of HTTP features provided by the server and middleware available on this request. And what we want is the iExceptionHandler feature. This one is defined in the Microsoft. ASP. NetCore. Diagnostic namespace. If it's found, we can look at the error property to get the actual exception. So with that, we've got the exception, now we just need to log it, and to log something, we need a logger instance. We can inject that through constructor injection in a class, which is what we'll do in the next demo, but in this part of the code, we can use the second possible approach. We've got a loggerFactory injected in the configure method. Through that loggerFactory, we can also create a logger. We do that by calling the createLogger methods on the loggerFactory instance, passing in a name for the logger. To effectively log, we have a few methods at our disposal on that logger, one for each level. So we've got logCritical, logDebug, logError, and so on. An exception like this, which can be handled, should be logged with logError. The first thing to pass in is an EventId, that's a numeric Id to associate with the log, which can be used to associate a series of logged events with one another. EventId should be static and specific to a particular kind of event that's being logged. This allows intelligent filtering and processing of log statements. In our case, the event we're logging is an unhandled exception in our application that will result in a 500 Internal Server error. So 500 sounds like a good Id for that. The EventId type can be implicitly cast to an int, so we can just pass in an int to this argument. Then we can pass in the actual exception. We get that through the exceptionHandlerFeature. error property. And lastly, we can pass in a message for which we can use exceptionHandlerFeature. error. message. And that should be it. Now we just need to throw an exception of course. Let's open the BooksController again, we're in the patch method, so let's just for the sake of this demo comment out the patchDoc. ApplyTo method that accepts the ModelState. If we just use patchDoc. ApplyTo passing in the Dto, we will get a 500 internal server error when we pass through an invalid patch document. Let's run this, we've got an invalid patch document here from the previous demo. Let's send this, and we hit an exception, so far so good. Now let's send the breakpoint in the startup class to see what will happen if we continue. We enter our exception handler middleware code. We look for the exceptionHandler feature, it's not null, so it's found. And if we look at it, we see that the error property contains the actual exception. A logger is created and logError will be called. This should write out the exception to the debug output window. So let's continue, and let's have a look at that output, and there we go, we see our full exception stack trace being written out to the debug output window. Now that's great and all, but logging exceptions through the debug output window is kind of already included by default when you're debugging. And it's still not very useful outside of development. What we really want to do is log this to a persistent store, like a file for example. And that's coming up, but first let's look into how we can log things from other parts of our code

Demo - Logging Errors and Other Information
In the previous demo, we ensure that faults are logged, but there's other types of information we might want to log from other parts of the application like from our controllers. A good example might be deleting a book for an author. That's quite an unrecoverable thing to do, so maybe we'd want to log an informational message when that happens. We're in our BooksController, and the first thing we'll need to do is get a logger somehow. In the previous demo, we created one through a loggerFactory, but loggers can also be injected. If an application component doesn't need the factory, we shouldn't inject it and use it. So we'll inject the logger for the BooksController. iLogger is defined in the Microsoft. extensions. logging namespace. Let's add that using statement. The IOC container can directly provide us with an iLogger of T. When this technique is used, the logger will automatically use the types name as its category name. So that's why we inject an iLogger BooksController. Let's make it accessible through a private field, and there we go. Then let's scroll down to our deleteBookForAuthor action. When a book has been deleted, we want to log this. That's a piece of information, so we call the log information method on the logger. We pass in an eventId, which we can choose, and an informational message. Let's give that a try. We've got a delete statement here from a previous module that will delete a book for an author. Let's send it, we get back a 204 No Content, so far so good. Now let's have a look at the debug output window, and there's our informational message. So with that, we know how to log. But we're still just logging to the debug output window. Time to look into how we can log to a file.

Demo - Logging to a File
When we're logging, we typically want to be able to look through logs afterwards, so we can see what went wrong. So that means we need a persistent store like a file or database. ASP. NET Core does not contain a built-in logger to a database or a file, but the built-in logging system was built in a way that allows third-party providers to easily integrate with it. We're at the GitHub page of the logging component. Let's scroll down a bit. At the moment of recording, there are providers for Serilog, elmah, Loggr, Nlog, and Graylog. Nlog is a provider that allows logging to a file, so we're going to use that one. The nice thing is that regardless of the logger provider we're using, the way to integrate them remains the same. So even though the configuration of Nlog is different than that of, for example, Loggr, the same principles are valid when integrating them in an application. Currently for Nlog on ASP. NET Core, it's advised to install the Nlog. web. aspnetcore package. So let's find that package. We'll install the latest stable version. At the moment of recording, that's 4. 4. 1. Note that this is one of the few clips in the course where it's best to write different code for ASP. NET Core 1 and ASP. NET Core 2. We're currently using ASP. NET Core 1, and we'll switch to ASP. NET Core 2 when applicable. The package has been installed, so now we have to configure Nlog. By default, it looks for a Nlog. config file in the root folder, so let's add one. We choose the XML. File template, and we name the file nlog. config. In this file, we'll want to configure Nlog so it logs messages to a file. This is purely Nlog related, and it can go pretty far. Let's have a look at the Nlog wiki on these configuration files. We can choose which levels to log, configure targets and rules, use wrappers, or escape content, and so on. It would definitely too far to get into all of that, but if you want, you now know where you can find this information. For our purposes, I've got a sample configuration ready. Let me paste that in. First we load to Nlog. web. aspnetcore extensions, an extension that contains targets and layout renderers specific to ASP. NET Core. The rest of the configuration file tells Nlog to log anything with level info or higher to a file named Nlog to shortdate and. log. Let's save this file, and now we have to make sure that we copy this to the output directory on build, so Nlog can pick it up. To do that, we right-click the file and open the Properties. Build Action should be set to None, and Copy to Output Directory should be set to Copy always. Let's click OK, and now we can integrate this, and we already know how to do that. Let's open the Startup class. We're in the configure method, we can use the AddProvider method on the loggerFactory for this. This AddProvider method expects an instance of a class that implements iLoggerProvider. In the case of Nlog, that's the Nlog. extensions. logging. NLogLoggerProvider class. But there's an easier way. Most of these logging providers include a shortcut extension method, so we don't have to directly call AddProvider. In the case of Nlog, that's called AddNlog. It's defined in the Nlog. extensions. logging namespace, so let's add a using statement. And let's scroll down again, and there we go, we can now use the AddNlogExtension method. And that's it for ASP. NET Core 1. Let's switch to ASP. NET Core 2. We could just leave all of this as is, and it'll just work in ASP. NET Core 2, but there's two changes we can make to improve our project. The first one concerns the package version. From version 4. 5 onwards and log. web. aspnetcore can specifically target the. NET standard 2. By doing that, we require less other dependencies as version 2 has an extended API surface that's partially used by this new version of Nlog. Currently version 4. 5 is in prerelease mode, so make sure that Include prelease checkbox is checked. Then let's click Update. On to the second change, and that's code related. We added the NlogLogger provider in the configure method in ASP. NET Core 1, and that'll work, but as we learned, ASP. NET Core 2 allows us to add logging earlier on in the bootstrapping process. That allows us to log errors that happen before we hit the configure method. We're in the Program class. We can add Nlog by calling into UseNlog on the web host builder. That's an extension method defined in the Nlog. web namespace, so let's add a using statement to Nlog. web by pressing Enter. What this call does is register the Nlog services, so they can be used by the dependency injection system, which is exactly what we need to ensure our logger. log statements actually use Nlog as well. Every issue that happens now will be logged to a file, and that's earlier in the process than what we get with ASP. NET Core 1. And this is it, the nice part about the fact that these third-party providers integrate with the built-in logging system is that we don't have to change any other parts of our code. Our logging code will keep on working, it's now simply logging to an additional provider. Let's build and run this. We're in Postman, let's send that DeleteBookForAuthor request again, that should result in something being logged, so far, so good. Now let's open the Output directory. We're running the. NET Core 2 version, so the output directory can be found underneath BIN/Debug/Netcoreapp2. 0. There's a new log file here, so let's open that. There's quite a few statements in here, let's scroll down a bit, and here we can find the statement we log when we delete the book. An interesting fact, by default, Entity Framework Core logs its generated SQL statements when debugging. If you remember the old Entity Framework versions, that wasn't as convenient as it should have been. And now we get it out of the box. And with that, we're at the end of this module. Time for the summary.

Summary
We learned about validation in this module. There's three parts we have to keep in mind. First, we have to define the rules. In ASP. NET Core, this is achieved with data annotations or with custom code. For more complex applications, a component like fluent validation can be used. Validation rules are defined for input and not for output. And if we do that with data annotations, well, that's another good reason to use separate Dtos for our different actions. Then we have to check these rules, that's what the model state is for, a dictionary containing both the state of the model and model binding validation. We can check its isValid property. And lastly we have to report them. The advised StatusCode to use is 422 Unprocessable Entity. How errors should be defined in the response body isn't covered by a standard, but field names combined with their errors and possibly a custom error code are advised. When errors or faults happen, or you just want to save some additional information, we can use ASP. NET Core's built-in logger. This can be extended with third-party middleware as we did for logging to a file. And with that, this module is finished. Most RESTful APIs expose additional functionality though, like paging, sorting, and filtering. In the next module, we'll cover those.

Implementing Paging, Filtering, and Searching
Coming Up
Hi there, and welcome to the Implementing Paging, Filtering, and Searching Module from the Building a RESTful API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. In this and in the next module, we'll implement common functionality for RESTful APIs. APIs typically have a need for paging on their collection resources to avoid performance implications, and those same collection resources sometimes have to be filtered or searched through. Even for these types of functionality for which the parameters are all passed through via the query string, it's important to keep RESTful principles in mind, especially when thinking about how we should return metadata. We'll notice this quite quickly as it comes into play when we cover pagination metadata. Let's dive into paging immediately.

Paging Through Collection Resources
Most APIs, just like ours, expose collection resources. In our case, that's authors and books, and these collection resources can grow quite large. It's considered best practice to always implement paging on each resource collection or, at least, on those resources that can also be created. This is to avoid unintended negative effects on performance when the resource collection grows. Not having paging on the list of ten authors might be okay, but if our API allows creating authors, this list can grow, and we don't want to end up with accidentally returning thousands of authors in one response, as that will definitely have an adverse effect on performance. If you remember from the second module, where we designed the outer-facing contract, options like paging, sorting, filtering, and others are passed through via the query string. These are not resources in their own right. They're parameters that manipulate a resource collection that's returned. You can see an example of this on screen. As far as paging is concerned, the consumer should be able to choose the page number and page size, but that page size can cause problems as well. If we don't limit this, a consumer can pass through 100, 000 as page size, which will still cause performance issues. The page size itself should be checked against a specified value to see if it isn't too large, and if no paging parameters are provided, we should only return first page by default. Always page even if it isn't specifically asked for by the consumer. We're manipulating collection resources, but for things like paging and the other options we're looking through this module to work correctly and have a positive impact on performance, we need to ensure this goes all the way through to our data store. If we have thousands of authors in our database, and we first return all those authors from the repository to the controller and then page them, well, we're still fetching way too much data from the database. We're not really helping performance in that regard then, but how can this happen technically, with link and Entity Framework Core? Well, it can happen thanks to the principle named deferred execution. Let's have a look at what that is.

The Principle of Deferred Execution
When working with Entity Framework Core, we use link to build our queries. With deferred execution, the query valuable itself never holds the query results, and only stores the query commands. Execution of the query is deferred until the query variable is iterated over. Deferred execution means that query execution occurs sometime after the query has been constructed. We can get this behavior by working with iQueryable implementing collections. iQueryable T allows us to execute a query against a specific data source, and while building upon it, it creates an expression tree. That's those query commands, but the query itself, isn't actually sent to the data store until iteration happens. Iteration can happen in different ways. One way is by using an iQueryable in a loop. Another, is by calling into something like ToList(), ToArray(), or ToDictionary() on it because that means converting the expression tree to an actual list of items. Another way is by calling singleton queries. Singleton queries are queries like average, count, and first, because, to get the count, or first item of an iQueryable, the list has to be iterated over. As long as we can avoid that, we can build our query by, for example, adding take and skip statements for paging and assure it's only executed after that. Exactly what we'll need in this module. Let's put it into practice in the first demo on paging.

Demo - Paging Through Collection Resources (Part 1)
In this demo, we're going to implement paging functionality. As an example, we'll do this on our old collection. We're in the GetAuthors action on the authors controller. Here, we'll want to accept two parameters from the query string. Page number signifies the page we want to get, and page size signifies the maximum amount of items on the page. It's important to give these parameters default values. Like that, we can ensure that if a consumer doesn't pass in values for these parameters, the defaults will be used. We give the page number a default of one, and page size a default of 10. These parameters should be bound to values coming from the query string. To signify that, we can use the FromQuery attribute. Now, this isn't absolutely necessary in this case. Framework will look for parameters with matching names to bind automatically. In fact, if we look at the GetAuthor action, we see that it accepts an Id parameter, and that's coming from the route template. We could've used a FromRoute attribute to explicitly signify that, but it isn't necessary, as said. However, it is very useful when our parameter names don't match with what will be implemented in the query string. For example, if we want the consumer of the API to input page instead of page number, we can use from query attribute passing in page as the name. There's one more thing we have to think about, a maximum page size. We've got a default here, 10, but a consumer of an API can still input 1, 000, and that'll be accepted, so we'll add a constant that defines the maximum page size. Let's name it "max Author Page Size" and give it a value of 20. In the action, we then check if the page size isn't higher than the maximum page size, and if it is, we set it to the maximum page size. With that, we've got our parameters. These should be passed to the GetAuthors action on the repository. We know this isn't the only thing that we'll build in this module. We'll end up with a long list of action parameters, all coming from the query string. What I like to do is create a small helper class that holds all these parameters. Let's create such a class in our Helpers Directory. Let's name it Authors Resource Parameters. First, we'll add a page number property, keeping in mind to matching name. We'll give it our default value of one. Then, we'll add our constant. We need this to calculate the actual page size. This time, we can just name it maxPageSize, as we're already in a class that signifies through its name, that this will be used for authors. Then, there's the page size property. That's it for this class for now, but how do we now ensure that the query string input is bound to the properties in this class? Let's go back to our authors controller. The model binding framework is pretty smart. If you change the parameter list here to an authors resource parameters instance, it will look for matching property names inside that class. This is all there is to it. We can now also remove the constant and the calculation. Then, we want to pass this instance to our repository. We haven't got a method like that yet on our repository, nor on our contract, so we'll have to change both. Let's open the contract. Let's change the GetAuthors method so it now accepts an authors resource parameters instance as a parameter. Let's do the same in our implementation. GetAuthors will eventually return a paged, sorted, and filtered collection of entities. We're in the current implementation of this method, and we talked about deferred execution. We can actually already see that at work here. The current implementation asks for the authors on the context and applies a default order on them. Without deferred execution, that would mean all authors would be fetched from the database and only ordered after that. If you look at what authors actually is, let's go to the definition, we see that it's a DbSet, and if you look at the definition of that, we see that it implements iQueryable, so what context dot authors returns is an iQueryable, which we can add additional options to, and that's what we do by calling in to OrderBy. If we look at OrderBy, that actually returns an IOrderedQueryable, and that's, of course, also an iQueryable. Same story for ThenBy. It's only when ToList is called that the query is created and executed on our database. What we can do is apply paging before calling ToList. It's important to add the paging functionality last because we want to page on the sorted, filtered, and searched collection. If we add paging first, the OrderBy and author extension methods will execute on the small page instead of on all the data, and that would lead to incorrect results. What you want to do is first skip an amount of authors. The amount of authors we want to skip is the page size times the requested page number minus one. That'll ensure that, for example, if page two is requested, the amount of items on page one will be skipped. Then, we take the current requested page size. That's it; let's give this a try. Let's send a request to get the authors. We don't pass in any paging parameters, so it should return as a page set with the default restrictions. The first page of 10 authors. Let's have a quick look at the Debug Output Window. Here we can see the actual query that was sent to the database. We see that the query first orders the author set and then offsets it by a parameter, that's filtered the value of skip, and then fetches another amount, and that's a parameter filled with what we pass in intake. All of this is sent right to the database. We're not first loading all authors in memory. Instead, only the amount requested is fetched. Now, let's pass in paging parameters. Say we want the first page of data with a maximum of five authors per page. We pass in those parameters through the URI, separated by an ampersand. Indeed, we only get the five first authors this time. Now, let's try getting the second page of data by changing the page parameter to two. Here we go; we only get one author because we only have six authors in our database, and this is the author that wasn't on the first page. Something seems to be missing. If we're paging through data, especially with defaults in place, we should return some metadata related to the resource collection. Like that, the consumer can get essential information on the collection resource, like the current page or total amount of pages, together with response. Let's have a look at how we can do that.

Returning Pagination Metadata
Let's think about what would be useful for the consumer of the API to know about a page set of data. At the very least, we should include URIs to request the previous and next pages, so the consumer doesn't have to construct those himself. Additional information, like total count or total amount of pages, is often included as well, just like page number and/or page size. How do we include this data in a response then? Well, what's often done, and you've probably already encountered this if you've worked with third party APIs, like Facebook's, is that paging information is included in the response body itself. Often with a metadata tag or paging info tag, but this isn't correct. If a consumer of the API requests a page of authors, with the application/json media type as value for the accept header, the API should return a JSON representation of the resource. An envelope that has a results field and metadata field, well, that doesn't match the media type we asked for. It's not a JSON representation of the authors collection; it's a different media type. Returning the paging info like this combined with application/json as media type, effectively breaks REST, as we are no longer adhering to the self-descriptive message constraint. That constraint states that each message must contain enough information on how to process it. Next to returning the wrong representation, the response will have content-type pattern with value application/json, which doesn't match the content of the response. In other words, we also don't tell the consumer how to process it. The consumer doesn't know how to interpret the response judging from the content-type. We will learn a lot more media types in the next module because, to be honest, application/json isn't such a good media type to ask for a header. But more on that later. When requesting application/json, paging information isn't part of the resource representation. It's metadata related to that resource. Metadata, well, that's something that's put in a response header. The consumer can empower that header to get that information. We should create a custom pagination header, like X-Pagination. Let's continue with the second part of the Pagination Demo.

Demo - Paging Through Collection Resources (Part 2)
In this demo, we'll learn how we can return pagination metadata. First of all, we'll need to generate that metadata. This is something we'd like to be able to reuse, and one way of doing that is by creating a custom list type that holds the data we need to generate the metadata from. Let's add a new class to the Helpers Folder, Page List. We'll make this a page list of T, and we just want to add a little bit of additional functionality to a list of T. We'll start with CurrentPage, and we'll give it a private setter, so it cannot be manipulated from outside this class because allowing that could lead to an invalid value for current page. We do the same for TotalPages. Two other properties that can be useful in the metadata are page size and total amount of items. Let's add those as well. Then, we'll want to know if there's a previous page or a next page. These are properties we can calculate from the CurrentPage and the TotalPages properties. HasPrevious should be true if the current page is larger than one. HasNext should be true if the currrent page is smaller than TotalPages. Now, we'll need a constructor for this. We'll except a list of T, items, the count, the page number, and the page size. From that, we can already set three of our properties. The total amount of pages can be calculated with Math. Ceiling. We divide count by the page size and call Math. Ceiling on it. Math. Ceiling returns the smallest integral value that is greater than or equal to the input, so if we have 101 records with a page size of 10, this will return 11. Lastly, we call AddRange, passing in the inputted items. AddRange is a method on list of T, which our page list derives from. This is adds all the items to the underlying list. Now, we won't directly call this constructor. Instead, we'll add a static method, create, that will create this page list for us. This allows us to call pagelist. create, passing an iQueryable, which is exactly what we get after applying the order by class in our repository. All casting and calculations required can then be done in this create method, rather than having to do that before creating the page list. The static method returns a page list of T, it accepts an iQueryable of T, the page number, and the page size, and that's all the information we have in our repository method. To be able to call the constructor from this method, we need to calculate a total amount of items and the correct page items. To get the actual page, we do the exact same calculations we did in the repository. Lastly, renew of the page list, passing in the items, count, page number, and page size. That's already it for our page list class. Now, let's use this. First, let's change the repository contract. The GetAuthors method should now return the page list of author, instead of an IEnumerable. Then, we do the same in the implementation. Now, we can call page list dot create, passing in the iQueryable right up to the skip statement. Let's just put that in an in between variable, and then we return, page list dot create, passing in the collection before paging, the page number, and the page size. Okay, so now we have a collection in our controller that contains all the data we need to create a metadata from. Back to our controller. The authorsFromRepo variable is now a page list of author. What we want to do is generate URIs to the previous and next page. If you've worked with the old ASP. NET web API, you know there used to be a class that would help with that, the URL Helper. A comparable class exists in ASP. NET Core, URL Helper, which implements IUrlHelper. We'll first have to register this on the container, so we can inject it into our controller. On to the configured services method on the startup class. Let's register the IUrlHelper with its implementing type, UrlHelper. IUrlHelper is defined in the Microsoft. AspNetCore. Mvc name space. We use AddScoped, so an instance is created once per request. This isn't sufficient. A URL Helper will generate URIs to an action, and to be able to do that, it requires the context in which the action runs. In ASP. NET Core, that's accessed through an action context accessor. We'll have to register that service as well. These are defined in the Microsoft. AspNetCore. Mvc. Infrastructure name space. We register it before we registered the URL Helper because the URL Helper will use this, and this time we use AddSingleton because we only want to create is the first time it's requested. If you use transient or scoped here, the action context will still be null. Then, let's make sure our URL Helper has access to it. Instead of just registering an instance of URL Helper, we're going to tell the container how it should be constructed. To do that, we pass in an action to construct it, an implementation factory. In that, we first get the action context, and we can do that by calling GetService and passing in IActionContextAccessor. That'll give us an instance of action context accessor, which has the action context as a property, and then we return a new URL Helper, passing in that action context. That should be it. Now, let's inject our URL Helper in our controller. There we go. Let's scroll to the GetAuthors action. We want to refer to the GetAuthors action, so let's give the route a name first. Then let's create our links. Let's add a private method for that, to help us with it. This one should accept the old resource parameter, as that's needed to generate URIs. It should also accept an enumeration, so we can pass in whatever we want to generate a previous or a next page link. Like that, we can reuse this create authors resource URI method. Let's add that enumeration to the Helpers Folder. It should have two values: previous page and next page. Back to the controller, and let me paste in the implementation, so we can run through it. To create a link to the previous page, we use URL Helpers link method. We pass in a route name, GetAuthors, and a set of values, and these values are the query string parameters, page number and page size, so we pass in a new object, and set the page number to the currently requested page minus one because we want to link to the previous page. Page size stays the same. Likewise, for the next page link. We use URL Helper to generate link, but this time, we set the page number to the current page plus one because we want to link to the next page. As a default, we simply return the link to the current page. These helper methods can now be used to generate links. Back to the GetAuthors action, here we can check if there's a previous page. We can do that by checking the HasPrevious Boolean on the page list, our authorsFromRepo variable. If there is a previous page, we call CreateAuthorsResourceUri, pass in the authorsResourceParameters, and the previous page as resource URI type because that's the type of link we want to create. If there is no previous page, the previous page link will be null. Then, we follow the same logic for creating the next page link. With that, we've got our previous page and next page links. Now, let's create a metadata. For that, we instantiate a new object, and we pass in the values we get from the page list, total count, page size, current page, and total pages, and next to that, we also pass in the previous and next page links we just generated. The last part of the story is adding this as a custom header to the response. We name the custom header X-Pagination, and as a value, we serialize the pagination metadata. For that, we use JSON. net, calling to SerializeObject on JsonConvert, passing in the pagination metadata object. That's it. Let's give this a try. Let's send that request to get one page five items again. We get back 200 okay, and we indeed see five authors in the list. Now, let's have a look at the headers. Here's our next pagination link. It contains the total count, the page size, the current page, and the total amount of pages. We also see that there is no previous page link, but there is a next page link. Let's give that one a try. We indeed get the next page of data, and if we look at the headers, we see that this time, there is no next page link, but there is a previous page, the first one. Alright, and with that, we covered paging, but there's more, of course. We can also filter our resource collection, or we can allow searching through it. Let's have a look.

Filtering and Searching
In the two upcoming demos, we'll implement filtering and searching on our API. Filtering a collection resource means limiting the collection resource, taking into account a predicate. For example, we want to return all authors where the genre matches horror, or fantasy. We're passing in the field name and the value we want that field to match, to make it part of the collection that will be returned. Searching goes beyond that. For searching, we don't pass in a field name that should match, we pass in a value to search for, and it's up to the API to decide which fields shall be searched for that value. Often, that's done with full-text search, and it's useful for cases where basic filtering isn't powerful enough. We might pass in search query, signified by search querying our example, and the text to search for, say, "king. " The API should then return all resources that have "king" as part of any string field. Filtering and searching are different. As far as the rest is concerned, two things are important. First of all, filter and search options are sent to the API via the query string. You can already see that on screen in the two examples. Secondly, and this is very important, for filtering, allow filtering only on fields that are part of the resource. For example, our author resource has a name field. That means that a filter should be on that name field, and not on fields like, "first name" or "last name" that aren't part of the resource. Those are part of the underlying entity, and that's of no importance to rest. That also means that depending on how we design the outer-facing contract, we can choose to offer less granularity in our filter. We can't filter on first name or last name in this case, only on name. Let's implement filtering and searching in the two upcoming demos.

Demo - Filtering Collection Resources
In this demo, we'll implement a filter on genre First thing to do is allow the consumer of the API to pass in value for genre through the query string, and that's why we created that authors resource parameters class. All we have to do now, is add an additional property to this. We'll leave this default value at null. String is a reference type, so that's what the default will be. By default, we don't want to apply a filter on genre to our resource collection. Then, we need to implement a filter, so on to the repository implementation. We're going to apply this to the collectionBeforePaging variable. First, we check if the (mumbles) genre is filled out. If it is, we trim it and lowercase it, so casing is ignored. This is a design decision, by the way. Your eyes are case sensitive, so you can choose to take the casing into account, if you wish. Once we've got the genre, we use a where class to filter on this value. We see there's a red line here. That's because we're assigning the result of. Where, which is an iQueryable, to the collectionBeforePaging, and collectionBeforePaging, is an IOrderedQueryable because we apply order by and then by first. We can easily fix that by casting it to a queryable. That's it already, save for one thing. Let's open the authors controller again. We're generating previous and next page links. It's important that these links contain the filter as well. Let's open that CreateAuthorsResourceUri helper method. Here's the actual code to generate the next and previous page links. If we go to the next or previous page, we must keep that filter into account, as otherwise, we end up with a page of data that comes from a different, non-filtered collection. Let's add Genre to the object that's used by the URL Helper to generate the link. We do that for the previous page, for the next page, and for the default link to itself. Okay, let's give this a try. Let's send a request that will return all authors which are a fantasy. We do that by passing in Fantasy as the value for the genre query string parameter. Indeed, we only get back two authors, those which are a fantasy. Let's see if we can page through this. We only have a very small number of authors which are a fantasy, so let's use page number two and page size one. That should give us the second author only, and that's indeed the case. If we look at the header, we see that the previous page link now, indeed, also includes the genre in the query string. Let's try that, and this time, we only get the first author. This works as expected. Let's continue with searching.

Demo - Searching Through Collection Resources
In this demo, we'll implement searching functionality for our authors collection. First, we need to ensure the search query can be passed in. That means, adding a search query property of type string to the authors resource parameters class. On to the repository. How search is implemented depends on your architecture or requirements. Often, this is a full-text search with the help of full-text search components, like Lucene, but it can also be much simpler. In our case, we'll use a where class again. This will return any author for which the genre first name or last name contains the search query. You can keep on adding where classes. This is, again, deferred execution at work. That's all I need for the changes to our repository. Just as with filtering, the next thing we need to ensure is that the next and previous page links are authored. We're back in the create authors resource URI Helper method. This time, we ensure that the search query is passed through. That's the previous page, and we do the same for the next page, and for the link to itself. Let's give this a try. First, let's search for all authors that have an A in their first name, last name, or genre. We get back all of them, minus one. Let's say we want to search for AD, this time, we only get back Douglas Adams because of the AD in his last name. Now, let's combine that with our filter. Let's search for all authors that have an A in their first name, last name, or genre and have fantasy as genre. We only get back George Martin and Neil Gaiman, so that looks good as well. Now we can continue. We can add some additional paging requirements. Let's say we want the second page and page size of one. That should just return one author. Indeed, we only get back Neil Gaiman, and if we look at the pagination link, we see that the previous page link, indeed, contains our search query. Let's give that a try. We get back the first author, George R. R. Martin. That already brings us to the end of this module. Let's have a look at the summary before we continue with supporting additional API functionality, like sorting and data shaping, in the next module.

Summary
We started out with paging, which can be very good for performance, if done correctly. We should pause the pageSize and pageNumber via the query string. Think about limiting the pageSize that's passed in to avoid consumers requesting one page with a 100, 000 records. We should page by default to avoid performance issues when the collection grows. The pagination metadata belongs in a custom header. Filtering limits the collection resource, taking into account a predicate. This predicate is passed via the query string, the field name, followed by the requested value for that name. Important to remember is to only allow filtering on fields that are part of the resource and not of objects related to lower-level layers. Searching is related to filtering, yet different. It's often used for full-text search. We pass a string to search for via the query string, and it's up to the implementation to decide how matching happens. That's all we covered in this module, but there's two other pieces of functionality that are often implemented on APIs, sorting and data shaping, and we're covering that next.

Implementing Sorting and Data Shaping
Coming Up
Hi there, and welcome to the Implementing Sorting and Data Shaping module from the Building a RESTful API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. We ended the last module with filtering and searching, but a lot of APIs require additional functionality. A consuming application might want to choose how to order the resources in a list, not only by one field, but by multiple fields, both ascending and descending. And then there's the principle named data shaping, or shaping resources. This allows consumers to choose which fields of the resource should be in the representation. So let's continue where we left off in the previous module by looking into sorting.

Sorting Collection Resources
A simple sample would be OrderBy=Age. That's along lines of what we know. One value for an OrderBy class. We immediately notice a likeness with filtering. Sorting statements relate to the fields of the resource and not on whatever lies beneath that layer. Let's have a look at another sample. Here, we're ordering by Age, but descending, and well, that's different, because we're now also stating how we should order. Only one OrderBy class, but in our code we have to take into account that part of the string is about a field and another part is about the order, order direction. And we can go beyond that. In this example, we're ordering by Age, descending, and then by Name, because sorting typically doesn't stop after the collection is sorted on one field. That's another thing we have to take into account when implementing this. Let's dive in.

Demo - Sorting Collection Resources (Part 1)
In this demo, we're going to implement sorting. Put in our Authors controller. To GetAuthors action, you must be able to accept an OrderBy query string parameter. So let's make sure we can accept that. All we have to do for that is add it to our AuthorsResourceParameters class. Currently, we're ordering by FirstName and LastName by default, which translates to Name on the AuthorDTO. So let's put that as the default value. Then we should apply the requested order. Let's open the repository. We gave the OrderBy parameter a default value of Name. So we should actually replace that with the OrderBy and ThenBy class we see here. And that means we needs a mapping from the property names on the DTO to the property names on the entities. That's part one. And secondly, we'll need to be able to apply the sorting. But what we get are strings. Let's have a look at that OrderBy class. If you look through the overloads, we see there is no overload that allows us to pass in a string. And we really want to avoid having to write a huge switch statement for all possible sorting combinations. Now, luckily, this isn't a new requirement. In fact, it's for these types of requirements that Microsoft created the System. Linq. Dynamic library. Through that library, we are able to sort on strings. So let's add that package. So we want to search for System. Linq. Dynamic. Core, and let's install this. And there we go. Okay, so now we should be able to sort by string. But doing all that in this repository method won't exactly lead to reusability. We can separate it out into an extension method on IQueryable. So what you want to end up with is a sort of ApplySort extension method on IQueryable that accepts the OrderBy string from our query string parameters, and a mapping dictionary. Let's think about that mapping dictionary.

Creating a Property Mapping Service
So we can have sorting on multiple fields of the resource, but one resource field might map to multiple properties on the entity. For example, the Name property of the AuthorDTO is concatenation of FirstName and LastName on the entity. Moreover, we allow sorting, ascending and descending, and in the case of Age, that'll be an issue. The higher your Age, the lower your DateOfBirth. So in that case, the passed in sort order should be reversed. We could just create a hardcoded list of mappings, but let's make this reusable. We're going to create a propertyMappingService and register it with the ASP. Net Core's built-in container. Now, how do we do that? We'll need a propertyMappingService class. It implements an interface, IPropertyMappingService, that allows us to register it on the container on interface instead of directly on implementation. The service holds a list of PropertyMappings like one that maps to properties we can sort on of an AuthorDTO to the properties on the underlying entity. One such property mapping from a sorted destination then holds all the mappings from, for example, the AuthorDTO fields to the Author entity fields. And that's a dictionary. A simple one could be ID maps to ID, but as we know, that's not sufficient. One field of the resource might map to multiple fields on the entity, like Name to FirstName and LastName. So the dictionary should have the field name on the resources key but another classes value, a PropertyMappingValue. That PropertyMappingValue should then contain a list of destination properties. With that, we can map the Name field to FirstName and LastName. And we'll need something else. As we mentioned in case of Age, the sort order should be reversed, as when we sort ascending by Age, that actually means we sort descending by DateOfBirth. So our PropertyMappingValue should also have a Revert property. And with that, we have all the information we need for our mappings. So on the PropertyMapping service, we should then create a method to get a specific mapping: GetPropertyMapping from source to destination. For example, from AuthorDTO to the Author entity. It's a bit of work, but in the long run, this really helps toward reusability. So let's dive in.

Demo - Creating a Property Mapping Service
In this demo, we'll create the propertyMappingService we just looked into. Let's start by adding a propertyMappingService class to the services folder. We know it has to contain a list of property mappings. Such a property mapping contains a dictionary of string PropertyMappingValue. So let's add a PropertyMappingValue class next. This one contains an IEnumerable of string. The destination properties, one resource property will map to, and next to that, a Revert Boolean allowing us to revert the sort order if needed. Back to our propertyMappingService. Let me paste in the mappings we'll use from an AuthorDTO to an Author entity. Of importance here is that we can now map Age to DateOfBirth using reverse ordering and Name to FirstName and LastName. Then let's add that method to get one property mapping. Through this, we'll be able to ask for a mapping from a source type to a destination type, like a mapping from AuthorDTO to Author. But we haven't yet got a class to hold such a property mapping. So let's add that. And to it we add one property: a mapping dictionary of string PropertyMappingValue. And that already takes care of our three class. So let's go back to the propertyMappingService. We can now add an IList of property mapping from Tsource to Tdestination. But we run into a bit of a problem. Tsource and Tdestination can't be resolved. That can be overcome by using a marker interface. That's an interface without any methods in it, and it's often used for cases like this. If we let our property mapping class implement the marker interface, we can add an IList of that marker interface. So let's add one. IPropertyMapping looks like a good name. So it's just an empty interface, and now we let our property mapping implement this interface. And now we can register a list of IPropertyMapping in our propertyMappingService. We're getting there, right? In the constructor we can then add our property mapping to this list as a mapping from the AuthorDTO to the Author entity. And all that's left in this class is implementing the GetPropertyMapping methods. First, we find the matching mapping by searching our list of IPropertyMapping. For that, we can use the OfType method. If one is found, we can return to mapping dictionary. And otherwise, we throw an exception. And that's it for this class. Now let's create an interface so we can register it as an interface implementing class on the container. Let's name that one IPropertyMappingService. And it only has one method, GetPropertyMapping. Our propertyMappingService class should then implement this IPropertyMappingService interface, and that's it. Now we can register this on the container. And we know how to do that. Let's open the ConfigureServices method on the startup class. Transient is lifetime advised by the ASP. Net Core team for lightweight, stateless services. So, let's register it as such. And with that, we can continue with implementing our sorting functionality.

Demo - Sorting Collection Resources (Part 2)
In this demo we'll continue with implementing sorting. We'll start by injecting our newly created propertyMappingService into our library repository. So let's open that. Let's use constructor injection and that should be it. With that, we can get our Author property mapping dictionary in the GetAuthors method. Now we just have to write that ApplySort extension method. So let's add another new class: IQueryableExtensions. Let's make it static, because we'll use this class to add an extension method to. And let's add a static ApplySort method. It's an extension method to an IQueryable of T, and it an accepts an OrderBy string and a mapping dictionary. As we're going to be using Dynamic LINQ for this, let's add a using statement. And the implementation is quite a lot of type, so let me paste that in and then we can run through it. Let's have a look. First, we check the input parameters. If those are null, we throw argument null exceptions. If there is no OrderBy string inputted, we can simply return the source. The OrderBy string is separated by commas, and we need each part individually. So we call split on OrderBy to get those parts. And what we then want to do is apply each OrderBy class, but we need to do this in reverse order, because otherwise the IQueryable will be ordered exactly the wrong way. An OrderBy class might contain trailing or leading spaces, and we don't want that, so we trim it. And as we know, we can also input the sort order, ascending or descending. If desc is inputted, we should order descending, and otherwise, ascending. Then we remove those parts of the string from the OrderBy class, because then we need a property name to look for in the mapping dictionary. For example, Name. We look through the mapping dictionary and look for a key with that property name. There should be one there, otherwise we cannot sort. So if it's not found, we throw an exception. If it is found, we fetch it, and then we can use the destination properties from the PropertyMappingValue. For example, if the inputted OrderBy class contained Name, the destination properties will be FirstName and LastName. As you remember, there's a possibility that we have to revert the sort order in case of Age and DateOfBirth, for example. So we check for that value on the PropertyMappingValue as well. And then we use Dynamic LINQ to generate our OrderBy class. For that, we call OrderBy passing into destination property as a string, combined with either ascending or descending, and that's it for the ApplySort extension method. If you have a look at our library repository now, we can see all the errors are gone. So that should be it, save, of course, for one thing. Just as in the previous demos, sorting can be combined with paging, filtering, and searching, so we have to change the next and previous page links. They should contain the requested sorting order. That's in our Authors controller, in the create Authors resource URI method. We extend the object by passing in the OrderBy class that was inputted. We do that for previous page, next page, and the link to itself. Okay, that was lot of typing, and quite a bit of work, but now we should really have something reusable. Let's give this a try. First, let's just send a request to get the Authors. These come back ordered by Name, so FirstName and LastName, as that's the default for the OrderBy parameter. So if we add an OrderBy=Name class as query string, we should get the exact same result, starting with Douglas Adams, and then George R. R. Martin, as is indeed the case. Now let's continue. What about ordering by Name, descending? This time, we get back our Authors, but in descending order. And we can, of course, combine OrderBy with paging. So let's give that a try, but first let me set the page heights to something smaller because we don't have that many authors in our database. And this time you only get back the second page of our authors ordered by Name, descending. Just to be sure, let's get the first page, and indeed, first a T, then the S, then an N, and so on. If you look at the expagination header, we can see that the previous page link and next page link also contain our OrderBy class. All right, let's continue with another example. Name was a field that translated to two properties on our entity. Genre, that's a direct mapping. So let's check if that sorting works as well. And that seems to check out. We're now sorted ascending by Genre. If we add desc, we should get them back in reverse order, which is the case. And let's combine that with that other special field we had, Age. Age maps to the DateOfBirth, and it should reverse the sorting order, because the higher the Age, the lower the DateOfBirth. Let's send request, and we get back the authors ordered ascending by Age, descending by Genre. Lastly, let's try an invalid request. We're going to try and sort by DateOfBirth. That shouldn't work. There isn't a mapping defined for that. We hit an exception, so far, so good. And if we continue, we get an unexpected fault that has happened. A 500 internal server error. So our code works. But we're not there yet. Remember that, when the consumer of the API makes a mistake, he should get back a 400 bad request? Well, the consumer made a mistake. He provided invalid input: DateOfBirth. So we should get back a 400 bad request. In other words, it's not because our code works that we're RESTful. Let's check out how we can fix that in the next demo.

Demo - Taking Consumer Errors into Account When Sorting
In this demo, we'll look into how we can take consumer mistakes into account when sorting. So we're back in Visual Studio. What we want to do is add a method that checks for invalid input. We can then use that result in the GetAuthors action to return the correct status code. So when is an OrderBy invalid due to a consumer mistake? Well, we already decided on that before. It's invalid when a consumer asks to order on a property for which no key in the dictionary exists. Let's open our propertyMappingService. Here we defined our list of Author property mappings. But this doesn't mean that, if that checks out, that other issues might not arise. For example, you might introduce an unwilling bug like make mistake in a mapping, or provide a wrong mappings to the ApplySort action. That can result in an exception, or it might pass without us noticing. It can be a bug. But those are mistakes of the server, or rather, of us, the developers, and not of the consumer of the API. So we need to check if, for a given string of fields like our OrderBy string, there is a valid mapping dictionary that contains mappings for all those fields. So, this propertyMappingService sounds like a good place to add such a method to. Let's name it ValidMappingExistsFor. It returns a Boolean that will be true if all checks out. Let me paste in the implementation so we can run through it. We get the mapping dictionary by calling in to GetPropertyMapping. Pass in the source and the destination types, Tsource and Tdestination. If no field is inputted, we can safely return true. But if there are fields inputted, we should run through them. We split the inputted fields and run through them, and we trim them so leading and trailing spaces are ignored. We also remove everything after the first space. That'll make sure that, if the fields are coming from an OrderBy string, asc and desc are ignored. Then we look for the property mapping with that field as key in our mapping dictionary. And if the key isn't found in the dictionary, we return false. If all checks out, we can safely return true. And that's it for this method. Let's add it to the IPropertyMappingService interface, so we can access it through that interface. Now we should be able to call ValidMappingExistsFor from wherever we inject an IPropertyMappingService implementing class. So let's go to the Authors controller. This is where we'll want to check if the OrderBy string is valid. We want to do that in the GetAuthor section so we can return a bad request if it isn't. That means we need to inject the propertyMappingService in our controller. And this is one of the reasons we separated all of this out into a class of its own, a propertyMappingService. It really helps with reuse of our code. And there we go, we now have access to a propertyMappingService instance. In the GetAuthors action, we can now call into the propertyMappingService's ValidMappingExistsFor method, passing in the source and destination type we want to check a mapping for, and the OrderBy class from the AuthorsResourceParameters. If it doesn't check it out, we return a bad request. Let's give that a try, and let's try sending that last request again. We used to get a 500 internal server error, and this time we get back the more correct 400 bad request. And with that, we know how to sort. There's one more piece of functionality that is sometimes supported by RESTful APIs, and it's named data shaping, or shaping resources. Let's have a look at what that's all about.

Shaping Resources
Let's talk about shaping resources. The word kind of gives away what it does. This principle allows the consumer of the API to choose the fields of the resource that have to be returned. So rather than returning all properties of say, an author, a consumer of an API might only need to know the ID and the Name. Data shaping allows for this by looking at the fields query string parameter, of which the value is a comma separated list of field Names. Implementing functionality like this can be very good for performance. In case of our authors, the impact will be quite small, but just imagine a resource with thirty properties of which the consumer needs a collection of fifty, but he only needs two fields. Without data shaping, 1, 500 property values would be sent over to the consumer. And with data shaping, only 100 will be sent, and that does have a noticeable impact. What's important here, just as for filtering and sorting, is that this field level selection is at level of the resource. Let's have a look at how we can implement this.

Demo - Creating a Method to Shape Resources
In this demo, we'll create a reusable extension method to shape data. First, let's have a look at what's returned from our GetAuthors action. We see that it's an IEnumerable of AuthorDTO. But with data shaping, we no longer want to return an IEnumerable of AuthorDTO. We want to return an IEnumerable of a new object, one that only contains the requested fields. We need a way to dynamically create an object that runtime starting from our AuthorDTO. So we're going to work with dynamically typed objects in this demo. This is where the ExpandoObject comes in. ExpandoObject represents an object whose members can be dynamically added and removed at runtime. The extension method we want to create, it doesn't matter to that extends. IEnumerable of T accepts our fields and returns an IEnumerable of dynamically typed objects, ExpandoObjects that only contain the fields we requested. So let's create a new class to hold this method, IEnumerableExtensions. Our method should return an IEnumerable of ExpandoObject. It should work on an IEnumerable of a specific type, Tsource, and it should accept the fields. Let me paste in the implementation, as it's a lot to type, so we can run through it. Let's import Name spaces we need. We're going to use bit of reflection, so we'll need a using statement for System. Reflection, and let's run through it. So the first part of this method is much like our ApplySort method. We check the source parameter, and if it's null, we return an argument null exception. We're going to return a list of ExpandoObjects, so we create a list to hold these. What we do next is create a list with PropertyInfo objects on Tsource. We're going to be using reflection to get PropertyInfo objects, which allow us to access property values. But reflection is quite expensive, so rather than doing it for each object in the list, we do it once and reuse the results. After all, getting the PropertyInfo objects is done on the type of the object, Tsource, and not on the instance. These properties we need to return depend on the Fields parameter. If it's empty, we want to return a list of ExpandoObject with all the public fields. So, we use a bit of reflection on the type of Tsource. GetProperties will return all properties on Tsource. We want to return all public instance properties. Private properties and static properties shouldn't be returned. That call returns an array of PropertyInfo objects, and these we put in the PropertyInfo list. If fields are passed in, you have a bit more work. You must run through that. So we split them up, and run through them with a foreach loop. In that loop, we trim the field name so leading and trailing spaces are ignored. To get a property value we can use reflection as before. This time, we call into GetProperty on the type of the source object. In that GetProperty method, we pass in the property Name. And this will return PropertyInfo object that contains information on that property. We want to ignore casing of the field name and return only public and instance properties, just as before. We check if it is found, and if it is, we add it to a list of PropertyInfo objects. Our PropertyInfo list. So now we have a correct list of PropertyInfo objects. In both cases, when fields are inputted or when no fields are inputted. Up next is creating the ExpandoObjects themselves, and we need to do that for each object in the source list. So we run through our source list. For each object in the source list, we new up an ExpandoObject. We need to get the value of each property from the list of requested properties on the source object. And we have that list as a list of PropertyInfo objects, so we run through that. We can get the actual value of a property by calling into GetValue on the PropertyInfo object, passing in our source object. And now we need to add this property to our dynamic object. An ExpandoObject, underneath the covers, uses a dictionary of string object to hold its properties. We can cast it with that, and then add our property. The key of the dictionary holds the property name, while the value of the dictionary holds the property value. And with that we have our new data shaped object. We added to the list that must be returned, and then we returned the list. And that's it for our data shaping functionality. Now we can use this to shape resources in the next demo.

Demo - Shaping Collection Resources
In this demo, we're going to add data shaping support to our API. The first thing we need, and this is becoming predictable, is a way to pass the fields we want to return to our API. So let's add a string parameter fields to the AuthorsResourceParameters class. Let's open the GetAuthors action on our Authors controller. Right before our IEnumerable of AuthorDTO is returned, we call into our newly created ShapeData methods passing through the fields parameter to shape our results. One more thing we need to do is make sure we return a bad request from our GetAuthors action when a consumer of our API inputs an invalid field name. Theoretically, we can reuse the check we previously wrote for checking the OrderBy class. However, that wouldn't be correct. It'll work because we stated that we need a mapping for each property. So for all our fields on the DTO, mappings will exist, but our data shaping component doesn't use that mapping dictionary. So if you have a resource collection that supports data shaping, but not ordering, well, we're out of luck. And it's also not a good separation of concerns. Thus, it's better to keep these separate. What we need to check is that, for a given string of fields, all properties matching those fields exist on a given type. So that sounds like something we want to create a service for. Like that, it'll be reusable and injectable wherever we need it. Let's name it TypeHelperService. We add one method to it, TypeHasProperties, which returns a Boolean and works on a given type, D. So how does the implementation look? Well, it's much like our ShapeData method, so let me paste that in. We're again using some reflection, so we need a using statement to System. Reflection. If no fields are passed in we can safely return true. If fields are passed in, we split the field string and run through all the fields. Each of them is trimmed to get the property name. And then we use GetProperty to check if a property with that property name can be found. If one can't be found, we return false. And if all checks out, we return true. And that's it for that method. Now let's define it on the contract and interface. Let's name that one ITypeHelperService. It's an interface with one methods, TypeHasProperties. And then let's have our TypeHelperService implement this interface. Like this, we can register it by interface on the container. And as we know by now, that's done in the ConfigureServices method on the startup class. Just as our property mapping service, this too is a lightweight and stateless service. So Transient is a good lifetime for this. Back to our controller, where we can now inject this service. In the GetAuthors method, we call into the TypeHasProperties method, passing in the AuthorDTO's type and fields as parameters. If it doesn't check out, we return a bad request. One thing left, and you know what's coming. We have to ensure that our previous and next page links also contain the fields parameter as query string. And we do that for the next page and the link to itself as well. And that should be it. We already applied ShapeData to the Authors we returned from our GetAuthors action, so let's give this a try. Let's try selecting only the Author IDs when we get the authors. We do that by passing in ID as a parameter value for the fields query string parameter. And that's looking good. We only get back a list with six IDs, the IDs of all our authors. Now let's try combining this with Name by passing in ID and Name, separated by a comma. And now we get back the IDs and the Names. But something's off. Our field names are no longer Camel-cased. Let's have a look at why that's the case, and what we can do about it.

Demo - Camel-casing Dictionaries When Serializing to JSON
Our field names are no longer CamelCased. In this demo, we'll solve that. They are no longer Camel-cased because the default contract resolver that serializes the output doesn't correctly work with the dictionary. The key is serialized as is instead of being CamelCased, and as an ExpandoObject uses a dictionary underneath the covers, where the key is the property name, this is the result. But, no worries, we can change that by using another contract resolver. Let's open the ConfigureServices method again on our startup class. By calling into AddJsonOptions, we can configure the options for serializing to and from JSON. The options object has a SerializerSettings property, and on that SerializerSettings property, there's a contract resolver property. We can set this to any contract resolver we want to use. What we want to use is the CamelCase property names contract resolver from Json. net that's found in the Newtonsoft. Json. Serialization name space. All right, let's give that another try. Let's send that same request again. This time, the field names again start with a lowercase letter, as expected. And with that, we know how to implement data shaping, including a fix for our serialization issue. It's perfectly valid to use this on a single resource as well, so let's check that out in the next demo.

Demo - Shaping a Single Resource
In this demo, we'll implement data shaping for a single resource. We're in the GetAuthor action on the Authors controller. The first thing we need to do is allow passing in the fields we want to see returned. We don't need a containing class like AuthorsResourceParameters this time. There's only one that can be passed through, and that's fields. And now we need another extension method. This time, one to shape one object and not an IEnumerable of objects. So let's add a class, ObjectExtensions. We make this static as it will contain an extension method. And let me paste that one in. It'll look fairly familiar. We're again using some reflection, so we need the using System. Reflection statement. And it's a ShapeData method that returns one ExpandoObject instead of an IEnumerable of them, so we also need to import System. Dynamic. What we do here is almost exactly the same as what we did for an IEnumerable in the previous demo, save for the fact that we don't run through a list. So instead of repeating all that, let's focus on why we use two different methods instead of only creating one. This one an object, and reusing this one for all the items in a list in the previous demo. The reason is performance. As explained when we implemented the ShapeData extension method on IEnumerable of Tsource, reflection is expensive. So in IEnumerable of Tsource, we get around that by saving a list of PropertyInfo objects and we reuse them for each object in the list. Were we to use ShapeData extension method on object, for each object in the list of Authors, we would fetch the PropertyInfo, so reflection, for each object in the list of Authors. By creating two extension methods, we avoid this performance impact. Okay, let's get back to the GetAuthor action on our Authors controller. The first thing we have to do is check if the fields are valid. We already have a service for that, our TypeHelperService. If this doesn't check out, we return a bad request. And then right before returning an okay, we call into ShapeData on our Author object. We pass in the fields parameter. Okay, let's give this a try. Let's try getting the ID and name of a single author. And there we go, this works as expected. And now, if we try fields that should be invalid, FirstName, we get back a 400 bad request. So, so far, so good. An interesting fact here is that by allowing data shaping, we could potentially violate a rest constraint. We have already covered this before, in the second module. We're currently in violation of the manipulation of resources through the presentations constraint that said that, when a client holds a representation of a resource, including any possible method data, it should have enough information to modify or delete a resource on the server, provided it has permission to do so. Let's send that previous request again. We are currently not adhering to that yet. We're just returning an ID. And to adhere to it, we should either include a resource URI in the response, or better yet, support HATEOAS. That's coming up in the next module. But let's imagine the ID is a resource URI for a second. We can't send this request only asking for the name. This time we don't even have that ID anymore, and that can't be good. So I just want to mention this here because not all APIs support HATEOAS, and if you don't want to support HATEOAS, and you're opting to add the resource URI to each response, just make sure you don't allow data shaping to omit this URI. And this kind of proves that it's not always that simple to adhere to these constraints and design a truly RESTful system. Before we know it, we introduce functionality that breaks our architectural style. Anyway, so far for data shaping. Now, if you've worked with third-party APIs before, you know there's other types of functionality that are often exposed by RESTful APIs. Let's have a look at those.

Exploring Additional Options
Let's explore a few additional options. We ended with data shaping, but that same principle can also be used to automatically expand or include child resources. One option would be to declare an additional expand parameter, and as a value, the child property you want to see included. The books for an author, in this case. Now those expanded resources can also be shaped. In the example on screen, we don't use a new expand parameter. Rather than that, we expand the possible options for the fields parameter. If it includes a field of books, it should automatically include the books collection. And something else that's sometimes seen on APIs is using more complex filters like a contains filter. The query in this example should contain all Authors with a genre that contains the word horror, and not only exact matches. The more of this we allow, the less we might need searching functionality and so on. If you want to implement functionality like this, you can build upon what we did in this module. The principles, by now, should be quite clear. But think well about how you want your API to be used. There's no need to implement all of this functionality if it's not needed. Just remember, pause the parameters, fire a query string, and only use fields that are part of the resource, not of objects related to lower level layers. Let's have a look at the summary.

Summary
We started out with sorting. Collection resources are typically sorted on multiple fields, and we should take the sorting direction into account. We also looked into data shaping, the principle that allows the consumer of the API to select the fields he or she wants to see in the resource representation, rather than automatically returning all the fields. For both types of functionality, it's important to only allow fields that are part of the resource, and not of objects related to lower-level layers. We mentioned HATEOAS a few times in the second module, and when shaping resources in this module. We can use it to adhere to the manipulation of resources by representations constraint.

Getting Started with HATEOAS
Coming Up
Hi there, and welcome to the Getting Started with HATEOAS module from the Building a RESTful API with ASP. NET core course at Pluralsight. I'm Kevin and I'll guide you through this module. Only one subject this time: Hypermedia as the Engine of Application State. One of the sub-constraints of the uniform interface constraint. In this module, we'll cover exactly what it is, what it helps with, and how we can implement it in different scenarios. It's quite a big subject, so let's dive in immediately.

Hypermedia as the Engine of Application State
Currently, our APIs already quite good but it's not exactly evolvable nor is it RESTful. HATEOAS will help with this. It makes APIs evolvable and even self-describing. Hypermedia like links drives how to consume and use the API. It tells the consumer how it can interact with the API. Can I delete this resource? Can I update it? How can I create it? And how can I access the next page of data? And so on. But, what problem does it solve? Well, imagine we want to allow the consumer of the API to update or delete a book. Currently it does not know how to do that by looking at the response body. It has to have some intrinsic knowledge of how our API contract is structured. From contractor documentation, the consumer can learn that he'd have to send a book or batch request to API, authors, author ID, books, book ID, to update a book with a specific payload. And delete request to delete it. But it cannot get this information from the response itself. The consumer has to know this in advance. Now imagine another case. Let's say we have an additional field, NumberOfCopiesInLibrary on our book. If there are still copies in our library, somebody can reserve a copy. This is obviously an optimistic example, set in the hopefully near future, as "The Winds of Winter" hasn't been released yet. Anyway, in this case, the consumer of the API will have to know it has to inspect that field before it can send a post request to, say, api/bookReservations'. And that's again intrinsic knowledge of how the API works. But say this changes. Say an additional rule is implemented. There have to be copies left, but if the user wants to make a reservation for a book that's marked with content mature, he must be older than 16. This rule will effectively break the consumers of the API. They have to implement that on their side as well. In short, if an API doesn't support HATEOAS the consumers of the API have to know too much. And if the API evolves, this might break the consuming applications because the assumptions made by those applications can become invalid. And that's the issue that HATEOAS solves. A lot of these assumptions can be thrown overboard by including links in the response that tell the consumer which action on the API is possible. And those links, well, that's hypermedia. So HATEOAS allows the API to truly evolve without breaking consuming applications. And that in the end results in self-discoverable APIs. So in our example, we could add an additional property, links to the response. And the client would just have to inspect these links. For example, the first link would be a link to get to resource itself. I abbreviated it a bit in the code sample for readability, but it must be a true link, so with the correct host and Guids for the author and the book ID. To update a book, you fool with put and partially with batch, links are added as well, if that functionality is offered by the API. And if deleting a book is allowed, we add link with URI and meta to delete a book. And if a reservation link appears, a reservation can be made for this book. So it's up to the server to decide whether or not to show this link. The consumer needs no knowledge about that business rule. And moreover, the rule can change without having to redo anything at the client level. If all of a sudden we expand the rule, stating that there must be copies left and that the user must be over 16 to reserve a book marked with content mature, this is no longer something that has to be checked by the client. The server would simply not include that link to make the reservation and the client only has to inspect the link to say, show a button to make the reservation. Let's have a look at another Roy Fielding quote, the guy who invented REST. "You can't have evolvability if clients have their controls baked into their design at deployment. Controls have to be learned on the fly. And that's what hypermedia enables. " And this quote does go quite far. If we match this to building applications, we're almost talking about self-generating client user interfaces. Most apps don't go that far. But, as the example we used teaches us, for things like rules that change, this is pretty great. And, additional pieces of functionality can be added. For example, marking a book as one of your favorite books. In that case, an additional link will be provided and this will not break existing client applications. But, from that moment on, they can implement this functionality starting from that link. We'll get back to this sample later on. If you think about all of this, this is actually nothing more than how we should work with the HTTP protocol. As we know, RESTy folks, any much of how a good web app should work. Well, on the app, it doesn't really matter if a link changes, we start at De Tijd, where I have a newspaper site for example. And from that we navigate to an article by clicking a link or submitting a comment to a forum. And that's two examples of hypermedia driving application state. And, if the server decides that the link should change, well, to request that we turn to route page, we'll contain that new link. The browser, which is our application in this context, does not break because that link changed. Instead, it's the hypermedia in the response that's used by the browser to show us what we can do next. Okay, now what do these links look like? JSON or XML don't have a notion on how to represent links. But HTML does, the anchor element. In HTML, href contains the URI, rel describes how the link relates to the resource, and type, which is optional, describes the media type. For supporting HATEOAS, this format is often expanded upon. Let's have a look at one of the links from our previous example again. They all follow the same principles. The method property defines the method to use. Rel defines a type of action. This is what clients look for in the links list. Href contains the URI to be invoked to execute this action. The client simply uses this link, he doesn't have to create it anymore. Mind you, HATEOAS does not completely lift a client of having to have some knowledge of what to expect. It still needs to know about the link types that can come back. In other words, the rel values and whether or not it wants to use them. But if we're no longer hard coding URIs and assumptions in the client, a URI or assumption change no longer breaks the client. Now, if this were a collection resource like our authors resource, this is also where we'd include the pagination links. So they're no longer in the pagination header, they're now in the links area on our response. For collection resources, we will need some sort of envelope, an object to hold the value, which has the list of authors and the links. Were we to just add an area of authors and a links property, which is an area of links, we'd have invalid JSON. And this might be the moment when you're thinking, Wait a minute, this does not make sense. When we learned about paging, we learned that we couldn't use an object like this because that would break the self-descriptive message constraint. After all, when requesting the authors with media type application JSON, the representation should be an area of authors. This isn't truly RESTful, but that can be fixed. We're covering that next. For now, let's see how we can implement this in the next few clips.

Demo Introduction – Supporting HATOEAS
In the upcoming demos, we'll change our API so it adheres to the hypermedia as the engine of application state constraint. But, how are we going to implement this? The logic for creating the links can't just be automated, as it will depend on the business rules. For example, a book might contain links to update or delete it but also a link that's supposed to book reservations. Which only appears when some rules are met. We can't just guess what these links might be for each resource. We'll have to code this ourselves. That said, what remains a fact is that we have to add the links to the output. So, there should be a links property on each representation we're sending back to the consumer of the API. Essentially, there's two approaches I often see used for this. And, coincidence or not, we've got a case for both. The first one, a statically typed approach, involves working with base and wrapper classes. If you think about our books controller, the get book for alter action returns a list of BookDto. We can ensure that that BookDto class inherits a class that contains links. So they can be serialized. And get books for author returns a list of books, so that's an action for which we'll have to wrap the results in a containing class so we can include the links. Second approach is a dynamically typed approach. It involves working with anonymous types and dynamics. For example, an ExpandoObject. If you think back about what we did with the get author action on our authors controller, we remember that it no longer works with the author Dto. It works with the ExpandoObject because we shaped the data before we return it. And that's a dynamically typed class. As it's an ExpandoObject, we can add links to it. For collection resources, we can wrap that in anonymous type. So, let's have a look at both approaches.

Demo - Supporting HATEOAS (Base and Wrapper Class Approach)
In this demo, we'll work towards supporting HATEOAS using the base and wrapper class approach. First, let's add a class for our link to the models folder. We'll name it LinkDto. It should contain three properties for the three values we talked about on the slides. Href, rel, and methods. We'll also give it a constructor, accepting three parameters for these three properties. So we can easily construct it. Then let's add another class. We'll name this one LinkedResourceBaseDto a base class for our linked resources. We make it abstract because we don't want this to be used on its own. Other classes should inherit it. We give it only one property: links, a List of LinkDto. Then let's have our BookDto class inherit this class. Now we can add a collection of links to the BookDto. So, on to our books controller. We're going to have to create these links. We'll need a URL helper for that. So let's inject it. And let's add a method to create these links for a given BookDto. It returns that BookDto after the links have been added. And let's name it CreateLinksForBook and as a parameter, it accepts a book Dto. The one we're going to add the links to. So the final statement will obviously be return book. Now this is where the custom logic would go. Here we decide on which links should be returned when a consumer of the API gets back a book or presentation. And the first link should be a link to itself, i. e. where we can get this book resource. And for the other links, we need to look at the functionality our API exposes. So let's collapse our codes to the definitions. When we return a single book, our API allows a full update, a partial update, and deleting of that book. So we need links to those in our link collection. To create URI through the URL helper, we have to be able to access the routes to these actions. So we need to give them names. And let's start with that. "GetBookForAuthor" to return a single book already has a name, so the next one is "DeleteBookForAuthor". Now we have "UpdateBookForAuthor" and "PartiallyUpdateBookForAuthor". But, what about creating a book? Well, we're covering that later on. Because creating a book doesn't really belong in the links related to a single book. For now, let's add the links. The first one is a link to itself. We call add on the links collection. A book now has that as it inherits the linked ResourceDto base class. To add a link, we must input a value for href. That is the URI to the action for the link. So we pass in a route name, "GetBookForAuthor" and we pass in an object containing the book ID. Then we need a value for rel. We can choose this ourselves, self in this case. This is the part that consumer or client will still have to know about. Because it's this that will be used by the consumer to see if a specific piece of functionality is offered by the API. And lastly, we input the method, 'GET'. Let me paste in the others. For delete, we need the link to the delete book for author action, passing in the ID. We'll give this a rel value of delete_book and the method, DELETE. To update the book, we need a link to the update book for author action. Also, needing the ID of the book. We name this update_book and the method is PUT. And same logic applies for PATCH, but this time we name it partially update book with underscores. Other links can be created depending on additional functionality and business rules. But for the book resource on our API, this is sufficient. Then, let's ensure these links are effectively created. And this should be done whenever a BookDto is returned. So that's when we return a list of books, one book, and right after we create a book. Let's start with amending the action we use when returning one book. That's to get book for author action. Instead of directly returning a book for author, we first create links for this book. And what we return now is not just a book, but it's the book including those links. And the same goes for post, ie; when we create a book for an author. Instead of directly returning the book to return, we first call in to create links for book. And we have one left, returning a book collection, that's to get books for author methods. Here we'll need to create these links for each book in the books for author list. So we run through them and call CreateLinksforBook on each book. For that we can use the select extension method. Alright, that should be it, let's give this a try. First, let's GET one book for an author. These are the same requests we've used previously in this course. Let's send it, and we get back our book, "The Winds of Winter", but with an additional links property. And this links property contains the links we added. Let's try creating a book by sending a post request to the books resource. We're going to create "The Restaurant at the End of the Universe". That's a Douglas Adams book. We get back a 201 created and the body of our response also contains our links. But what about getting all the books for an author? Let's send the get request to the books resource. This should give us back all George R. R. Martin's books. And we see that we indeed get back "A Dance with Dragons", "A Game of Thrones", "The Winds of Winter", and that's all that's in our library. We also see that for each book, the links are included. So that's great. But what we're not getting back are the actions on the books resource itself. We only have actions available on each separate book. We can't just add a links property to this JSON because it would result in an invalid JSON. So we're going to need a wrapper class. Let's add a new class and name it LinkedCollectionResourceWrapperDto. We'll make it work on a specific type, and it should inherit LinkedResourceBaseDto. We do this because our LinkedResourceBaseDto is an abstract base class that contains the links property. And we also want to make sure that the type it works on is in itself a LinkedResourceBaseDto implementing class. We do this to ensure that the Dto we're returning a list from will also have the links property. Next to links, this one only has one property, value. This must be an ienumerable of T, in the constructor we allow passing in this ienumerable. In case of a list of books, this will be an IEnumerable of BookDto. Okay, on to the books controller. We already have a create links for book method, now let's add an additional one, create links for books. This one works on our new link collection ResourceWrapperDto for BookDto in this case, and it also returns itself. That's the same as in the create links for book methods. So here we want to add a link to itself. For that we need to be able to refer to the get books for author action. So let's give that one a name, and then we add a link to itself. We already know how to do that. In the get books for author action, we then create a wrapper instance. The value should be our books for author list. And instead of returning the books for author, we now return the wrapper but first we call into CreateLinksforBooks on that wrapper. This creates the self-link to the books resource. Let's give that another try. Now let's send the request again. And this time we see an envelope, the value now contains that list of books including the links for each book instance. And if we scroll down a bit, we see that we now also have a links field, containing a link to the books resource itself. And with that, we've implemented HATEOAS support for our books by using a base class and a wrapper class. But there was another option, one that's suited for when you're not working with Dto's but rather with dynamics or anonymous objects like what we get when data shaping our resources. So let's check that out in the next demo.

Demo - Supporting HATEOAS (Dynamic Approach)
In this demo, we're going to work towards supporting HATEOAS using a dynamic approach. So this approach involves working with dynamics and anonymous types. And if we look at what we did with the get authors action, which is what you see onscreen now, we see that we no longer return a list of authorDto. The get authors action returns an ienumerable of ExpandoObject, the datashaped version of our authorDto. So we can't just extend this datashaped object with links property to a base class. For that, we'll use the anonymous type approach. But let's start with a simpler case, one author. That returns one ExpandoObject, a shaped authorDto. And what we want to do is add a property links with the list of links to the response body. So let's add a method to create these links first. Let's name it create links for author. It returns an IEnumerable of LinkDto. To create these links, we need the input parameters of the get author action. So Guid ID and field string. Let's add a variable to all the links, which is a List of LinkDto. In the end, we'll want to return this. Now, how do we create it? Well the first thing to create is the self-link. If the field string is null or wide space, we need a URI to the author resource. We can again use the URL helper for this. Likewise for when there is a fields value. The only thing that changes is that we now also pass through the fields to generate a link. Our API supports deleting a book, so that's another link we want to add. As we know, we need to be able to refer to the delete author route to create that URI. So let's give the delete author route a name. And then we can add a link. I'm pasting these in because this is exactly the same as what we did before. And this is where this is starting to shine. Once a client application has an author, we can decide we can now let the client application create a book for an author. So in the previous demo, I mentioned this was coming back. Well, this is where it is coming back. We are truly driving application state now. We're deciding on the functionality of the consumer. So we need to be able to refer to the route to the create book for author action. And that's on the books controller. So let's give it a name, create book for author sounds like the obvious name to choose. And back to our authors controller. And let's add the link. Next to this, we can also add information on how to navigate through our API. An author has a list of books. So we use the URL helper to create a link to the get books for author action. Like this, again, we're really driving application state through these links. And this is where HATEOAS shines. What's next is adding these links to the response body. Let's scroll up to the get author action. Let's create links by calling the create links for author method. We pass in the ID and the fields. Then we need to add the links. As we know, the shaped data action returns an ExpandoObject. And that's an IDictionary of string object. So we can use that knowledge. We call author dot shape data, passing in the fields. And we cast it to an IDictionary of string objects. And through this dictionary, we can then add a new key value pair. So let's add one with links as key and our generated links as the value. And then we return our linked resource to return. Before we test this, there's one more place where we return an author and that's after successful post, so let's scroll down to the create author action. We've got our author to return here. This is the object you want to add those links to, so let's generate them first. We pass in the ID and null for fields. There's no datashaping on post. Then we need to add these links to the response. We need an ExpandoObject for that. So we have to convert our AuthorDto to an ExpandoObject. And that's exactly what a shape data extension method does when we don't pass in fields. So let's call that. There had to be a good reason for including all of those checks when we created the shape data method, right? Well, this is one of them. We call add and pass in links as the key for the links property and the links list as value. And lastly, we want to return this linked resource, so we pass it in in the created add route action. This requires us to input the ID, we could just leave it at author to return dot ID, but it isn't exactly correct because we're passing in the ID of another object, even though it's in the same scope and in the same methods, I do prefer to keep the code as correct as possible. So we should pass in the ID from the linked resource to return. And we can get that because it's an ExpandoObject, which is a dictionary of string object, so we can use the indexer passing in the key, in this case ID, the property name. And there we go, let's give this a try. Let's get one author by sending a get request to a specific author. We get back George R. R. Martin, including the links to itself, the link to add a new book for George R. R. Martin, we may hope, and the link to his books. Now let's send a datashaped request, just to see if it still works. We are only selecting the ID and the name this time. And indeed, that works out as expected. We also see that the link to self now included fields equals ID, name. The nice thing is, with this approach, datashaping cannot ask to not include links. If you remember from the previous module, we briefly mentioned that datashaping could allow a user to omit the ID link or URI link, which would then violate the manipulation of resources by a presentation constraint. In this case, that's no longer the case. Even though we might omit the ID value through datashaping, we cannot omit the links through datashaping anymore. So that's pretty nice. And now let's post an author. Our response also includes the links, so this looks good. Up next is the authors resource. So let's open that author's controller again. The first thing we want to do is add a method to create the links for the authors resource. The GetAuthors method accepts an author's resource parameters instance. So we'll also need that to generate our links. The return value is again an IEnumerable of LinkDto. We'll name it create links for authors. And it accepts an authorsResourceParameters instance. Let's create a variable to hold those links, the new list of LinkDto. And let's already write the final statement, return links. Okay, so now we can create the links for the authorsResource. Or at least almost, because there's something else. Let's crawl up to the GetAuthors action again. We support paging on the authorsResource and when we talked about paging, we discussed the various means of returning metadata. And we learned that the best place to put this is in the header. But in fact we included things in the metadata that aren't really paging metadata. The previous and next page links, these are, as we know now, links that drive application state, so we can now put the previous and next links in the links area. That means our create links for authors method should also accept a hasNext and a hasPrevious boolean value. So let's add those. Okay, let's create links. First, a link to self. We've got this CreateAuthorsResource URI methods from when we learned about pagination. And this already creates a few links for us so we can use those to generate links. Let's just add another type, current, to the ResourceUriType numeration. And let's make sure that when that type is passed in, we return a link to the current resource. That's actually our default case. Okay, we've already got our URI generation for the links. Now let's add the links. Let's scroll down and let me paste that in, after all we've done that before. First we add the link to self. For this we create a new LinkDto and to generate the actual URI, we call into CreateAuthorsResourceUri. We pass in the authors resource parameters, and we state that we want the current link to link to itself. If there is a next page, we call into same create authors resource URI methods. This time, stating we want the next page. And we do the exact same to generate URI for the link DTO for the previous page. And that already takes care of link generation. On to the get authors action. We want to add the links to the response body but we also want to wrap the authors in an envelope, just as we did with books. First, let's change the pagination metadata, link generation for previous and next page can be removed and those two values can also be removed from the metadata object. They are, after all, now included in our links area. Then we create links by calling our create links for authors methods. As a second parameter, we pass in the hasNext value from the authors from repo paste list. And as the third parameter, we pass through hasPrevious. From that same page list. Then we shape the data. This returns an ienumerable of ExpandoObject. And for each of these, we need to add the author links. So we call select on our datashaped author list. We cast each author to an IDictionary of string object. For each author we then create links, this requires us to pass through the ID, which is a Guid. We can get that ID from the author calls to a dictionary. And we cast it to a Guid. And as a fields parameter, we need to pass through the fields from the authors' resource parameters. These assure us that the links that are generated for each author also match the requested fields for those authors. And then we add the author links. And we return the author as dictionary. That takes care of creating our shaped authors with links list. So we now have the links for the authors' resource, we have the shaped authors themselves, and for each of those shaped authors, we have their own specific set of links. That leaves us with thinking about what we actually need to return and that's a resource with this shaped authors with links list as a value for the value field, and the links for the authors resource as a value for the links field. So we create a new anonymous object and we return that object. By the way, it's not because we've now used this approach on authors and the base class approach for books, that we couldn't use the dynamic and anonymous type approach for the books as well. In fact, I'd advise you to use one of both rather than mixing both of these approaches. But I wanted to show you how to use them both. The approach you want to take tends to depend on the architecture of your API and the functionality it offers. Anyway, that should be it. Let's build a run and give this a try. Let's send a request to get the authors. As a value, we have a list of authors and each author has their own set of links. And if you scroll all the way down, we see that the authors resource itself also has an area of links. Now let's try a paged, ordered, filtered and shaped request. So what we get now are the ID and named fields of each author. We see that in the self-link for a specific author, the fields to take into account when datashaping have been added. We only get two authors, Neil Gaiman and George R. R. Martin, as the page size we've asked for is two. And we see that the link to this authors resource contains the same parameters we've inputted in our get request. Just one thing left, it would be nice to see a previous or next page link here. We only have two authors that match this request, so let's change the page size to one. And if you scroll down now, we see that this time we get a next page link as well. So we're already pretty far at driving application state, via these links. We're now always returning these links, but should they really be part of the resource presentation? When asking for application JSON, we already learned that what we're doing now isn't the correct implementation. To fix that, we'll have to dive into media types a bit deeper. And that's coming up next, in the Advanced HATEOAS Media Types Inversioning module. But first, let's look into this module's summary.

Summary
This module was focused on HATEOAS. Hypermedia, like links, drive how to consume and use the API, and the functionality of the consuming application. A good example would be a piece of functionality that allows someone to make a book reservation without HATEOAS, the consumer of the API must know how to do that, typically from documentation. So it needs intrinsic knowledge of the API. Even if functionality and business rules change, client applications won't break as they need to inspect the links they get back from the API. These links are included in the response body. We implement this using two approaches. One using a base class, and one using dynamics. But even though we kind of have HATEOAS support now, we ended the last demo with a bit of an issue. We're asking for a media type application JSON, but we're returning links and sometimes even an envelope. And that can't be right. That brings us to a perfect use case for using media types correctly, instead of just returning anything as long as it's in JSON format, when asking for application JSON. In the next module, we'll keep building on our HATEOAS implementation and look into media types and versioning. All of these are related, as you'll notice.

Advanced HATEOAS, Media Types, and Versioning
Coming Up
Hi there, and welcome to the Advanced HATEOAS, Media Types and Versioning module of the Building a RESTful API with ASP. net Core Course at Pluralsight. I'm Kevin, and I'll guide you through this module. We ended the last one with a bit of an issue. Are the HATEOAS links part of each resource or representation? We'll look into that by looking into content negotiation again. That also means we'll dive a bit deeper into media types in this module, which quite naturally leads to versioning. Let's get started.

HATEOAS and Content Negotiation
Onscreen is part of a response from a GET request to the author's resource. We've got two fields, value containing the authors and links containing a self-link, which you see onscreen. The links are part of the resource body. That begs the question, Is this really the correct place to put these links? If we think back about pagination, we talked about where the pagination information should go. We concluded that it's metadata, so it should be in the header. It describes the resource. And that's true for fields like total count, current page, and so on. But what about those next page and previous page links? We put the paging links in the response body and kept the rest of the paging information as metadata in the header. So are these links part of the resource or do they describe the resource, i. e. are they metadata? Same goes for the other links, links to self and so on. If it's metadata describing the resource, they should go in the header. If they are explicit parts of the resource, they can go in the response body. What we're actually dealing with here are two different representations of the resource. With content negotiation, we ask through the accept header to get back a JSON representation of the resource. We ask for media type application/json, but what we return isn't a representation of the author's resource. It's something else. Additional semantics, the links, on top of the JSON representation. So the links should not be part of the resource when we ask for media type application/json. We've effectively created another media type which were wrongly returning when asking for application/json. So by doing this, we're breaking the self-descriptive message of constraint, which states each message must include enough info to describe how to process the message. If we return a representation with links with an accept header of application/json, we're not only returning the wrong representation, the response will have a content-type header with value application/json, which does not match the content of the response. So the consumer of the API does not know how to interpret the response judging from the content type. In other words, we also don't tell the consumer how to process it. The solution is creating a new media type. So how do we do that? Well, there's a format for this, a vendor-specific media type. You can see an example of that onscreen. First part after application is vnd, the vendor prefix. That's a reserve principle has to begin the mime type with. It indicates that this is a content type that's vendor specific. It's then followed by a custom identifier of the vendor and possibly additional values. A good one to use in our case would be vnd. marvin. hateoas, as you see onscreen. Vnd plus my company, which happens to be called Marvin, the paranoid android from Douglas Adams' Hitchiker's Guide to the Galaxy books and then followed by HATEOAS, stating we want to include those resources links. Then we add a plus sign and JSON. What we're actually stating here is that we want a resource representation in JSON with HATEOAS links. If that new media type is requested, the links should be included. The consumer must know about this media type and how to process it. That's what documentation is for. If this media type isn't requested, so we simply request application/json, the links should not be included. Let's see how we can support this with a demo.

Demo - HATEOAS and Content Negotiation
In this demo, we'll build the final part of the HATEOAS support for our API. We'll make sure that the additional information, the links, are only returned when the correct media type is requested. So when requesting application/json, only the JSON representation of the resource should be returned. And when requesting vnd. marvin. hateoas+json, the links will be included. We're back in the Get Authors action on our author's controller. The first thing we need to do is get that accept header because our decision depends on that. To do that, we can accept an additional parameter using the FrontHeader attribute. That attribute tells the framework that this parameter should be bound using the request headers. We pass in the name of the header, accept. And then we can use this value. Let's scroll a bit to where we currently return the authors with links. And let's surround this with an IF statement. This piece of code should only be executed when the media type parameter equals our custom media type. We must not forget to include the pagination metadata in this case. That will also change depending on the media type. So there we go. If the media type is our newly created media type, we create pagination metadata without the previous and next page links. All the rest is the code we ended up with previously. If we have a different media type, the results shouldn't react nor contain links, and that's actually the same code we had before we started implementing HATEOAS support. So let me just paste that in. And there we go. We create the previous and next page links, add it to the pagination metadata, add that as a header, and then return the shaped author's resource. All right, that's it already. Let's build and run and give this a try. First, let's get the authors with an accept header of application/json. We see that we simply get the authors back. If we look at the headers, we have the X-pagination header with previous and next page links. Both are no, because we only have six authors in our database. There are no links in the body, nor is there an envelope. What if you tried changing this accept header? This time, the accept header is our new media type. We get back a 406 Not Acceptable. What's happening? Well, we just used a custom media type, but unlike common types like application JSON, and application JSON patch plus JSON. This custom media type isn't linked to a formatter yet. So, let's open the configure services method on the startup class. We've worked with formatters before, to support XML, input and output. Now, you want to get the JsonOutputFormatter, and we want to add our custom media type to the supported media types for that formatter. You get the formatter from the list of output formatters on the setup action. And we search for one OfType, JsonOutputFormatter. If it's found, we add our new media type to the supported media types collection. Let's try this again. Let's send that request with our custom media type as the Accept header again. This time, you get a 200 okay, containing the list of authors, with links for each author as value, and links related to the author's resource as the value of the links field. So, this looks good. Up next, we'll be doing the same as we just did for the author's resource on GET and on POST. But, we already know how to do that, it's just a bit of copy, paste. So, let's leave it at this. One thing left, though, to be self-discoverable, we need a route document. That'll be the only URI the consumer has to know to start navigating. Let's check that out in the next demo.

Demo - Working Towards Self-discoverability with a Root Document
In this demo, we'll add a starting point for the consumers of our API, the Root Document. From that Root Document, the consumer can learn how to interact with the rest of the API. This document will live at the API Root. So let's add a controller for this. RootController. That's a new class and we name it RootController. As it's a Controller, it must inherit controller. We'll at a route attribute to API. And then we'll inject the urlHelper, because we need that to create our links. We only need one action. Let's name it GetRoot. It should be executed on a get request to /api. So, we'll use the httpGet attribute. We'll immediately give the route a name as we'll want to refer to this when creating the links. And just as in the last table, we should only return links when our custom media type is requested. So, we accept a string mediaType, and we use the FrontHeader attribute to say it should get its value from the Accept header. Then, let's generate these links if our custom media type was passed in. What we want to generate are links to the document itself, and links to actions that can happen URI's at root level, or that aren't accessible otherwise. In our case, to start with, that means a URI to authors. As the books are children of an author, and a consumer of an API can get to them after requesting the author's resource. So, the first one is linked to the root document itself. And then, there's that action to get the authors. There's no other resource the consumer has to get first, to be able to create an author. So, this is a good place to put it. To be able to do that, we need to be able to refer to the CreateAuthor action on the Author's controller. So, we give the route a name, and then we can go back to our rootController. We add a link to that CreateAuthor route, and we give it a real value of create_author, and the method is POST. When the links have been added, we return them. If the media type is something else than our custom media type, we should just return an empty response. Let's build and run, and let's give that a try. We've got a request lined up here to the route of the API, with our custom media type in the Accept header. And we, indeed, get back our three links. That's to root documents. And from here, consumers of the API can start interacting with the API. And this looks kind of good. But, are we really there now? Is this standardized? Do we know from this what to send to it and update or a post requests? Well, let's dive a little bit deeper into media types to try and answer that question.

Revisiting Media Types
On screen, we see part of the response body for a book, requested with our new media type in the Accept header. There's two issues with this, first the format on the links. This format isn't a standard, it's something we're cooking up. The client will have to know how to interpret this format. And it learns that from the API documentation where we describe the custom media type, and where we describe Rel Values. As we learned before, media types are also part of our outer facing contract. And then there's another issue. Hyper media allows the application controls, the links, to be supplied on demand. So, say we have a link to update the book, as we see on screen. How does the consumer know what exactly to send as far as the representation goes? We touched upon media types before, and we learned how to create our own media types. It defines the presentation of a resource. It's a central principle in the RESTful world. And we actually already have a great example we can use to explain this. Remember that we have different representations for getting an author and creating an author? When we get one, we get back an id, a name, an age, and a genre. To create one, you must pass in a first name and last name, a date of birth, and a genre, but no id. We just used application/json as media types for both of these, as most APIs do. But, in fact, these are different representations of the same author's resource. Using application/json is actually a mistake. That just tells us we want data format to be JSON. But we need to be more specific. So how do we correct this? By using media types. Instead of using application/json, you could create vendor-specific media types for these representations. An author representation to get can then be represented with media type author. friendly, for a friendlier format with age and name. And alt for the presentation to create can then be represented with media type author. full. And we can go even further. We could support different representations when we get an author. Pass in the author. full media type, would return the author with first name, last name and date of birth. We just learned how to work with these media types in the previous two demos. So, we already know how to support this. And this almost automatically brings us to versioning. After all, we are talking about an evolvable API. And that includes changes in representations.

Versioning in a RESTful World
When we talk about versioning, we're actually talking about adapting to change. An API changes. What we can do with the API changes. We may be allowed to create an author today, but we might not be tomorrow. Or resources might be added or removed. Those are functional changes. Related, are changes in business rules, like in our example in the beginning of the module. The minimum age to reserve a book might change. And then there's changes in resources or representations. Sometimes, fields are added or removed. We need to be able to adapt to this without breaking consumers of the API. There's various ways to version an API. Let's have a look at a few. One way is to change the URI for each version. So, v1 would have v1 in the URI. Version two would have v2, and so on. Another way is by working with query string parameters. The requested version is added as a query string parameter. And yet another way is through a custom header. The request would have a new header, say api-version, and that header contains the requested version. The thing is, we're not just building any type of API. We're building a RESTful system. And there's not so many ways to do it in a RESTful manner. In fact, let me quote Roy Fielding once again. In August of 2013, he delivered the key note talk at the Adobe Evolve conference. When he was asked, how we should version RESTful APIs, his answer was, "Don't". I read that and I thought, Well, this doesn't make sense. One of the principle rules when building APIs is, always version the API. Stuff changes, we should be able to adapt to it. He added to his "Don't" answer, by asking the question, When was the last time you saw a version number on a website? Well, he does make a great point. But, there's a bit of an issue with this currently. How do you adapt to change in a RESTful world then? Well, change, what we actually want to control when talking about versioning, comes in those flavors we saw in the previous slide. And we actually already know how to tackle this. If functionality changes, the HATEOAS links take care of this. If business rules change, same story. The applications that consume the API must look at these, and if they do, they will not break in front of these changes. But what if resource or representations change? Well, that is still a problem. According to Mr. Fielding, these should not be versioned either. But to allow for something like that, we're back to our initial problem. How can we let consumers of the API know what the representations should look like. And moreover, how can we make sure these consuming applications automatically evolve when the representation evolves? The option to fix this, according to Mr. Fielding, is the Code on Demand constraint. That's the optional constraint that stated that the API can extend client functionality. This is a good fit for java script based applications. The consuming application downloads to java script, and it's up to the API to change this java script, when a resource or representation change. The java script could then contain object definitions for each of these media types. From that moment on, we will be truly evolvable. But that's java script. It's a different story when you need to support different types of clients built in different languages. For public RESTful APIs, some you might not even know about. Moreover, all these types of applications will almost have to be alter generated. An additional field would mean an additional text box in the UI. So, if the resource changes, well the UI should adapt. Think back about that image of a well-designed web app, rest evokes. The browser gets back a response from the server, interprets it and shows us the web page. It fully generates the user interface from the representation by interpreting HTML, Java Script, CSS, and the likes. So, what Mr. Fielding states makes sense. But, more often that not, our applications don't self-generate. What we build today is, well, not ready for this yet. So, it's a great idea and it leads to true evolvability. But currently, it's not feasible for the majority of applications we build. So for this specific part, I prefer to divert a bit, from the best practice Mr. Fielding suggests. So what do we do then? Well, for resource representation changes, we can use versioning on that level. It's a new type of representation, so we create a new media type. Instead of versioning the URIs, we version these representations through media types. For our friendly version of the author, we'd add the version number to the media type. And to ensure that the clients don't break, we support both versions, of course. So, if we want, can still be used by existing applications. And v2 can be used when these applications are changed. This is, in fact, exactly the same as using different media type names like, friendly author, or full author. We're just adding a version number now to differentiate them, but technically it's the same thing. So, you can still use friendly names if you wish. We already covered how to work with these media types in the previous demos, but, as this is such an important principle, let's do one more demo on working with different media types. This time, focusing on the part we haven't covered yet, dealing with input.

Demo - Media Types and Versioning
In this demo, we'll look into using different media types for different versions of input and/or output. We've made a small mistake. Let's open the author entity. It's a shame, but people die, and so do authors. Currently, we have a date of birth, but no date of death. And that gives us faulty results. Douglas Adams, who is in our database. Well, he isn't with us anymore. And the age calculation is just wrong. What we're going to do is add a date of death field, for our authors in our database. And in the future, we want to ensure that when posting an author, this field can optionally be passed in. We don't want to break existing implementations though. A consumer should still be able to pass through an author without that field. For the sake of the demo, we'll just work with two possible classes for input. Let's start by adding the date of death field to the author entity. This should be a nullible value. Let's quickly build, and then, let's add a migration. That way, our database gets updated as well. We do that by executing the add-migration command in the Package Manager Console. So, we execute the command and we give the migration a name. AddDateofDeathToAuthor sounds like a good name. The migration has been added. If this is the first time you're using these migrations, well, once you call add migration, the tooling will create a class like this. It contains an Up method, which is code that will be executed when we migrate from the previous version to this one. And the Down method, which will be executed in case we would downgrade from the new version to the current one. So that takes care of that. But now we must, of course, also change the way we calculate the age. It's been a while, I think, from the beginning of the course, but we created the mapping from Author to AuthorDto, where we called into GetCurrentAge on date of birth, to calculate the age. It's this method, that we want to change. We want to get the current age by passing in an optional date of death. Let's open the method. We accept this new optional DateTimeOffset. And if the date of death isn't null, we set the date to calculate value to the date of death. That's it for the plumbing. Now, let's create a new class. Author for creation with date of death dto. This will be our version two class. It contains a first name and last name, the date of birth, the genre, and our additional nullible date of death property. Let's open the Authors Controller. We've got two headers that can contain media types, that we can possibly take into account. One is the Accept header, related to output. In fact, we already developed something like this, in our GetAuthor section. Here, we check the media type from the Accept header. We do that in the action. So, we use a FromHeader attribute on the parameter to make sure that the media type string gets bound to the AcceptHeader value. But, there's another header that can contain a media type. ContentType. The ContentType header signifies the input, an author for CreationDto. But unless you want to start working on object, we need to be able to accept two different types here. An author for CreationDto, and an author for CreationWithDateOfDeathDto. So, we need another approach. Let's add an extra action already, that accepts such an AuthorForCreationWithDateOfDeathDto. For now, let's just copy over what we have. We do have to somehow differentiate between these actions, depending on the content type header this time. And that's where action constraints come into play. These allow us to select an action depending on, for example, the content type header. In other words, we'd have an additional constraint on our action. Something along the lines of this, if the constraint matches, we can enter the action. Otherwise, we can't. Let's add a new class. We'll name that RequestHeaderMatchesMediaTypeAttribute. This is going to be our action constraint attribute, so we want it to inherit Attribute and IActionConstraint. IActionConstraint is defined in the Microsoft. AspNetCore. Mvc. ActionConstraints namespace. In our constructor, we want to accept the RequestHeaders to match, and the media types that will result in a match. But why use an area of media types? Well, we'll get to that immediately. But first, let's implement the IActionConstraint interface. That one contains an Order and an Accept method. The order property decides which stage the constraint is part of. Action constraints run in groups based on the order. For example, all of the framework provided http method attributes, used the same order value, so they're not in the same stage. And what we want, is for this constraint to run in that same stage as well. So, we returned zero. The Accept method returns a bullion, if we want the Route attributed with this constraint to be matched, it should return true, and otherwise false. So, let's implement that. We want to check if the RequestHeader to match contains one of the media types as values. So we get the headers. We can get to that via context. RouteContext. HttpContext. Request. Headers. If the RequestHeaders to match, for example, ContentType as input isn't found in the request, we return false. Otherwise, we run through our media types, and we check each with the value of the RequestHeader. If a match is found, we return true. If at the end, no match is found, we return false. And that's it for our constraint. On to the AuthorsController. Let's have a look at the existing action first. CreateAuthor. This one should stay as is, but it should now only accept application/vnd. marvin. author. full+json as ContentType. So, we pass in Content-type, as the as the RequestHeader to match, and we pass in our custom media type as the only media type in the media type list. Our new action then should only accept requests which have another media type as content type. This time signifying that the author contains a date of death. That's it for the action constraints. But there is two more things we need. Let's open the Startup class. This time, we need an additional mapping, so our AuthorForCreationWithDateofDeathDto can also be mapped to an author by AuthorMapper. Then let's scroll up a bit to the ConfiguredServices method. We added a custom media type for our HATEOAS links to the JsonOutputFormatters. This time, we have two new media types as input, so we need to add those to the JsonInputFormatters. We do that by looking for the JsonInputFormatter on the InputFormatters list, and if it's found, we add our two new media types, and support media types. And that's it, but let's go back, for just a minute, to the AuthorsController. We have our CreateAuthor, and our CreateAuthorWithDateOfDeath methods here. If we open the CreateAuthorWithDateofDeath action, well, that's still exactly the same as the CreateAuthor action. So, interesting fact, is that the action content itself, hasn't changed yet. Both actions contain the exact same code. Depending on how we write our codes, and what components we use, it will change. We use AuthorMapping for mapping, and that's not a requirement. Other frameworks, or custom code might not be so friendly. And we currently don't have any validation rules on an author. If we were to implement a custom rule, we might need these to be different. For example, the date of death, should be higher than the date of birth, and so on. So, it really depends on what other components and frameworks we use. But, this does add to the fact that creating different actions doesn't necessarily mean that we have huge amounts of code to rewrite. In our case, we could simply separate these codes into one method, and call into it. The additional effort to correctly work with these constraints, can be very minimal. Which really makes this a good option to consider. Anyway, let's set two break points on these actions, and let's build and run. Reading post mail, and we've got our first request here. The post requests two authors. It'll create James Ellroy. But, if we look at the headers, the content type is applicationJson, let's see what gives. We get back is a 404 Not Found. That makes sense. There isn't an action that matches this content type any more. Let's try sending another request. This is the exact same request, so the body still contains James Ellroy, but in the header, the content type is now our new content type. The custom content type signifies the original author. Let's send this. And we hit our break point. We hit the original CreateAuthor action. So far, so good. Now let's try posting a new author. The content type is now our custom media type, AuthorWithDateofDeath. And if we look at the request body, we see that we now also include a date of death field. Let's send this. This time, we're in our new action, CreateAuthorWithDateOfDeath. Let's continue. And we see that JD Salinger has been added to our database. His age was 91 when he passed, so our new age calculation also works as expected. And that's it for this demo. We still have one question to answer though. Why did we allow passing in multiple media types on the action constraint? Let's check that out in the next demo.

Supporting Multiple Media Types
In this demo, Supporting Multiple Media Types, we'll learn how we can do this and why. We're in our AuthorsController again. We ended up with a custom media type that matches our class, AuthorforCreationWithDateOfDeathDto. The format of that type is JSON signified by +JSON. But what if our input is XML? We don't want to write an additional action. After all, an author with a date of death might be inputted in XML, or in JSON. That doesn't change the fact that it can be deserialized into that same AuthorForCreationWithDateOfDeathDto. Our InputFormatters take care of this. We can now add a new media type. The exact same one as we had, but this time, ending on xml. And let's add that media type to the list of XML InputFormatters. We create a new XML DataContractSerializerInputFormatter. Immediately add our new media type to its SupportMediaTypes, and add it to the InputFormatters list. Okay, let's give this a try. We've got one more request left. The content type header is set to AuthorWithDateOfDeath. full+xml, and if we look at the request body, that's an XML representation of our our AuthorForCreationWithDateOfDeathDto. Let's send this. And we hit the break point. We're again in our CreateAuthorWithDateOfDeath action. And the new author gets created in our database, as expected. Okay that takes care of input. But, what about output then? Should we create additional constraints, or should we just go for the approach we took for including the HATEOAS links? Let's open our code again. Well, to be honest, I feel the constraint approach is the way to go. Also, for the HATEOAS links. But it doesn't necessarily mean we have to create different constraints. We made our constraint in a way that allows us to input the header name to match as well. So, let's add another one of these. That does give us an error. Duplicate Use of Attribute. But we can easily solve that, by allowing multiple attributes of this type. Let's open this ActionConstraint attribute again. By using the AttributeUsage Attribute on top of our attributes, we can signifiy that we want to allow multiple instances of this attribute to be used together. Let's look back at our action. And we see that the error is now gone. Now, we're not going to implement this again. After all, it's nothing new. We already know how to do this from this, and the previous demos. And as you can guess, the same principles can also be used for input and patch. But, I did want to show you how easy it can be to work with these constraints. And with this, we see that REST, and the way most http APIs are built, are really two very different things. Let's look into some other approaches and options we have regarding HATEOAS and media types.

Other Approaches and Options
There are efforts on the way that describe languages for link representations. And there's even efforts to include media type descriptions in a resource or a presentation with links. So, we would know what to send to, for example, update a book. That avoids the need for separate documents documenting it. Let's have a look at a few notable ones. The first one, is HAL, or Hypertext Application Language. HAL provides a set of conventions for expressing hyperlinks, in either JSON or XML. So, it's a standardized format for those links. And I'm a big fan of adopting standards. But, this one is still in draft stages at this moment. Another notable one is SIREN, or Structured Interface for Representing Entities. It includes a way to format the links, and a way to describe what to send to those links. It's currently at version 0. 6. 1. So, it's also not quite there yet. There's. NET implementation for this, which you can find at the link on screen. And there's more. JSON-LD is a lightweight linked data format. And JSON-API is a specification for building JSON-APIs that also includes a way to represent links. And there's another one, and that is an Oasis standard, supported by big vendors like Microsoft, SAP and others. And that's Odata. But, Odata goes beyond just HATEOAS. It's an effort to standardize RESTful APIs. So, not only HATEOAS, but also rules on how to create URIs. How to page data, how to call function works with batch requests, and so on. And these are not all of them. There's quite a few others. In other words, there's a lot coming up, but given the current state of things, it's not that easy to decide on which one is best. Especially, since most of them aren't yet standardized. Let's have a look at this module's summary.

Summary
The last two modules were focused on HATEOAS and media types. These were also the modules in which we saw that building a RESTful system is a lot different from building a standard web API. As we learned, a custom media type should be used when a consumer wants to include these HATEOAS links in the response body. These media types are a key principle when building RESTful APIs. What we did up until this module, using Application/json for our resource or presentations, actually isn't truly correct. An author representation with a first name, last name, and a date of birth, is different from a representation with a name and age. For these, we can create separate media types. That then will let us to adapting to change, and versioning. We learned that we should that we should use HATEOAS for changes to functionality and business rules, and that until the code on the model constraint is feasible, we should use version media types. Building a truly RESTful API does take a lot of work. But the evolvability it was designed for is worth it. And we're talking about systems that are built to adapt to change over years, or even decades, and not just months. That's it for HATEOAS and media types. If used correctly, it can be a tremendous addition to an API. One more constraint remains. The cashable constraint. We'll look into that in the next module.

Working with Caching and Concurrency
Coming Up
Hi there, and welcome to the Working with Caching and Concurrency module of the Building a RESTful API with ASP. NET Core course at Pluralsight. I'm Kevin, and I'll guide you through this module. We've got one more constraint to cover. Cacheability. Caching responses can dramatically improve performance, but caching is not easy. It's also one of the first things to look at when unexplainable bugs seem to happen. We'll tackle caching first. After that, we'll look into concurrency, i. e., how can we ensure that when two people are working on the same resource, an update of the resource by the first person no longer overrides the changes the second person made in the meantime. But first, let's dive into caching.

Working with Caching
The cacheable constraint states that every response should define itself as cacheable or not. So, it's quite easy to adhere to this constraint. We can simply add a response header stating the response isn't cacheable, and that's perfectly valid, but well, that would make this a very short clip. So instead, we're going to learn how we can ensure our HTTP responses are cacheable. For this, we'll use HTTP Caching, which is part of the HTTP standard, RFC 2616, and it's described in full in RFC 7234. Let's have a look at what a standard states as a bit of a mission statement on caching. Caching would be useless if it did not significantly improve performance. The goal of caching in HTTP/1. 1 is to eliminate the need to send requests in many cases and to eliminate the need to send full responses in many other cases. Eliminating the number of requests that have to be sent reduces the number of network-roundtrips required for many operations. HTTP cache uses an expiration mechanism for that purposes. Eliminating the need to send full responses reduces network bandwidth requirements. And for that, a validation mechanism is used. It does depend on where the cache lives on what exactly is reduced by a specific mechanism. But we'll get to that. So, we'll look into expiration and validation. But let's start at that cache first, looking at it from an API point of view. We're working with HTTP, and the cache is a separate component. It sits between the client application, the consumer of the API, which makes the requests, and the API itself. The cache must accept requests from the consumer and pass them to the API. It will also receive response messages from the API, and it will store responses that are deemed cacheable and forward them to the consuming application. If the same response is requested again, the cache can send the cached response. So, we can look at the cache as the middle-man of the request-response communication between the consuming application and the API. In HTTP, there are three cache types. One is the client cache, or browser cache. It lives on the side of the client application. A browser is an HTTP client, but we can extrapolate that. If we're building, for example, a mobile client application with Xamarin, that client application, which communicates with the API through an HTTP client instance will often have a private cache. It's called private, because the resources aren't shared with anyone else. They're kept on the client for each consuming instance. So, good example to our cache on your mobile device, but also a cache in local storage for an angular application. Second type is the gateway cache. This is a shared cache. The resources it caches are shared across different applications. This type of cache is used on the server-side. Other names for this are reverse-proxy caches, or even HTTP accelerators. The third type is the proxy cache. It's also a shared cache, but it doesn't live with the consuming side nor at the side of our API. It lives somewhere on the network. They are often used by large corporations and ISPs to serve thousands of users. If you ever have to change your browser settings and input a proxy address, well, you've used one of these. We can't focus on this last one, it's out of our control. Most of the time, combinations are used, a private cache at consuming site combined with a gateway cache server-side, and maybe even a proxy in between. Depending on whether or not a response is cacheable, and where it is allowed to be cached, we might hit our back-end to write up to the database, or we might hit a proxy or gateway cache without having to communicate with our database. Or we might only hit a private client-side cache, eliminating communication between client and server completely. How can we handle this? Let's look into expiration first.

The Expiration Model
The expiration model is a way for the server to say how long a requested resource, so the response message, is considered fresh. A cache can then store this response, so subsequent requests are served from the cache as long as it's still fresh. For this, two response headers are used. The expires header is the simplest one. It contains an HTTP date, stating at one time the response expires. That has a potential issue. The clocks between the cache and the server have to be synchronized. It also offers little control over what type of response can be cached, when, and where. It's been superseded by the cache-control header, which addresses these limitations. In the example, you can see a cache-control header value, public, max-age=60. This means this response header contains two directives, public and max-age. Max-age states that the response can be cached for 60 seconds, so clock sync is no longer an issue. And public states that it can be cached by both shared and private caches. So, the server can decide on whether or not a response is even allowed to be cached by a gateway or proxy cache, or at all for that matter. It's just the preferred header for expiration. There's quite a few possible directives next to public and max-age. You can find those at the link onscreen. They are divided by directives that can be sent in a request, so a client can decide on some expiration related values, and directives that can be sent in a response, so the server can decide. We'll look into these a bit later on, as some have to do with expiration and how it works with validation. So we have to cover that first. Let's have a look at how this expiration model works. Imagine we have a client application, the consumer of our API, a cache, be it shared or private, and our API, the back-ends. The application sends a request to get the authors to request it to cache. There's no cached version of the response available, so the cache forwards the request to our API. Our API should then return a response, including a Cache-Control header that, in the case of our example, states that the response will stay fresh for 1800 seconds, or half an hour. That hits the cache again, and the cache then forwards it to the consuming application, but at the same time stores a copy of this response. Ten minutes later our app sends a request to the author's resource again. We hit the cache, and this time that cache has a response that hasn't expired. It sends back that response, including an age header. In our case, that age is 10 minutes or 600 seconds. A request to the back-end is thus eliminated. Only when the cached version reaches expiration time, which means it becomes stale, the cache will hit the back-end again. If this cache is a private cache, like one that lives in local storage for an Angular app or on a mobile device for a mobile app, that's where this stops. But if we assume this cache is a shared cache, living on the server, for example, it's a bit different. Ten minutes later, a subsequent request from another instance of our client application, or another client application that consumes our API, is sent to get the authors. It will hit that cache, and assuming your response hasn't expired, it will serve that up from the cache, this time with an age header of 20 minutes, or 1200 seconds. So there's again no communication with our API. Let's look at how these two types of caches behave regarding the expiration model. If it's a private cache, subsequent requests save bandwidth and reduce requests to the API. As long as the response hasn't expired, we don't hit the server anymore for requests coming from the same client instance. But of course when different clients and the same request, they still hit our back-end, as they all have their own private cache. If it's a shared cache, subsequent requests don't save bandwidth between the cache in our API, but it does drastically eliminate requests to our API. Imagine there's 1000 simultaneous consumers our API. The first one hits a specific resource on our API, and all the other ones who want to get that same resource hit the cache, which also means a lot less code has to be executed. The number of times the database has to be accessed goes down drastically, and so on. So, this is a pretty good reason to combine a client cache, or private cache, and a shared cache, or public cache. But when we look at expiration like this, we see that it's typically only good for content that's quite static, like images and web pages. So, it's actually not a good fit for APIs like ours of which the data can dynamically and often change. If an author is added by one of our thousand consuming applications, the author's response that's cached isn't correct anymore. Worse case with an expiration of 30 minutes, we end up with as good as half an hour of invalid data. And that's what the validation model takes care of. Let's have a look.

The Validation Model
Validation is used to validate the freshness of a response that's been cached. When I cache as a stale entry that it would like to use as a response to a client's request, it first has to check with the origin server, or possibly an intermediate cache with a fresh response, to see if its cached entry is still usable. To be able to validate, we need something to, well, validate against. And that's validated. These validators are used to check if they represent the same or different responses. There's two types of validators, strong validators and weak validators. A strong validator changes if the body or headers of the response change. A typical example of such a validator is an ETag, or Entity Tag, sent through via the ETag response header. Such an ETag is an opaque identifier assigned by a web server, or API, to a specific version of a resource. Strong validators can be used in any context with caching, but also for concurrency checks when updating resources, as we'll learn later on in this module. A weak validator doesn't always change when the resource changes. It's up to the server to decide when a change is warranted. For example, only change on significant changes, not when less significant aspects change. A typical example is the last-modified response header, containing the time when the resource was last modified. These are implicitly weak. We've got a possible 1 second gap. Now, it is possible to make these strong by adhering to a set of rules, like an additional date value on cached entry that's at least 60 seconds after the last modified date, among others, but that would lead us too far. So we treat these as weak. But even then, the clocks must be synchronized, so it suffers the same issues as the expires header, which is why ETags are a better option, and there's something like a weak ETag as well. When it's post-fixed with w/, it's treated as being weak. It's up to the server to decide this. When an Entity Tag is in this format, it doesn't necessarily change if the response changes. Weak validators can only be used when an equivalent resource is good enough, but equality isn't required. The HTTP standard advises to send both ETag and Last-Modified headers, if possible. Let's have a look at how that works. We have again got a client application, consumer of our API, a cache, shared or private, and our API, the back-end. The application sends a request to get the authors. This request hits the cache. There's no cached version of the response available, so the cache forwards the request to our API. Our API returns a response that includes an ETag and Last-Modified header. That's sent to the cache again, which forwards it to the consuming application, but at the same time stores a copy of this response. After that, our app sends a request to the author's resource again, 10 minutes later. We hit a cache, but we can't be sure that the response that's been cached is still fresh. There are no cache control headers to check for this in our example. So the cache has to check this with the server, the API. It does this by adding request headers, If-None_Match set to the ETag value, and If-Modified-Since, which is set to the last modified value of the cached response. These make this request conditional. The server receives this request, and it checks these headers against the validators it holds or generates for that request. If this checks out, the server does not have to generate the response. It sends back a 304 Not Modified, after which the cache can return the cached response with an updated Last-Modified header, if it supports Last-Modified. If the response has been modified, the API will, of course, have to generate a new response. If the cache is a private cache, that's again where it stops. But, if we assume this cache is a shared cache, a subsequent request, 10 minutes later from another instance of the client application or another client application that consumes our API, will hit that cache. The cache will set the If-Modified-Since header to the last modified value of the cached response, and If-None-Match to the ETag value again. If it still checks out, the response still doesn't have to be generated. In other words, with validation, the same response only has to be generated once. So, let's look into these private and shared caches in a bit more detail regarding the validation model. If it's a private cache, subsequent requests save bandwidth. We have to communicate with the API, but we don't need to send over the full response from the API, only a 304 Not Modified if the response is the same. If it's a shared cache, subsequent requests don't save bandwidth between the cache and the client, but it does save bandwidth between the cache and the server as the server does not have to regenerate the response and resend it if it's still valid. So, two different strategies, two different uses. More often than not, expiration and validation are combined. Let's have a look at what that looks like. And let's immediately make a separation between private and shared caches. Let's start with the private cache. We've got a response to our author's resource cached. It has a cache control header for expiration checks, and a last-modified and ETag header for validation checks. A new request is sent, and as long as the response hasn't expired, the cached response can be used. This reduces communication with the API, including regenerating responses, and it reduces bandwidth requirements; we don't even hit the API. Once it expires, we do hit the API. If we only had expiration checks, this means the API would have to regenerate the response. But with the validation checks, we can potentially avoid this, because it's not because the response is stale that it's invalid. So the API checks the validators, and if the response is still valid, it sends back a 304 Not Modified. Bandwidth usage and response generation is thus potentially reduced even more. Because even an expired response doesn't necessarily result in a response body. If the cache is shared, the cached response is used as long as it hasn't expired. This doesn't result in bandwidth reduction between client and cache, but it does result in bandwidth reduction between cache and API, and dramatically reduces the amount of requests to the API. A lot more than a private cache, as this one is shared across potentially unlimited client instances. If the response expires, the cache must communicate with the API. But here as well it doesn't mean the response has to be regenerated. If validation checks out, a 304 Not Modified is sent to the cache without a response body. That potentially reduces bandwidth usage between cache and API, and response body generation. The new response must still be sent to the client from the cache, of course. Combine these two caches, a private cache for each client instance, and a shared cache at server level and with each the Holy Grail of caching.

Cache-control Directives
As we learned a bit earlier, there's quite a few cache-control directives available that control caching, both a request side and a response side. As we've now covered expiration and validation, it's a good time to look at those. Let's start with the response cache-control directives. The first two have to do with how long a response can be considered fresh. Max-age defines the maximum age after which a response expires in seconds. S-maxage overrides the maxage value for shared caches. So a response can expire differently in a private cache than in a shared cache. Then we have two that relate to cache location. Public indicates that the response may be cached by any cache, private or shared. And private indicates that all or part of the response message is intended for a single user and must not be cached by a shared cache. And then there's three that relate to validation. No cache indicates that the response should not be used for subsequent requests without successful revalidation with the origin server. Two more directives have to do with that, must-revalidate and proxy-revalidate. With must-revalidate, the server can state that if a response becomes stale, i. e. it's expired, well, then revalidation has to happen. This allows the server to force a revalidation by the cache, even if the client has decided that stale responses are okay. We'll get to that. And proxy-revalidate is exactly the same as must-revalidate, but it doesn't apply to private user age in caches, like browser cache. Lastly, there's no-store and no-transform. No-store states that the cache must not store any part of the message, mostly used for confidentiality reasons. And no-transform states that the cache shouldn't convert the media type of the response body. Now, all of these are decided by the server, but the client can override some of these settings, or rather use the same directives to tell the cache or server what it's willing to accept. Three have to do with freshness. Max-age indicates that the client is willing to accept a response whose age is no greater than the specified time in seconds. Min-fresh indicates that the client is willing to accept a response whose freshness lifetime is no less than its current age, plus the specified time in seconds. That is, the client wants a response that will still be fresh for at least a specified number of seconds. And max-stale indicates that the client is willing to accept a response that has exceeded its expiration time. There's only one related to validation, and that's a no-cache directive, stating that the response list request should not be used for subsequent requests without successful revalidation with the original server. No-store and no-transform are the same as for the response headers. There's only one new one, only-if-cached. This states that a client wants the cache to return only responses that it currently has stored, and not reload or revalid with the origin server. This one is typically used when there's a very poor network connection. Now, these are already pretty advanced directives and options. In a lot of cases, we're quite okay with max-age and public or private. Next to that, there are even more headers relate to caching, like Vary headers, but going even deeper into this would really lead us too far. It's time to look into how we can implement all this.

Supporting Cache Headers
To implement support for HTTP caching, according to REST constraints, we'll need a component that generates the correct response headers for us, and checks the headers that are sent through with a request, so it can only turn a 304 Not Modified or 412 Preconditioned Failed. That last one is a new one. We'll get to that when we talk about concurrency. This component does live at the back-end behind those cache stores we saw on the slides. In ASP. NET Core, there is a built-in ResponseCache attribute, which can be used on controller actions. It generates a cache control header, but that's about it. It doesn't do anything as far as storing the responses in a cache is concerned, so it's not a caching store, but that's okay, but it also doesn't work with ETags. So, I wouldn't advise this at the moment. We need a bit more than that. In the old ASP. NET, there's a popular component that does handle expiration and validation, and it works with ETags, CacheCow. Server. It's both generated of cache-control and ETags, and a cache store. But at the moment of recording, there's no version available that supports ASP. NET Core. So, as the piece of middleware we need is missing, I started the new open source project at GitHub for this, Marvin. Cache. Headers. This is middleware for ASP. NET Core that adds http cache headers to responses, cache control, expires, ETag, and last modified, and implements cache expiration and validation models. There's a reason it's preferable to separate generation of the headers from the cache store. Those ETags are also used for other purposes, like concurrency. Even though both are often used together, you don't necessarily need to or want a cache store if all you use the headers for are concurrency checks. But more on that later on in this module. Let's start with this, and we can talk about cache stores after the demo.

Demo - Supporting HTTP Cache Headers
In this demo, we'll have a look at how expiration and validation related response headers can be configured and added to our responses. The first thing we'll need to do is add the middleware to generate these. So, let's open the NuGet dialog. We'll look for the Marvin. Cache. Headers package. Let's install it, and let's open the startup class. We're in the ConfigureServices method. Here we should add the services used by the middleware to the container. For that, we call into AddHttpCacheHeaders. This is also where we can configure options for how the headers are generated. For now, we'll leave it at defaults. In the configure method, we then add the middleware to the request pipeline. The order in which we do this is important. We want to add it before the MVC middleware is added to the request pipeline. As in some cases, this middleware must ensure the request doesn't continue to the MVC middleware. For that, we call into app. UseHttpCacheHeaders. An example of when such a request shouldn't go to the MVC middleware is when validating that a response is still valid, even after it has expired. In that case, 304 Not Modified should be returned and the response body should not be regenerated. And that's it as far as code is concerned for now. We're going to test this by getting a book for an author, so let's set a breakpoint there, so we can see when the action gets hit. That's our breakpoint. Now let's give this a try. We're in Postman. The first thing to do is open Postman settings. We're working with caching now, but Postman automatically sends a no-cache cache control attribute. This is often desired when testing APIs, because you want to hit your code. But in our case, we want to test if our cache is functioning as it should. So, make sure this is set to off. Let's send the request to get one book for an author. We hit the GetBookForAuthor action. So far, so good. Let's continue. And we get back The Winds of Winter. Let's have a look at the headers. A cache-control header has been added, stating this response can be cached by both shared and private caches, and that the response is valid for 60 seconds or 1 minute. The expires header also reflects this. It expires 1 minute from now. Next to that, validation related headers have been added as well. A strong ETag has been generated, and a Last-Modified date, which is the current, has been added as well. How ETag should be generated isn't part of the standard. It's up to the server, to us, to decide on this. And that makes sense, as it's only us who can decide when a response should be considered equal or equivalent. By default, this middleware takes the request path, accept and accept-language headers into account, and the response body. Let's send this request again. At least one minute has passed by now, so we should hit the GetBookForAuthor action again, which is indeed the case. Let's continue. And now let's immediately send it again. Sixty seconds haven't passed yet. And we again hit the GetBookForAuthor action. If we look at the response, the cache-control and expires headers have been updated to reflect this current request. But, what happened now? Is this still correct? Well, yes, because this middleware isn't the cache: it lives near the back-end and only handles generation of these headers, and validation as we'll learn soon. But it is not a cache store. If we think back at our diagram, this component takes care of the concerns at the back-end. To actually cache responses, a cache store, be it shared or private, or both, is required. We're covering that next, but first let's have a look at validation. If a response is stale, we learned that a cache must re-validate it, preferably using ETags. It does that by adding the ETag value as the If-None-Match header. So, let's do that. Now, normally a cache does this. Depending on how it's been configured, it does this on each request or when the response has actually expired. So what we're doing here it something we wouldn't have to do if there was a shared or private cache added. But we can do this to see what happens. So, let's send this. This time, we didn't hit the GetBookForAuthor action. Instead, we get back a 304 Not Modified. As we learned on the slides, this tells the cache that it can still use the cached response, so the body doesn't have to be regenerated, again improving performance. Now let's update that book by sending a PUT request. We're updating The Winds of Winter by changing the title and description. We get back a 304 No Content, so our PUT request was successful, and now let's send that same Get request with that same if-none-match header again. We hit our breakpoint, the GetBookForAuthor action. So this time, validation failed, because the ETags don't match anymore. A new ETag was generated when we sent the PUT request. So, we don't get back a 304 Not Modified; instead, the response gets regenerated. We can also configure how these headers are generated. Let's open the ConfigureServices method of the Startup class again. The AddHttpCacheHeaders method has a few overloads that accept options related to expiration and validation respectively. We accept an action as a parameter to configure the ExpirationModel options. Like that, we get access to the options related to expiration. For example, we can set the max-age to 600. These are related to the directives we learn about on the slides. And there's a few more, like CacheLocation, shared MaxAge, AddNoTransformDirective, and AddNoStoreDirective. As a second parameter, we can accept an action to set the validation model options. Here, we find options related to validation. A cache-control directive related to validation that's often added is must-revalidate. We learned that that tells a cache that if a response becomes stale, revalidation has to happen. Okay, and let's build, run, and send our Get request again. We're back in Postman. Let's close what we had, and let's start new by sending that Get request. We hit the GetBookForAuthor action, as expected. And let's have a look at the headers. We see that this time, the cache-control header's max-age is set to 600 instead of the default of 60, and the must-revalidate directive has been added. These few tests we just ran showed us how the caching principles we learned about on the slides work. And as you can see, it has the potential to dramatically improve performance. But we're not effectively caching anything yet. Let's have a look at the cache store options we have.

Cache Stores
Okay, so we can now generate the correct headers. But this isn't the cache store yet. We're okay as far as REST is concerned, but we are not actually saving the responses anywhere. For that, we need a cache store. This is a component that's responsible for storing those responses and returning them if they're not expired or invalid; it also checks with the back-end for validation. So, it's that middleman we talked about in the beginning of the module. There are a few options we have, depending on the framework used to build an application, and whether or not we want a private or shared cache store. Private stores live at the client, so at the side of the consumer. That's out of scope for this course, as we're not building a client application. But if you are, there are a few options to look into. Angular-http-etag is a private cache store for use with Angular applications. As that's one of the most popular frameworks to build client-side applications with, this one tends to get used quite often. Marvin. HttpCache is an implementation of the RFC2616, the caching standard. Essentially, it's an http handler that's used together with an HttpClient instance. It's a private cache, but it's created with a portable class library profile. So, it's not for. NET Core clients currently. Then, we have a mix. Some cache clients can act as both private or shared caches, depending on where they're used. CacheCow. Client is one of those. It's positioned as a library for implementing HTTP caching on both client and server in ASP. NET Web API. It uses message handlers on both client and server to intercept requests and responses and apply caching logic and rules. It does only work with the full. NET framework, so no PCL, and no. NET Core. As far as shared cache stores that do work with. NET Core are concerned, Microsoft offers one: the ResponseCaching middleware. This can be found in the microsoft. aspnetcore. responsecaching package. This cache store takes into account the headers the marvin. cache. headers middleware generates, and it will save responses in the cache and serve them up, depending on the headers. Note that it does not generate these headers itself. Be careful though: earlier versions, like 1. 0 and 1. 1, had some nasty bugs, like a severe issue when a no-cache directive is added; it also missed quite a few features. So I would advise to only use this from ASP. NET Core 2. 0 onwards. Version 2. 0 of the middleware is much more stable and rich in features. Let's have a look at how this works with a demo.

Demo - Adding a Cache Store
In this demo, we'll add a cache store to the request pipeline. We're using ASP. NET Core 2 for this demo, because as mentioned the response caching middleware is much more stable in this version. As we're using ASP. NET Core 2, there's no need to add a reference to microsoft. aspnetcore. responsecaching, as we already have a reference to the microsoft. aspnetcore. all metapackage. We're in the Startup class in the ConfigureServices method. So, let's register the ResponseCaching-related services by calling into services. AddResponseCaching. In the Configure method, we then add this cache store to the pipeline. We should do that before we call into app. UseHttpCacheHeaders. This ensures this cache sits in the pipeline before the header generation and MVC middleware, allowing it to serve up responses from the cache rather than continuing with generating the responses. And let's build and run. Let's fire up Postman. As mentioned previously, make sure that the Postman setting to send a no-cache cache control attribute is set to false. Let's send that request to get an author again. We hit our breakpoint. If we look at the headers, we see our cache-control, ETag, Expires, Last-Modified, and Vary headers. Let's send that request again. And this time, we don't hit the breakpoint anymore. When we did that in the previous demo, without a cache in the pipeline, we simply hit the GetAuthor action again. This time, on the other hand, the cache serves up the author response for us without hitting the rest of the pipeline. We also get an additional age header, which tells us the age of the response. Apparently, this response has an age of 38 seconds. Send it again. And now the age is 93 seconds, which is still less than the maximum age of 600, which we put in the cache-control header. As you can guess, this allows for some serious performance improvements. Up until now, we're only getting resources and we're adding the necessary cache-control and ETag headers. But Etags are also used for something we haven't covered yet: concurrency. Let's have a look.

Dealing with Concurrency in a RESTful World
Let's start with an example. Say two client applications or two users, Kevin and Sven, are working with a specific resource at the same time. For example, they are working on the same author. Kevin gets the author from the API, and a bit after that, Sven gets the same author from the API. Sven edits the author, and issues a PUT request. The author has been updated. Now Kevin, who is finally done editing, issues a PUT request as well. At this moment, Sven's changes are lost. They are overwritten with Kevin's changes, and that's a problem. I'm guessing this is a familiar concept for most of you. Issues like these, concurrent updates, are handled with a concurrency strategy. Pessimistic or optimistic concurrency control. Pessimistic concurrency control implies that the author would be locked for Kevin. As long as it remains locked, no one else but Kevin would be able to modify it. So, the name pessimistic implies that there will be some sort of fight over the resource. Pessimistic concurrency control isn't possible in REST, as one of the constraints is statelessness. So, we cannot use this. But there's also optimistic concurrency control. This implies that Kevin gets a token that allows him to update the author. As long as a token remains valid, Kevin will be able to perform the update. So, optimistic implies that there won't be an issue, but if there is, the token will tell you that. Optimistic concurrency is possible. And that token, well, we learned about that already. That's actually a validator. A strong validator is required for handling issues like these, so we can use strong ETags for that. Let's go back to our example. Kevin gets an author, and the author is returned with an ETag value. Sven gets an author, and that's returned with the same ETag. Sven updates the author, passing in the ETag in the if match header. The API checks this header, and compares it with the ETag it saved for that response. At this moment, a new ETag is generated for the response. The response then includes that ETag as well. But then Kevin sends the update with an If-Match header containing the ETag Kevin currently has. It reaches the API and there the API sees that this doesn't match with the most current ETag for that resource. So the API returns a 412 pre-conditioned failed status code. Kevin's update isn't applied, because he was working on an older version of the author. And that's how this works. The same applies to patch instead of PUT. This makes it all so reasonable to separate the cache store from the component that just generates the cache-control and ETag headers. Those headers serve different purposes. We don't need a cache to handle concurrent updates. Often, of course, these are combined, in which case the cache follows the same rules as we previously looked into. Let's have a look at how this works with a demo.

Demo - Dealing with Concurrency
In this demo, we'll have a look at how ETags can help with concurrency. The nice thing is that we don't have to code anything else. The Marvin-Cache-Headers middleware to generate ETags is already in place. So we're in Postman. Let's assume we're user Kevin and let's get an author. We still have that request saved, so let's send it. If you look at the headers, we see we get back an ETag. Now let's assume we're user Sven, and let's get this author again. Let's do that in a separate tab. So I'm copying over the header value, accept, and URI. That should be it. Let's send this request and let's have a look at the headers, and we see that the ETag of this request is the same as the ETag of the previous request. Let's have a quick look. Two ETags, both the same. And that makes sense. It is, after all, the same response. Now let's assume you want to update a response, but we want concurrency check in place to see if the update can go through. We've got a PUT request here, which we can use for that. Let's assume Sven is first, so the consumer or possibly an in-between cache. If we have that installed, it should take the ETag value we got when getting the author and put it in the If-Match header for the PUT request. So let's add an If-Match header, and we give it that ETag as its value. Now let's send this request. We get back a 204 No Content. The update was successful. If we now get the author again, we'll see it now has another ETag. So let's give that a try. This now is an ETag value that starts with 348. The ETag value we send in the If-Match header starts with EDF, as does the ETag we got back when we got the author previously. But what we have onscreen here with the ETag starting with EDF, well, that's the copy Kevin is still working on. He hasn't got the most recent copy with a new ETag. So, once Kevin wants to update it, he'll send a PUT request with that ETag in the If-Match header. Now this is something the client application or an in-between component typically does. It's not the responsibility of us as the builders of the API. So we're mimicking a consuming application here, or an in-between cache. Let's send this, and this time we get back a 412 precondition failed. The update isn't allowed. This tells Kevin that he should first get a fresh copy to work on. Like this, we've implemented optimistic concurrency checking. That's it for this demo.

Summary
We started this module with quite a big part on caching. One of the REST constraints states that each response must state whether or not it can be cached. We looked into the HTTP caching standard, and that was quite a lot of information. Caching really isn't a simple subject, but the most important parts to remember are that we use the expiration model to allow the server to state how long a response is considered fresh. For that we used the cache-control header. The second part to remember is that the validation model is used to validate the freshness of a response that's been cached. It does this with the help of strong validators, like ETags, or weak ETags. The cache itself can be private or shared, and depending on the type a model can reduce bandwidth requirements and/or network roundtrips. The strong ETags we used for validation can also be used as a token or validator for an optimistic concurrency strategy. This allows concurrent users to work on the same resource without running the risk of accidentally overwriting a newer version of a resource. And with that, our API adheres to all REST constraints. So we could say that we're done. Yet there's one module left. In that next module, we'll look into some tools and strategies that can help us protect and test our API.

Protecting, Testing, and Documenting Our API
Coming Up
Hi there, and welcome to the Protecting, Testing, and Documenting Our API module from the Building a RESTful API with ASP. NET Core Course at Pluralsight. This is the last module, and I'm still Kevin, so I'll guide you through it. We'll start out by looking into protecting our API with rate limiting and throttling. That allows us to protect the API against too many requests. Too many requests might be detrimental for performance, so we do want to keep that in check. Then we'll cover testing. How can we write tests for API actions, and can this be integrated into a continuous integration scenario? We'll see what our options are. After we've protected and tested our API, we may want to expose it to consumers. Especially for public APIs, documentation that's different from the technical design you might not be allowed to share outside of company walls can come in handy. And lastly, we'll look into how we can support consumers testing our API. For example, how they can check if a resource exists, and how they can check what is allowed on a specific resource. As you notice from that description, that kind of adds to what HATEOAS already does. But let's start out with rate-limiting and throttling.

Rate Limiting and Throttling
Rate limiting means limiting the amount of allowed requests to our API or a specific resource. It can thus protect the API against unintended usage scenarios, like robots scraping our data to commercialize it, or against too many requests that can result in deteriorated performance for all consumers of the API. A throttle in this regard is then used to control the rate of requests that are allowed to our API. It determines whether or not a specific request is allowed. For example, a specific client might have a throttle of 100 requests per hour. This can be limited even further or combined, for example, by stating that a specific IP is allowed 1000 requests per day, but at the same time it's limited to 100 requests per hour. Another example would be limiting the total amount of requests to the API for a given IP to 1000 a day, of which only 50 are allowed to the author's resource. Now for caching there were cache-related headers on the responses that tell the consumer, or an in-between cache, how to handle those. Likewise, response headers are used for rate limiting, even though they are not part of the HTP standard. A response to a request should contain rate limiting information in the response headers. They all start with X rate limits. The first one adds limit and contains the time period for which a limit is valid. Remaining provides the consumer with info on the remaining number of requests before the limit is reached, while reset provides information on when the limits will be reset. Now all this is for allowed requests, but if the limits are hit, we need to return something as well. Those responses warrant a 429 - Too many requests status code. It may include a Retry-After response header, and the response body should contain details explaining the condition. That's it for the theory. Now let's check how we can implement this.

Demo - Rate Limiting and Throttling
In this demo, we'll add support for rate limiting to our API. There's a NuGet package we can use for that, so let's open the NuGet dialog. Let's look for aspnetcoreratelimit. This is a NuGet package by Stefan Prodan containing ASP. NET Core rate-limiting middleware. So let's install it, and there we go, and let's open the startup class. First thing to do is registering the services. We do that in the ConfigureServices method. This middleware needs to store the counters and rules somewhere. For that, it uses a memory cache, so we register the services related to the memory cache on our container. We do that by calling into AddMemoryCache. Then we can configure the options. The ASP. NET Core rate limit package includes two pieces of middleware, one for rate-limiting by IP, and another one for rate-limiting by client ID. Working with any of these two follows the same principles, so we'll look into one of those, the IP rate-limiting middleware. So, we're going to configure the options for that. For that we call into Configure, and we pass in the options we want to configure. That's the IP rate limit options. They are part of the ASP. NET Core rate limit namespace. Here we accept an options parameter. On these options, there's quite a few properties we can use to configure the IP rate limit middleware. We can whitelist certain IPs, for example, but for our purposes the rules are more interesting. So, let's create a new list of general rules, and we pass in one rule to start with. An example would be limiting requests to the full API, any resource to 3 requests for each 5 minutes. To do that, we set the endpoints to star, we set limits to 3, and period to 5m for minutes. That's our rule. Now we just need to register a policy store and a rate limit counter store. Both of these are used by the middleware. They serve to store the policies and rate limit counters respectively. So we need to register these services. We add them as singleton services, as we only want these to be created once, and not for each request. If they were created for each request, well, we wouldn't be able to store the policies and rate limit counters across requests, which is what we need. So, we register an IRateLimitCounterStore, and we register the store to hold the policies. All of these interfaces and implementations are part of the ASP. NET Core Rate Limit package. Then, let's scroll down to the Configure method, so we can add the middleware to the request pipeline. This middleware should be registered before any other pieces of middleware in the request pipeline, save for logging and exception handling. That's because it's this middleware that's allowed to reject requests if the limits are hit. We use the IP rate limiting middleware, and for that we call into app. UseIpRateLimiting. And that should be it. Let's give this a try. Let's send the request to get authors. We get back our authors list. Let's have a look at the headers. Here are the headers we looked into before the demo. Important to notice is a that only one more request is allowed. Shouldn't that be two, as we configured the middleware to allow three requests? Well, the first request was already made when we launched the app to the local host root. So let's send that request again. The count is now 0. Once more, and we get back a 429. That's good. Let's have a look at the response body. This contains a message stating what went wrong. And let's have a look at the headers, and we see a Retry-After value telling the consumer when he or she can try again. So, our rate-limiting works. Let's open the code again and combine a few rules. Say we want to allow up to 10 requests every 5 minutes. So we change our limit to 10. But we only want to allow 2 requests each 10 seconds. For that we add a new rule. We set the limit to 2, and period to 10 seconds. Let's try that again. Let's send the requests to get the authors. If we look at the rate limits, we see that 9 requests are still remaining. Send it again. Eight requests are remaining. Let's send it again. Seven requests are remaining. And let's send it again. And this was the second time in 10 seconds we get back a 429 unknown. So that's our second rule at work. We were still allowed 7 requests in 5 minutes, but another rule stated that we were only allowed 2 requests each 10 seconds. That's why we now get back a 429. In case you're wondering why it says 429 unknown here in Postman, that's simply because Postman doesn't know the description of the status code. The headers also tell us we can retry after 3 seconds, and 3 seconds have definitely passed, so let's send the request again, and we see that we still have 6 requests remaining from our rule that allows 10 requests every 5 minutes. So, this works as expected. As mentioned, there are a lot of options to configure. We can add policies for each IP. We can configure IP ranges and client IDs, limit requests, depending on the methods or on the resource. So, 10 posts per minute to the author's resource is allowed, but we only get 100 each hour. And we can even read those options from configuration files instead of inputting them in code. For our purposes, I think the principles are clear. But before we continue, let's up the limits a bit, so we don't run into them during the rest of the demos. And let's continue by looking into how we can test our API. And we're diving straight into a demo with that.

Demo - Testing our API with Postman
In this demo, we'll look into how we can test our API during development. We're in Postman, and as a matter of fact, that's where we'll stay. We could say in a way that we've been testing all along. We write requests here and execute them. For example, getting our authors. But that's not really a test. From a test we'd expect to be able to check the outcome of a request automatically, so we can run a full set of tests and with one look can see if they all check out. In other words, much like running a set of unit tests. Postman actually allows us to write those tests. Let's open one of those saved requests, a request to get an author. Postman actually contains a full featured testing sandbox that lets us write and execute JavaScript-based tests for our API. And it's been right in front of us all this time. If we have a look at the screen, we see that there's a test step. Let's open that. In this tab, we can write our tests. On the right-hand side, there's even a small snippet section. Now let's add a test. One of the snippets is a test to check if the status code is 200. We add that by clicking the snippet. The tests collection signifies the collection of tests. A name is passed in, and after the equal sign, we should write a statement that will be tested. If that statement evaluates to true, the test will be successful. There's a few built-in variables we can use. We already see the response code onscreen. Via that we can get the status code of the response. But there's also a responseBody through which we can get access to the responseBody and responseHeaders through which we can get access to the responseHeaders. For now, let's just send this request. And let's have a look at the results. We get back a 200 OK and our test step now states 1 out of 1. That means that 1 out of 1 of our tests passed. In the window here, we see whether or not the test has passed, and we also see the name we gave to the test. Now this was an easy one. What about checking if the responseBody returned indeed contains the author we asked for? We can do that by checking that the ID from the URI matches the ID field in the body. To access the body as JSON, we call into JSON. pars, passing in the responseBody. Then we add a variable to check if the ID matches. Let's name it idMatches. It's the ID field from the response we want to check, and we want to check if it matches the GUID from the URI. Lastly we add a test. To do that, we call into the tests collection, passing in a name, and we set the result of that test equal to idMatches. Okay, let's run this again by sending the request. We had an error. Apparently I made a typo. Okay, let's fix that, and let's send it again. And this time both of our tests pass. So, we've now got two tests on our request to get one author. Let's try writing another one for getting a collection of authors. So we open the GET request to authors and the Tests tab. As we remember, it was important to always page these collection resources, even if no page information was passed through. We also wanted to have a X-Pagination header in the response headers collection. So, let's check if that header actually exists. We add a new test, let's say X-Pagination is present. To check for that, we can call into the postman. getResponse header function. We then pass in the name of the response header we want to test for presence. In our case, that's X-Pagination. Okay, let's run this. We get back a 200 OK, and let's have a look at the Tests tab. We get X-Pagination is present. So far, so good. I'm going to close what we just wrote as the exact same requests we just wrote, including the tests are already in the collection. Okay, so now we've got two requests with three tests. Can't we combine this, i. e. rather than having to click each request one by one, can we execute all of them and then look at the results? Well, yes we can. There's a collection runner in Postman, which we can use for this. It's underneath the Runner button, so let's click that, and here's our test runner. Here we can select a collection to run. In our case, let's select the module 9 collection, as that one contains our 3 tests. If requests in this collection contain tests, these tests will be run as well, after the requests. And after the run, we'll be presented with an overview. So, let's give this a try. Now, this module does contain a few requests that will fail, as we'll only implement the code for the upcoming requests in the next demos. But let's not worry about those; it's the tests that are of importance. Let's start the test. All requests in the collection ran, one for options and one for head, which both give a 403 Not Found, because we haven't implemented that yet, a GET Author with tests and GET Authors with tests, and then the two requests we got from the previous demo. And if we have a look both at the left of the screen and on the right, we can see the test results from our complete collection. So, this is pretty nice, and a lot more test samples can be found at getpostman. com, but the principles, as we've just learned about, stay the same. Yet one thing seems to be missing. We're still manually triggering these tests. These days a lot of projects use build servers and continuous integration. These are tests that we'd like to run on our build server. Let's have a look at how that can be achieved.

Automated Testing and Continuous Integration
Continuous integration is a development practice that requires integrating code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing the team to detect problems early. Needless to say, it's a popular way of working on projects these days. After the build is okay, we want to run tests, and like that, we can find out if the code is behaving as it should. These can be unit tests, but also tests like we just wrote. There's a tool, Newman, which we can get at the getpostman. com site. It's a command-line collection runner for Postman. So, it allows us to effortlessly run and test a Postman collection directly from the command-line. So, just what we did manually at the end of the demo. Now to show that we need a build server, and we haven't got one. Moreover, different teams use different build servers. We might use Team Foundation Build Service, or Jenkins, or any other product. But luckily the principle is always the same. Add an additional build step right after the build that calls into Newman. Let's have a look at a code sample. First thing to do is install Newman on the build server. How to do that depends on the build server, of course. But once that's done, it's a matter of adding one additional line of code as a build step, Newman for the command, -c for the collection, the name of the collection, and --exitCode 1. This states that if Newman is going to exit with this code, the tests have failed. And that's it. Like this, we can have automated test runs of our Postman collections. Let's continue by looking into documenting the API.

Documenting our API
From this moment on, we'll look at tools and principles that somewhat overlap with HATEOAS and media types. As we remember, we're not really there yet as far as a fully subscribing API and self-generating consuming applications are concerned. So, we might need some more extensive documentation. Purpose is to provide documentation for the consumers of the API. So, this isn't a technical design document. That's used when creating the API, or if the API is consumed inside of a company where sharing the technical design is allowed. So, what should be in good documentation then? Well, for the last time in this course, let's look at our outer-facing contract again. URIs, or resource identifiers, methods, and payloads, including the media types. That's what consumers of the API need to know about. And often, that's combined with some sort of interactive UI, which can be used to send requests. So, there's a bit of overlap with testing as well, but this is testing on a consumer level, not a development level. How do we do this then? One of the most popular formats is Swagger OpenAPI, positioned as a definition format to describe RESTful APIs. It was recently updated to version 3. To generate documentation that uses that format, we can use Swashbuckle, which implements the standard. It generates API documentations straight from our routes, controllers, and models, including a UI to explore and test operations. If you've ever created an API in ASP. NET web API, chances are you already encountered this. There's an ASP. NET Core version of that, which you can find at the URI onscreen. But there's an important issue that effectively blocks using this for our API at the moment. Swashbuckle uses the OpenAPI 2 format, and not OpenAPI 3. Version 2 has no decent support for using overloads for actions on URIs. In the last module, we looked into media types in HATEOS. We learned that we can use different media types to get post or update resources. Those media types describe the format of input and output when used in a content type or except header respectively. For creating an author on Post, we support overloading for input, opposed to authors can accept an author with a date of that, or one without. There is no support for this currently in Swashbuckle.

Working with OPTIONS
When we looked into routing in the second module, we talked about HTP methods. We covered GET, POST, PUT, PATCH, and DELETE, but we also mentioned two more, and we haven't covered them yet, OPTIONS and HEAD. Both of these can be used by a consumer to test something related to the API. Let's talk about options first. An OPTIONS request represents a request for information on the communication options available on that URI. And this allows a client application to determine the options and/or requirements associated with the resource, or the capabilities of the API. It does that without implying a resource action, or initiating a resource retrieval. You can see an example of such a request and its response headers onscreen. OPTIONS will tell us whether or not we can get to resource, POST it, DELETE it, and so on. It does work on resource level. OPTIONS are returned in an allow response header as a comma-separated list of methods. Let's see how we can implement that.

Demo - Supporting the OPTIONS Method
In this demo, we're going to add support for OPTIONS. We do that on our AuthorsController, so let's open that. Let's add a new action, GetAuthorsOptions, and this time we want to annotate it with the HttpOptions attribute. This will ensure that an Options request to API authors will end up in this action. What we want to do is add a response header, allow. For that we call into Response. Headers. Add. First value is the response header key, Allow. The second value is a comma-separated list of allowed methods on the authors resource. So in our case, that's GET, OPTIONS, and POST. A successfully OPTIONS request should return a 200 OK status code, even when empty. But if it's empty, the response must include a content length field with a field value of 0. ASP. NET Core MVC automatically adds that for us, so we don't have to do anything for this. Optionally a response body can be included, but that format isn't covered by the HTTP standard. So, for our demo, we're going to leave it empty. Okay, let's give this a try. We're in Postman, and I've got an OPTIONS request to API authors lined up here. Let's send this and let's see what we get back. We get back a 200 OK status code. So far, so good. Let's have a look at the headers. Here we see the content length set to 0, so ASP. NET Core MVC handled this for us. And we also see our Allow header, and this one has the comma-separated list with the Allow methods on the Authors resource as its value. Let's continue with looking into the HEAD method.

Working with HEAD
The last method that we want to cover is HEAD. HEAD is identical to GET with the notable difference that the API shouldn't return a response body. So, no response payloads. It can be used to obtain information on the resource, like validity, accessibility, and recent modification. So, let's give that a try.

Demo - Supporting the HEAD Method
In this demo, we'll implement support for the HEAD method for our Authors Resource. We are in the AuthorsController. As HEAD must return the exact same response as GET, but without a response body, that also means that in case of our GetAuthors action, it must return paging information. If we scroll down a bit, we see that we add an X-Pagination header with metadata related to paging. And this could become quite cumbersome. If we have to write the new action for this, that action must still execute everything we execute in the GetAuthors action, but simply not fill out a response body. Luckily the people who design the ASP. NET Core MVC made this really easy for us. All we have to do is annotate the GetAuthors action with HttpHead attribute. Next, the already existing HttpGet attribute. Head is not meant to help with performance at the level of the API. It's used to retrieve information in the response headers without transporting the response body. So, it helps to conserve bandwidth. Let's give this a try. We're in Postman, and we've got a HEAD request to API Authors lined up here. Let's send it, and we get back a 200 OK status code, and we see the response body is empty. Let's have a look at the headers. All the response headers we would normally get with a GET request or include, including the X-Pagination header. And if you look at the value of the X-Pagination header, we see that it even includes the total count of authors. That means that all our code was executed server-side, so the headers were generated correctly, but response body wasn't transferred. And with that, we added support for the HTTP HEAD method. It's time for the final summary.

Summary
We started out this module by looking into rate limiting. With that, we can limit the amount of allowed requests to an API. It can thus protect the API against unintended usage scenarios, like robots scraping our data to commercialize it, or against too many requests that can result in deteriorated performance for all consumers of the API. We learned that we can write tests for our API in Postman, using JavaScript. We can also run those tests with the collection runner. In a real-life environment, we want to do that at the level of a build server, and we can do that. Tests can be integrated in a continuous integration environment by using Newman, a command-line interface for Postman. We learned that good documentation for consumers includes the Resource identifiers, HTTP methods, and payload information including media types. So, not unlike what's in a technical design document. However, those documents often contain more information, and aren't exactly user-friendly, so we don't necessarily want to share them with everyone. Luckily, tools and frameworks like Swagger and Swashbuckle help us with this. The only thing that's a bit of a shame currently is that Swashbuckle doesn't support action overloading, which is what we do need. Then, we looked into OPTIONS. We can use OPTIONS to determine the options and/or requirements associated with a resource, or the capabilities of a server, or API. And lastly, we learned we can use HEAD to test for validity, accessibility, and recent modification. And that's almost it. I hope you had as much fun watching this course as I had creating it. If you have any comments, feel free to send me a message on Twitter, or write it on the discussion tab. Before I sign off, I do want to say one more thing: you're ready to be awesome!